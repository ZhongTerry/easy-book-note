import os
import json
import sqlite3
import hashlib
import time
import uuid
import re
from datetime import datetime, timedelta
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import session, g, has_request_context
from shared import USER_DATA_DIR, CACHE_DIR, DL_DIR
import shared

# ==========================================
# 0. æ•°æ®åº“æ ¸å¿ƒ (SQLç‰ˆ)
# ==========================================
DB_PATH = os.path.join(USER_DATA_DIR, "data.sqlite")

def get_db():
    """è·å–æ•°æ®åº“è¿æ¥"""
    if has_request_context():
        if 'db' not in g:
            g.db = sqlite3.connect(DB_PATH)
            g.db.row_factory = sqlite3.Row
        return g.db
    else:
        conn = sqlite3.connect(DB_PATH)
        conn.row_factory = sqlite3.Row
        return conn

def close_db(e=None):
    db = g.pop('db', None)
    if db is not None: db.close()

def get_current_user():
    if not has_request_context():
        return 'default_user'
    return session.get('user', {}).get('username', 'default_user')

# ==========================================
# 1. åŸºç¡€ç±»å®šä¹‰ (BaseJsonManager)
# ==========================================
class BaseJsonManager:
    def __init__(self, module_type):
        self.module_type = module_type

    def load(self, username=None):
        u = username or get_current_user()
        try:
            conn = get_db()
            row = conn.execute("SELECT json_content FROM user_modules WHERE username=? AND module_type=?", (u, self.module_type)).fetchone()
            if row and row[0]: return json.loads(row[0])
        except Exception as e:
            # print(f"DB Load Error ({self.module_type}): {e}")
            pass
        return {}

    def save(self, data, username=None):
        u = username or get_current_user()
        try:
            json_str = json.dumps(data, ensure_ascii=False)
            conn = get_db()
            conn.execute("REPLACE INTO user_modules (username, module_type, json_content) VALUES (?, ?, ?)", (u, self.module_type, json_str))
            conn.commit()
            if not has_request_context(): conn.close()
        except Exception as e:
            print(f"DB Save Error ({self.module_type}): {e}")

# ==========================================
# 2. è§’è‰²ç®¡ç† (System Config)
# ==========================================
class RoleManager:
    def load(self):
        return self._load_config()
    def _load_config(self):
        try:
            with get_db() as conn:
                row = conn.execute("SELECT value FROM sys_config WHERE key='roles'").fetchone()
                return json.loads(row[0]) if row else {"admins": [], "pros": []}
        except: return {"admins": [], "pros": []}

    def _save_config(self, data):
        with get_db() as conn:
            conn.execute("REPLACE INTO sys_config (key, value) VALUES (?, ?)", ('roles', json.dumps(data)))
            conn.commit()

    def get_role(self, username):
        if not username: return "guest"
        data = self._load_config()
        if username in data.get("admins", []): return "admin"
        if username in data.get("pros", []): return "pro"
        return "user"

    def set_role(self, username, role):
        data = self._load_config()
        if username in data["admins"]: data["admins"].remove(username)
        if username in data["pros"]: data["pros"].remove(username)
        if role == "admin": data["admins"].append(username)
        elif role == "pro": data["pros"].append(username)
        self._save_config(data)

# ==========================================
# 3. ä¸šåŠ¡ç®¡ç†å™¨ (ç»§æ‰¿ BaseJsonManager)
# ==========================================

class HistoryManager(BaseJsonManager):
    def __init__(self): super().__init__('history')

    def add_record(self, book_key, title, url, book_name=None):
        data = self.load()
        if "records" not in data: data["records"] = []
        # å»é‡å¹¶ç½®é¡¶
        records = [r for r in data["records"] if r.get('key') != book_key]
        records.insert(0, {
            "key": book_key,
            "title": title,
            "url": url,
            "timestamp": int(time.time()),
            "book_name": book_name or book_key
        })
        data["records"] = records[:50] # ä¿ç•™æœ€è¿‘50æ¡
        self.save(data)

    def get_history(self): return self.load().get("records", [])
    def clear(self): self.save({"records": []})

class IsolatedBooklistManager(BaseJsonManager):
    def __init__(self): super().__init__('booklists')

    def add_list(self, name):
        data = self.load()
        list_id = str(int(time.time()))
        data[list_id] = {"name": name, "books": []}
        self.save(data)
        return list_id

    def add_to_list(self, list_id, book_data):
        data = self.load()
        if list_id in data:
            if not any(b['key'] == book_data['key'] for b in data[list_id]['books']):
                data[list_id]['books'].append(book_data)
                self.save(data)

    def update_status(self, list_id, book_key, status, action):
        data = self.load()
        if list_id in data:
            books = data[list_id]['books']
            if action == 'remove':
                data[list_id]['books'] = [b for b in books if b['key'] != book_key]
            else:
                for b in books:
                    if b['key'] == book_key: b['status'] = status
            self.save(data)
    
    # å…¼å®¹æ—§ä»£ç è°ƒç”¨ load æ–¹æ³•ç›´æ¥è¿”å›å­—å…¸
    def load(self, username=None):
        return super().load(username)

class IsolatedTagManager(BaseJsonManager):
    def __init__(self): super().__init__('tags')
    
    def update_tags(self, key, tags):
        d = self.load()
        if tags: d[key] = [t.strip() for t in tags if t.strip()]
        elif key in d: del d[key]
        self.save(d)
        return d.get(key, [])
    
    def get_all(self): return self.load()

class UpdateManager(BaseJsonManager):
    def __init__(self): super().__init__('updates')
    
    def set_update(self, book_key, latest_data, username=None):
        data = self.load(username)
        # å…¼å®¹æ€§å¤„ç†
        title = latest_data.get('title') or latest_data.get('latest_title') or "æœªçŸ¥"
        url = latest_data.get('url') or latest_data.get('latest_url')
        cid = latest_data.get('id') or latest_data.get('latest_id') or -1
        
        data[book_key] = {
            "latest_title": title,
            "latest_url": url,
            "latest_id": cid,
            "toc_url": latest_data.get('toc_url'),
            "last_check": int(time.time())
        }
        self.save(data, username)
        
    def get_update(self, book_key):
        return self.load().get(book_key)

    def update_progress(self, book_key, unread_count, status_text, username=None):
        data = self.load(username)
        if book_key in data:
            data[book_key]['unread_count'] = unread_count
            data[book_key]['status_text'] = status_text
            self.save(data, username)

class IsolatedStatsManager(BaseJsonManager):
    def __init__(self): super().__init__('stats')

    def update(self, t, w, c, bk):
        d = self.load()
        if "daily_stats" not in d: d["daily_stats"] = {}
        k = datetime.now().strftime('%Y-%m-%d')
        if k not in d["daily_stats"]: d["daily_stats"][k] = {"time":0,"words":0,"chapters":0,"books":[]}
        r = d["daily_stats"][k]
        r["time"]+=t; r["words"]+=w; r["chapters"]+=c
        if bk and bk not in r["books"]: r["books"].append(bk)
        self.save(d)
        
    def get_summary(self):
        today = datetime.now()
        summary = {
            "24h": {"time": 0, "words": 0, "chapters": 0, "books": 0},
            "7d":  {"time": 0, "words": 0, "chapters": 0, "books": 0},
            "30d": {"time": 0, "words": 0, "chapters": 0, "books": 0},
            "all": {"time": 0, "words": 0, "chapters": 0, "books": 0, "heatmap": []},
            "trend": {"dates": [], "times": []}
        }
        books_sets = {"24h": set(), "7d": set(), "30d": set(), "all": set()}
        data = self.load()
        daily = data.get("daily_stats", {})
        
        for i in range(29, -1, -1):
            day = today - timedelta(days=i)
            d_str = day.strftime('%Y-%m-%d')
            rec = daily.get(d_str, {})
            summary["trend"]["dates"].append(d_str[5:])
            summary["trend"]["times"].append(int(rec.get("time", 0) / 60))

        for date_str, rec in daily.items():
            try:
                rec_date = datetime.strptime(date_str, '%Y-%m-%d')
                delta = (today - rec_date).days
                t, w, c = rec.get("time", 0), rec.get("words", 0), rec.get("chapters", 0)
                b_list = rec.get("books", [])
                summary["all"]["time"] += t; summary["all"]["words"] += w; summary["all"]["chapters"] += c; books_sets["all"].update(b_list)
                if t > 0: summary["all"]["heatmap"].append({"date": date_str, "count": int(t/60)})
                if delta == 0: summary["24h"]["time"] += t; summary["24h"]["words"] += w; summary["24h"]["chapters"] += c; books_sets["24h"].update(b_list)
                if delta < 7: summary["7d"]["time"] += t; summary["7d"]["words"] += w; summary["7d"]["chapters"] += c; books_sets["7d"].update(b_list)
                if delta < 30: summary["30d"]["time"] += t; summary["30d"]["words"] += w; summary["30d"]["chapters"] += c; books_sets["30d"].update(b_list)
            except: pass
        for k in books_sets:
            summary[k]["books"] = len(books_sets[k])
            summary[k]["time"] = int(summary[k]["time"] / 60) 
        return summary

# ==========================================
# 4. æ ¸å¿ƒ KV æ•°æ®åº“ (SQLç‰ˆ)
# ==========================================
# managers.py ä¸­çš„ IsolatedDB ç±» (æ›¿æ¢åŸæœ‰çš„)

# managers.py -> IsolatedDB ç±» (å®Œæ•´æ›¿æ¢)

# managers.py -> IsolatedDB ç±» (å®Œæ•´æ›¿æ¢)

# managers.py -> IsolatedDB ç±» (å®Œå…¨æ›¿æ¢)

# managers.py -> IsolatedDB ç±» (è¯·æ›¿æ¢æ•´ä¸ªç±»)

class IsolatedDB:
    def _get_db_conn(self):
        username = session.get('user', {}).get('username', 'default_user')
        db_path = os.path.join(USER_DATA_DIR, f"{username}.sqlite")
        conn = sqlite3.connect(db_path)
        return conn

    def _ensure_history_table(self):
        try:
            with get_db() as conn:
                conn.execute('''CREATE TABLE IF NOT EXISTS book_history (
                                id INTEGER PRIMARY KEY AUTOINCREMENT,
                                username TEXT NOT NULL,
                                book_key TEXT NOT NULL,
                                value TEXT,
                                recorded_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                            )''')
                # === [æ–°å¢] è¿½æ›´è¡¨ ===
                conn.execute('''CREATE TABLE IF NOT EXISTS book_updates (
                                book_key TEXT PRIMARY KEY,
                                username TEXT NOT NULL,
                                toc_url TEXT,
                                last_local_id INTEGER DEFAULT 0,
                                last_remote_id INTEGER DEFAULT 0,
                                has_update BOOLEAN DEFAULT 0,
                                updated_at TIMESTAMP
                            )''')
                conn.commit()
        except: pass

    # === 1. è¿ç§»é€»è¾‘ (å¯åŠ¨æ—¶è¿è¡Œ) ===
    def migrate_legacy_data(self):
        """å°†æ—§çš„çº¯æ–‡æœ¬ URL è½¬æ¢ä¸º JSON å¯¹è±¡"""
        print("[DB] æ£€æŸ¥æ•°æ®ç»“æ„ç‰ˆæœ¬...")
        try:
            with sqlite3.connect(DB_PATH) as conn:
                cursor = conn.cursor()
                cursor.execute("SELECT username, book_key, value FROM user_books")
                rows = cursor.fetchall()
                
                count = 0
                for username, key, val in rows:
                    if not val or key.startswith('@') or key.endswith(':meta'): continue
                    
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯ JSON
                    is_json = False
                    try:
                        d = json.loads(val)
                        if isinstance(d, dict) and 'url' in d: is_json = True
                    except: pass
                    
                    if not is_json:
                        # è¿ç§»ï¼šçº¯å­—ç¬¦ä¸² -> JSON
                        new_data = json.dumps({
                            "url": val,
                            "cover": "",
                            "author": "",
                            "desc": "",
                            "updated_at": int(time.time())
                        }, ensure_ascii=False)
                        cursor.execute("UPDATE user_books SET value=? WHERE username=? AND book_key=?", (new_data, username, key))
                        count += 1
                
                conn.commit()
                if count > 0: print(f"[DB] âœ… å·²è¿ç§» {count} æ¡æ—§æ•°æ®ä¸º JSON æ ¼å¼")
        except Exception as e:
            print(f"[DB] è¿ç§»æ£€æŸ¥è·³è¿‡: {e}")

    # === 2. æ ¸å¿ƒå†™å…¥é€»è¾‘ (è‡ªåŠ¨åŒ…è£…) ===
    def insert(self, key, value, username=None):
        if not key: return {"status": "error", "message": "Key cannot be empty"}
        u = username or get_current_user()
        
        final_json = ""
        if isinstance(value, dict):
            final_json = json.dumps(value, ensure_ascii=False)
        else:
            try:
                temp = json.loads(value)
                if isinstance(temp, dict) and 'url' in temp: final_json = value
                else: raise ValueError()
            except:
                final_json = json.dumps({
                    "url": value, 
                    "updated_at": int(time.time())
                }, ensure_ascii=False)

        try:
            with get_db() as conn:
                conn.execute("INSERT OR REPLACE INTO user_books (username, book_key, value) VALUES (?, ?, ?)", (u, key, final_json))
                conn.commit()
            return {"status": "success", "message": f"Saved: {key}", "data": {key: final_json}}
        except Exception as e: return {"status": "error", "message": str(e)}

    def update(self, key, value, username=None):
        u = username or get_current_user()
        try:
            conn = get_db()
            row = conn.execute("SELECT value FROM user_books WHERE username=? AND book_key=?", (u, key)).fetchone()
            
            # [å…³é”®ä¿®å¤] å¯¹äº :meta ç»“å°¾çš„ keyï¼Œç›´æ¥å­˜å‚¨åŸå§‹å€¼ï¼Œä¸è¦åŒ…è£…
            if key.endswith(':meta'):
                # value å·²ç»æ˜¯ JSON å­—ç¬¦ä¸²ï¼Œç›´æ¥å­˜å‚¨
                conn.execute("INSERT OR REPLACE INTO user_books (username, book_key, value) VALUES (?, ?, ?)", 
                           (u, key, value))
                conn.commit()
                return {"status": "success", "message": f"Updated meta: {key}"}
            
            # æ­£å¸¸çš„ä¹¦ç± keyï¼Œä½¿ç”¨åŒ…è£…é€»è¾‘
            current_data = {}
            if row and row[0]:
                try: current_data = json.loads(row[0])
                except: current_data = {"url": row[0]}
            
            if isinstance(value, dict):
                current_data.update(value)
            else:
                current_data['url'] = value
            
            current_data['updated_at'] = int(time.time())
            
            final_json = json.dumps(current_data, ensure_ascii=False)
            conn.execute("UPDATE user_books SET value=? WHERE username=? AND book_key=?", (final_json, u, key))
            conn.commit()
            return {"status": "success", "message": f"Updated: {key}"}
        except:
            return self.insert(key, value, username=username)

    # === 3. æ ¸å¿ƒè¯»å–é€»è¾‘ (è‡ªåŠ¨è§£åŒ… - å…¼å®¹æ—§æ¥å£) ===
    def get_val(self, key, username=None):
        """é»˜è®¤åªè¿”å› URL å­—ç¬¦ä¸²ï¼Œä¿è¯æ—§ä»£ç ä¸å´©"""
        # [å…³é”®ä¿®å¤] å¯¹äº :meta ç»“å°¾çš„ keyï¼Œç›´æ¥è¿”å›åŸå§‹å€¼
        if key.endswith(':meta'):
            u = username or get_current_user()
            try:
                conn = get_db()
                row = conn.execute("SELECT value FROM user_books WHERE username=? AND book_key=?", (u, key)).fetchone()
                return row[0] if row and row[0] else None
            except:
                return None
        
        # æ­£å¸¸çš„ä¹¦ç± keyï¼Œä½¿ç”¨è§£åŒ…é€»è¾‘
        full = self.get_full_data(key, username=username)
        return full.get('url') if full else None

    def get_full_data(self, key, username=None):
        """æ–°æ¥å£ï¼šè·å–å®Œæ•´å…ƒæ•°æ®"""
        u = username or get_current_user()
        try:
            conn = get_db()
            row = conn.execute("SELECT value FROM user_books WHERE username=? AND book_key=?", (u, key)).fetchone()
            if row and row[0]:
                try:
                    d = json.loads(row[0])
                    return d if isinstance(d, dict) else {"url": row[0]}
                except: return {"url": row[0]}
            return None
        except: return None

    # === 4. åˆ—è¡¨æŸ¥è¯¢ (è‡ªåŠ¨è§£åŒ…) ===
    def list_all(self):
        u = get_current_user()
        try:
            conn = get_db()
            cursor = conn.execute("SELECT book_key, value FROM user_books WHERE username=? AND book_key NOT LIKE '@%' ORDER BY updated_at DESC", (u,))
            result = {}
            for row in cursor.fetchall():
                k, v_str = row[0], row[1]
                if k.endswith(':meta'): continue
                try:
                    obj = json.loads(v_str)
                    if isinstance(obj, dict): result[k] = obj
                    else: result[k] = {"url": v_str}
                except: result[k] = {"url": v_str}
            return {"status": "success", "data": result}
        except Exception as e: return {"status": "error", "message": str(e)}

    # ... (find, remove, rename_key, rollback, add_version, get_versions ä¿æŒä¸å˜) ...
    def find(self, term):
        u = get_current_user()
        try:
            t = f'%{term}%'
            conn = get_db()
            cursor = conn.execute("SELECT book_key, value FROM user_books WHERE username=? AND (book_key LIKE ? OR value LIKE ?)", (u, t, t))
            result = {}
            for row in cursor.fetchall():
                k, v_str = row[0], row[1]
                if k.endswith(':meta'): continue
                try:
                    obj = json.loads(v_str)
                    result[k] = obj if isinstance(obj, dict) else {"url": obj}
                except: result[k] = {"url": v_str}
            return {"status": "success", "data": result}
        except Exception as e: return {"status": "error", "message": str(e)}

    def remove(self, key):
        u = get_current_user()
        try:
            with get_db() as conn:
                conn.execute("DELETE FROM user_books WHERE username=? AND book_key=?", (u, key))
                conn.commit()
            return {"status": "success", "message": f"Removed: {key}"}
        except Exception as e: return {"status": "error", "message": str(e)}

    def rename_key(self, old_key, new_key):
        if not old_key or not new_key: return {"status": "error", "message": "Key cannot be empty"}
        u = get_current_user()
        try:
            with get_db() as conn:
                exists = conn.execute("SELECT 1 FROM user_books WHERE username=? AND book_key=?", (u, new_key)).fetchone()
                if exists: return {"status": "error", "message": f"ç›®æ ‡ Key [{new_key}] å·²å­˜åœ¨"}
                conn.execute("UPDATE user_books SET book_key=? WHERE username=? AND book_key=?", (new_key, u, old_key))
                conn.execute("UPDATE user_books SET book_key=? WHERE username=? AND book_key=?", (f"{new_key}:meta", u, f"{old_key}:meta"))
                conn.execute("UPDATE book_history SET book_key=? WHERE username=? AND book_key=?", (new_key, u, old_key))
                conn.commit()
            
            tags_data = tag_manager.load(u)
            if old_key in tags_data:
                tags_data[new_key] = tags_data.pop(old_key)
                tag_manager.save(tags_data, u)
            
            updates_data = update_manager.load(u)
            if old_key in updates_data:
                updates_data[new_key] = updates_data.pop(old_key)
                update_manager.save(updates_data, u)

            bl_data = booklist_manager.load(u)
            changed = False
            for lid in bl_data:
                for b in bl_data[lid].get('books', []):
                    if b['key'] == old_key:
                        b['key'] = new_key
                        changed = True
            if changed: booklist_manager.save(bl_data, u)

            return {"status": "success", "message": f"å·²å°† [{old_key}] é‡å‘½åä¸º [{new_key}]"}
        except Exception as e: return {"status": "error", "message": str(e)}

    def rollback(self): return {"status": "error", "message": "Use version history instead"}
    
    def add_version(self, key, value):
        self._ensure_history_table()
        u = get_current_user()
        try:
            with get_db() as conn:
                conn.execute("INSERT INTO book_history (username, book_key, value) VALUES (?, ?, ?)", (u, key, value))
                conn.execute(f'''
                    DELETE FROM book_history 
                    WHERE id IN (
                        SELECT id FROM book_history 
                        WHERE username=? AND book_key=? 
                        ORDER BY recorded_at DESC 
                        LIMIT -1 OFFSET 5
                    )
                ''', (u, key))
                conn.commit()
            return True
        except Exception as e: return False

    def get_versions(self, key):
        self._ensure_history_table()
        u = get_current_user()
        try:
            with get_db() as conn:
                cursor = conn.execute("SELECT value, recorded_at FROM book_history WHERE username=? AND book_key=? ORDER BY recorded_at DESC", (u, key))
                return [{"value": row[0], "time": row[1]} for row in cursor.fetchall()]
        except: return []

class UpdateRecordManager:
    """ç®¡ç†è‡ªåŠ¨è¿½æ›´çš„æ•°æ®åº“æ“ä½œ"""
    def __init__(self):
        self._ensure_table() # [ä¿®å¤] åˆå§‹åŒ–æ—¶è‡ªåŠ¨å»ºè¡¨
        pass

    def _ensure_table(self):
        """ç¡®ä¿ book_updates è¡¨å­˜åœ¨"""
        try:
            # è¿™é‡Œè°ƒç”¨ get_db() å¯èƒ½ä¼šå› ä¸ºæ²¡æœ‰ request context æŠ¥é”™
            # ä½†æˆ‘ä»¬åœ¨ __init__ é‡Œè°ƒç”¨æ—¶é€šå¸¸æ˜¯åœ¨ import é˜¶æ®µï¼Œä¹Ÿä¸è¡Œ
            # æ‰€ä»¥åªèƒ½æŠŠå»ºè¡¨é€»è¾‘é€šè¿‡ç‹¬ç«‹çš„è¿æ¥æ¥åšï¼Œæˆ–è€…æ¯æ¬¡æ“ä½œå‰æ£€æŸ¥
            
            # ä½¿ç”¨ç‹¬ç«‹è¿æ¥å»ºè¡¨ï¼Œé˜²æ­¢ Flask context æŠ¥é”™
            conn = sqlite3.connect(DB_PATH) 
            conn.execute('''CREATE TABLE IF NOT EXISTS book_updates (
                            book_key TEXT PRIMARY KEY,
                            username TEXT NOT NULL,
                            toc_url TEXT,
                            last_local_id INTEGER DEFAULT 0,
                            last_remote_id INTEGER DEFAULT 0,
                            has_update BOOLEAN DEFAULT 0,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )''')
            conn.commit()
            conn.close()
        except Exception as e:
            print(f"[DB Init] è‡ªåŠ¨å»ºè¡¨å¤±è´¥ (ä¸ç”¨æ‹…å¿ƒï¼Œå¯èƒ½æ˜¯æ–‡ä»¶é”å®š): {e}")

    def subscribe(self, username, book_key, toc_url, current_id):
        """å¼€å¯è¿½æ›´"""
        # ä¸ºäº†åŒé‡ä¿é™©ï¼Œå¦‚æœ __init__ å¤±è´¥äº†ï¼Œè¿™é‡Œå†è¯•ä¸€æ¬¡
        try: self._ensure_table()
        except: pass
        
        with get_db() as conn:
            conn.execute("""
                INSERT OR REPLACE INTO book_updates 
                (book_key, username, toc_url, last_local_id, has_update, updated_at)
                VALUES (?, ?, ?, ?, 0, CURRENT_TIMESTAMP)
            """, (book_key, username, toc_url, current_id))
            conn.commit()

    def unsubscribe(self, book_key):
        """å–æ¶ˆè¿½æ›´"""
        with get_db() as conn:
            conn.execute("DELETE FROM book_updates WHERE book_key=?", (book_key,))
            conn.commit()

    def is_subscribed(self, book_key):
        with get_db() as conn:
            row = conn.execute("SELECT 1 FROM book_updates WHERE book_key=?", (book_key,)).fetchone()
            return bool(row)

    # [æ–°å¢] è·å–æ›´è¯¦ç»†çš„çŠ¶æ€ï¼Œä¾›å‰ç«¯æ¸²æŸ“çº¢ç‚¹
    def get_book_status(self, book_key):
        with get_db() as conn:
            row = conn.execute("SELECT has_update, last_remote_id FROM book_updates WHERE book_key=?", (book_key,)).fetchone()
            if row:
                return {"subscribed": True, "has_update": bool(row['has_update']), "remote_id": row['last_remote_id']}
            return {"subscribed": False, "has_update": False}
    
    def update_status(self, book_key, remote_id, has_u):
        with get_db() as conn:
            conn.execute("UPDATE book_updates SET last_remote_id=?, has_update=?, updated_at=CURRENT_TIMESTAMP WHERE book_key=?", 
                         (remote_id, 1 if has_u else 0, book_key))
            conn.commit()

    def get_all_updates(self, username):
        """è·å–æŸç”¨æˆ·æ‰€æœ‰æœ‰æ›´æ–°çš„ä¹¦"""
        with get_db() as conn:
            rows = conn.execute("SELECT book_key FROM book_updates WHERE username=? AND has_update=1", (username,)).fetchall()
            return [r[0] for r in rows]

    # [æ–°å¢] è·å–æŸç”¨æˆ·æ‰€æœ‰å·²è®¢é˜…çš„ä¹¦ (ç”¨äº api_get_updates_status ç¡®å®šæ£€æŸ¥èŒƒå›´)
    def get_all_subscribed(self, username):
        """è·å–æŸç”¨æˆ·æ‰€æœ‰å¼€å¯äº†è‡ªåŠ¨è¿½æ›´çš„ä¹¦"""
        with get_db() as conn:
            rows = conn.execute("SELECT book_key FROM book_updates WHERE username=?", (username,)).fetchall()
            return [r[0] for r in rows]

    def get_all_tasks(self):
        """åå°çº¿ç¨‹ç”¨ï¼šè·å–æ‰€æœ‰ä»»åŠ¡"""
        # æ³¨æ„ï¼šè¿™é‡Œå¯èƒ½æ˜¯åœ¨ request ä¸Šä¸‹æ–‡ä¹‹å¤–è°ƒç”¨çš„ï¼Œæ‰€ä»¥ä¸èƒ½ç”¨ get_db()ï¼Œè¦æ‰‹åŠ¨è¿
        # ä½†å› ä¸º DB æ˜¯æŒ‰ç”¨æˆ·åˆ†æ–‡ä»¶çš„ï¼Œæˆ‘ä»¬è¿™é‡Œéœ€è¦éå†æ‰€æœ‰ç”¨æˆ·çš„ DB æ–‡ä»¶ï¼Ÿ
        # ç®€åŒ–ç­–ç•¥ï¼šç›®å‰å•æœºç‰ˆå¾ˆå¤šé€»è¾‘è¿˜æ²¡åšå®Œå…¨çš„ç”¨æˆ·éš”ç¦»ï¼Œæˆ‘ä»¬å…ˆåªå¤„ç†ä¸»DB
        # æˆ–è€…ä¿®æ­£é€»è¾‘ï¼šschedule_auto_check è´Ÿè´£éå†æ–‡ä»¶
        pass

# ==========================================
# 5. æ–‡ä»¶/ç¼“å­˜ç®¡ç†
# ==========================================
class OfflineBookManager:
    def __init__(self):
        self.offline_dir = os.path.join(USER_DATA_DIR, "offline_books")
        if not os.path.exists(self.offline_dir): os.makedirs(self.offline_dir)
    def _get_book_path(self, k): return os.path.join(self.offline_dir, f"{k}.json")
    def is_downloaded(self, k): return os.path.exists(self._get_book_path(k))
    def save_book(self, k, d): 
        with open(self._get_book_path(k), 'w', encoding='utf-8') as f: json.dump(d, f)
    def get_chapter(self, k, u):
        if not self.is_downloaded(k): return None
        try:
            with open(self._get_book_path(k), 'r') as f: return json.load(f).get(u)
        except: return None
import redis
import threading
class CacheManager:
    def __init__(self, ttl=604800): 
        self.cache_dir = CACHE_DIR
        self.ttl = ttl 
    def _get_filename(self, url):
        hash_object = hashlib.md5(url.encode('utf-8'))
        return os.path.join(self.cache_dir, hash_object.hexdigest() + ".json")
    def get(self, url):
        fp = self._get_filename(url)
        if not os.path.exists(fp): return None
        if time.time() - os.path.getmtime(fp) > self.ttl: return None
        try:
            with open(fp, 'r', encoding='utf-8') as f: return json.load(f)
        except: return None
    def set(self, url, data):
        fp = self._get_filename(url)
        # [ä¿®å¤] ä¿®æ­£è¯­æ³•é”™è¯¯ï¼Œæ‹†åˆ†ä¸ºæ ‡å‡†å†™æ³•
        try:
            with open(fp, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False)
        except Exception as e:
            print(f"[Cache] Write Error: {e}")
            
    def cleanup_expired(self):
        now = time.time(); count = 0; size = 0
        for f in os.listdir(self.cache_dir):
            fp = os.path.join(self.cache_dir, f)
            if os.path.isfile(fp) and now - os.path.getmtime(fp) > self.ttl:
                try: size += os.path.getsize(fp); os.remove(fp); count += 1
                except: pass
        return count, size / (1024*1024)

class DownloadManager:
    def __init__(self):
        self.downloads = {}
        self.executor = ThreadPoolExecutor(max_workers=5)
    def start_download(self, book_name, chapters, crawler_instance):
        task_id = hashlib.md5((book_name + str(time.time())).encode()).hexdigest()
        self.downloads[task_id] = {'book_name': book_name, 'total': len(chapters), 'current': 0, 'status': 'running', 'filename': f"{re.sub(r'[\\/*?:|<>]', '', book_name)}.txt"}
        threading.Thread(target=self._master_worker, args=(task_id, chapters, crawler_instance)).start()
        return task_id
    def _master_worker(self, task_id, chapters, crawler):
        task = self.downloads[task_id]
        results = [None] * len(chapters)
        with ThreadPoolExecutor(max_workers=8) as pool:
            future_to_index = {pool.submit(self._fetch_worker, c['url'], crawler): i for i, c in enumerate(chapters)}
            for future in as_completed(future_to_index):
                idx = future_to_index[future]
                try:
                    c, t = future.result()
                    # [ä¿®å¤] æ ¼å¼åŒ–
                    results[idx] = f"\n\n=== {t} ===\n\n" + '\n'.join(c)
                except Exception as e: 
                    results[idx] = f"\n\nError: {e}"
                task['current'] += 1
        try:
            with open(os.path.join(DL_DIR, task['filename']), 'w', encoding='utf-8') as f:
                f.write(f"=== {task['book_name']} ===\n")
                for r in results: f.write(r or "")
            task['status'] = 'completed'
        except Exception as e: task['status'] = 'error'; task['error_msg'] = str(e)
    def _fetch_worker(self, url, crawler):
        data = crawler.run(url)
        if data and data['content']: return data['content'], data.get('title', '')
        raise Exception("Empty")
    def get_status(self, tid): return self.downloads.get(tid)

# ==========================================
# å¯¼å‡ºç®¡ç†å™¨ (TXT/EPUB) - æ”¯æŒæ–­ç‚¹ç»­ä¼ 
# ==========================================
import threading
class ExportManager:
    def __init__(self):
        self.exports = {}  # å†…å­˜ä¸­çš„æ´»è·ƒä»»åŠ¡
        self.task_file = os.path.join(USER_DATA_DIR, 'export_tasks.json')
        self._load_tasks()
        
    def _load_tasks(self):
        """åŠ è½½æŒä¹…åŒ–çš„ä»»åŠ¡"""
        if os.path.exists(self.task_file):
            try:
                with open(self.task_file, 'r', encoding='utf-8') as f:
                    saved_tasks = json.load(f)
                    # åŠ è½½æ‰€æœ‰ä»»åŠ¡ï¼ˆåŒ…æ‹¬å·²å®Œæˆçš„ï¼Œç”¨äºå†å²è®°å½•ï¼‰
                    for task_id, task in saved_tasks.items():
                        if task.get('status') not in ['completed', 'error']:
                            task['status'] = 'paused'  # æœªå®Œæˆçš„æ ‡è®°ä¸ºæš‚åœ
                        # æ·»åŠ åˆ›å»ºæ—¶é—´ï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
                        if 'created_at' not in task:
                            task['created_at'] = time.strftime('%Y-%m-%d %H:%M:%S')
                        self.exports[task_id] = task
            except Exception as e:
                print(f"[ExportManager] åŠ è½½ä»»åŠ¡å¤±è´¥: {e}")
    
    def _save_task(self, task_id):
        """ä¿å­˜å•ä¸ªä»»åŠ¡åˆ°æ–‡ä»¶"""
        try:
            all_tasks = {}
            if os.path.exists(self.task_file):
                with open(self.task_file, 'r', encoding='utf-8') as f:
                    all_tasks = json.load(f)
            
            all_tasks[task_id] = self.exports[task_id]
            
            with open(self.task_file, 'w', encoding='utf-8') as f:
                json.dump(all_tasks, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"[ExportManager] ä¿å­˜ä»»åŠ¡å¤±è´¥: {e}")
    
    def find_unfinished_task(self, book_name):
        """æŸ¥æ‰¾æŒ‡å®šä¹¦ç±çš„æœªå®Œæˆä»»åŠ¡"""
        for task_id, task in self.exports.items():
            if task.get('book_name') == book_name and task.get('status') == 'paused':
                return task_id
        return None
    
    def start_export(self, book_name, chapters, crawler_instance, export_format='txt', metadata=None, resume_task_id=None, delay=0.5):
        """å¯åŠ¨å¯¼å‡ºä»»åŠ¡ï¼ˆæ”¯æŒç»­ä¼ ï¼‰
        
        Args:
            delay: æ¯ä¸ªç« èŠ‚æŠ“å–åçš„å»¶è¿Ÿæ—¶é—´ï¼ˆç§’ï¼‰ï¼Œé»˜è®¤ 0.5 ç§’ï¼Œé˜²æ­¢è¢«å°
        """
        if resume_task_id and resume_task_id in self.exports:
            # æ–­ç‚¹ç»­ä¼ 
            task_id = resume_task_id
            task = self.exports[task_id]
            task['status'] = 'running'
            task['delay'] = delay  # æ›´æ–°å»¶è¿Ÿè®¾ç½®
            print(f"[Export] ç»­ä¼ ä»»åŠ¡ {task_id}ï¼Œå·²å®Œæˆ {len(task.get('completed_chapters', []))} ç« ")
        else:
            # æ–°ä»»åŠ¡
            task_id = hashlib.md5((book_name + str(time.time())).encode()).hexdigest()
            safe_name = re.sub(r'[\\/*?:|<>]', '', book_name)
            filename = f"{safe_name}.{export_format}"
            
            task = {
                'book_name': book_name,
                'total': len(chapters),
                'current': 0,
                'status': 'running',
                'format': export_format,
                'filename': filename,
                'metadata': metadata or {},
                'chapters': [{'name': c.get('name', f'ç¬¬{i+1}ç« '), 'url': c['url']} for i, c in enumerate(chapters)],
                'completed_chapters': [],  # å·²å®Œæˆçš„ç« èŠ‚ç´¢å¼•
                'results': {},  # å·²æŠ“å–çš„ç« èŠ‚å†…å®¹ {index: {title, content}}
                'delay': delay,  # æŠ“å–å»¶è¿Ÿï¼ˆç§’ï¼‰
                'paused': False,  # æš‚åœæ ‡å¿—
                'created_at': time.strftime('%Y-%m-%d %H:%M:%S')  # åˆ›å»ºæ—¶é—´
            }
            self.exports[task_id] = task
        
        self._save_task(task_id)
        threading.Thread(target=self._export_worker, args=(task_id, crawler_instance)).start()
        return task_id
    
    def pause_export(self, task_id):
        """æš‚åœå¯¼å‡ºä»»åŠ¡"""
        if task_id in self.exports:
            self.exports[task_id]['paused'] = True
            self.exports[task_id]['status'] = 'paused'
            self._save_task(task_id)
            return True
        return False
    
    def resume_export(self, task_id, crawler_instance):
        """æ¢å¤æš‚åœçš„å¯¼å‡ºä»»åŠ¡"""
        if task_id in self.exports:
            task = self.exports[task_id]
            if task.get('status') == 'paused':
                task['status'] = 'running'
                task['paused'] = False
                self._save_task(task_id)
                threading.Thread(target=self._export_worker, args=(task_id, crawler_instance)).start()
                return True
        return False
    
    def _export_worker(self, task_id, crawler):
        """å¯¼å‡ºå·¥ä½œçº¿ç¨‹ï¼ˆæ”¯æŒè·³è¿‡å·²å®Œæˆç« èŠ‚å’Œæš‚åœï¼‰"""
        task = self.exports[task_id]
        chapters = task['chapters']
        completed = set(task.get('completed_chapters', []))
        results = task.get('results', {})
        delay = task.get('delay', 0.5)  # è·å–å»¶è¿Ÿè®¾ç½®
        
        # è½¬æ¢ results çš„ key ä¸ºæ•´æ•°ï¼ˆJSON ä¿å­˜åä¼šå˜æˆå­—ç¬¦ä¸²ï¼‰
        results = {int(k): v for k, v in results.items()}
        
        # [æ–°å¢] å°è¯•é›†ç¾¤å¹¶è¡Œçˆ¬å–
        use_cluster = cluster_manager.use_redis and len(cluster_manager.get_active_nodes()) > 0
        
        if use_cluster:
            print(f"[Export] ğŸš€ å¯ç”¨é›†ç¾¤å¹¶è¡Œçˆ¬å–æ¨¡å¼ï¼ˆ{len(cluster_manager.get_active_nodes())} ä¸ªèŠ‚ç‚¹åœ¨çº¿ï¼‰")
            results = self._cluster_parallel_fetch(task_id, chapters, completed, results, delay)
        else:
            print(f"[Export] ğŸ¢ ä½¿ç”¨æœ¬åœ°å¹¶å‘æ¨¡å¼ï¼ˆé›†ç¾¤ä¸å¯ç”¨ï¼‰")
            # åŸæœ‰çš„æœ¬åœ°å¹¶å‘é€»è¾‘
            pending_chapters = [(i, c) for i, c in enumerate(chapters) if i not in completed]
            
            with ThreadPoolExecutor(max_workers=3) as pool:
                future_to_index = {
                    pool.submit(self._fetch_chapter, c['url'], crawler): i 
                    for i, c in pending_chapters
                }
                
                for future in as_completed(future_to_index):
                    # æ£€æŸ¥æš‚åœæ ‡å¿—
                    if task.get('paused'):
                        print(f"[Export] ä»»åŠ¡ {task_id} å·²æš‚åœ")
                        # å–æ¶ˆæ‰€æœ‰æœªå®Œæˆçš„ä»»åŠ¡
                        for f in future_to_index:
                            f.cancel()
                        break
                    
                    idx = future_to_index[future]
                    try:
                        results[idx] = future.result()
                        completed.add(idx)
                    except Exception as e:
                        results[idx] = {
                            'title': chapters[idx].get('name', f'ç¬¬{idx+1}ç« '), 
                            'content': f'æŠ“å–å¤±è´¥: {str(e)}'
                        }
                        completed.add(idx)
                    
                    # æ›´æ–°è¿›åº¦
                    task['current'] = len(completed)
                    task['completed_chapters'] = list(completed)
                    task['results'] = results
                    
                    # æ¯å®Œæˆä¸€ç« æ·»åŠ å»¶è¿Ÿï¼Œé˜²æ­¢è¢«å°
                    if delay > 0:
                        import random
                        actual_delay = delay * random.uniform(0.8, 1.2)  # éšæœºæµ®åŠ¨ Â±20%
                        time.sleep(actual_delay)
                    
                    # æ¯å®Œæˆ 10 ç« ä¿å­˜ä¸€æ¬¡
                    if len(completed) % 10 == 0:
                        self._save_task(task_id)
        
        # å¦‚æœè¢«æš‚åœï¼Œä¸ç”Ÿæˆæ–‡ä»¶
        if task.get('paused'):
            self._save_task(task_id)
            return
        
        # ç”Ÿæˆæ–‡ä»¶
        try:
            # æŒ‰ç´¢å¼•æ’åºç»“æœ
            sorted_results = [results[i] for i in range(len(chapters))]
            
            if task['format'] == 'txt':
                self._generate_txt(task, sorted_results)
            elif task['format'] == 'epub':
                self._generate_epub(task, sorted_results)
            
            task['status'] = 'completed'
            # å®Œæˆåæ¸…ç† results ä»¥èŠ‚çœç©ºé—´
            task.pop('results', None)
        except Exception as e:
            task['status'] = 'error'
            task['error_msg'] = str(e)
        
        self._save_task(task_id)
    
    def _fetch_chapter(self, url, crawler):
        """æŠ“å–å•ä¸ªç« èŠ‚"""
        data = crawler.run(url)
        if data and data.get('content'):
            return {
                'title': data.get('title', 'æ— æ ‡é¢˜'),
                'content': '\n'.join(data['content']) if isinstance(data['content'], list) else data['content']
            }
        raise Exception("ç« èŠ‚å†…å®¹ä¸ºç©º")
    
    def _cluster_parallel_fetch(self, task_id, chapters, completed, results, delay):
        """é›†ç¾¤å¹¶è¡Œçˆ¬å–ç« èŠ‚"""
        import uuid as uuid_lib
        import json
        import time
        from spider_core import _remote_request
        
        task = self.exports[task_id]
        pending_chapters = [(i, c) for i, c in enumerate(chapters) if i not in completed]
        
        if not pending_chapters:
            return results
        
        print(f"[Cluster] ğŸ“¦ å¾…çˆ¬å–ç« èŠ‚: {len(pending_chapters)} ç« ")
        
        # æ‰¹é‡æ¨é€ä»»åŠ¡åˆ°é˜Ÿåˆ—
        task_mapping = {}  # {task_uuid: chapter_index}
        
        for idx, chapter in pending_chapters:
            # æ£€æŸ¥æš‚åœæ ‡å¿—
            if task.get('paused'):
                print(f"[Cluster] ä»»åŠ¡ {task_id} å·²æš‚åœï¼Œåœæ­¢æ¨é€")
                break
                
            task_uuid = str(uuid_lib.uuid4())
            task_package = {
                "id": task_uuid,
                "endpoint": "run",
                "payload": {"url": chapter['url']},
                "timestamp": time.time()
            }
            
            try:
                cluster_manager.r.lpush("crawler:queue:pending", json.dumps(task_package))
                task_mapping[task_uuid] = idx
                print(f"[Cluster] âœ… å·²æ¨é€: ç¬¬{idx+1}ç«  ({chapter.get('name', 'æ— æ ‡é¢˜')})")
            except Exception as e:
                print(f"[Cluster] âŒ æ¨é€å¤±è´¥: {e}")
                # å¤±è´¥çš„ç« èŠ‚æ ‡è®°ä¸ºé”™è¯¯
                results[idx] = {
                    'title': chapter.get('name', f'ç¬¬{idx+1}ç« '),
                    'content': f'æ¨é€å¤±è´¥: {str(e)}'
                }
                completed.add(idx)
        
        print(f"[Cluster] â³ ç­‰å¾…èŠ‚ç‚¹å¤„ç† {len(task_mapping)} ä¸ªä»»åŠ¡...")
        
        # è½®è¯¢ç­‰å¾…ç»“æœ
        start_time = time.time()
        timeout = 300  # 5åˆ†é’Ÿè¶…æ—¶
        check_interval = 0.5  # æ¯0.5ç§’æ£€æŸ¥ä¸€æ¬¡
        
        while task_mapping and (time.time() - start_time < timeout):
            # æ£€æŸ¥æš‚åœæ ‡å¿—
            if task.get('paused'):
                print(f"[Cluster] ä»»åŠ¡ {task_id} å·²æš‚åœ")
                break
            
            completed_tasks = []
            
            for task_uuid, idx in list(task_mapping.items()):
                result_key = f"crawler:result:{task_uuid}"
                res = cluster_manager.r.get(result_key)
                
                if res:
                    # è§£æç»“æœ
                    json_res = json.loads(res)
                    cluster_manager.r.delete(result_key)  # è¯»å®Œå³ç„š
                    
                    if json_res.get('status') == 'success':
                        data = json_res.get('data')
                        if data and data.get('content'):
                            results[idx] = {
                                'title': data.get('title', 'æ— æ ‡é¢˜'),
                                'content': '\n'.join(data['content']) if isinstance(data['content'], list) else data['content']
                            }
                            completed.add(idx)
                            print(f"[Cluster] âœ… å®Œæˆ: ç¬¬{idx+1}ç«  (Worker: {json_res.get('worker_uuid', 'unknown')[:8]}...)")
                        else:
                            results[idx] = {
                                'title': chapters[idx].get('name', f'ç¬¬{idx+1}ç« '),
                                'content': 'çˆ¬å–ç»“æœä¸ºç©º'
                            }
                            completed.add(idx)
                    else:
                        # å¤±è´¥çš„ç« èŠ‚
                        results[idx] = {
                            'title': chapters[idx].get('name', f'ç¬¬{idx+1}ç« '),
                            'content': f'çˆ¬å–å¤±è´¥: {json_res.get("msg", "æœªçŸ¥é”™è¯¯")}'
                        }
                        completed.add(idx)
                        print(f"[Cluster] âŒ å¤±è´¥: ç¬¬{idx+1}ç« ")
                    
                    completed_tasks.append(task_uuid)
            
            # ç§»é™¤å·²å®Œæˆçš„ä»»åŠ¡
            for task_uuid in completed_tasks:
                del task_mapping[task_uuid]
            
            # æ›´æ–°è¿›åº¦
            if completed_tasks:
                task['current'] = len(completed)
                task['completed_chapters'] = list(completed)
                task['results'] = results
                
                # æ¯å®Œæˆ 10 ç« ä¿å­˜ä¸€æ¬¡
                if len(completed) % 10 == 0:
                    self._save_task(task_id)
            
            # å¦‚æœè¿˜æœ‰å¾…å¤„ç†ä»»åŠ¡ï¼Œç­‰å¾…ä¸€ä¼šå†æ£€æŸ¥
            if task_mapping:
                time.sleep(check_interval)
        
        # è¶…æ—¶æˆ–æš‚åœåï¼Œæ ‡è®°å‰©ä½™ç« èŠ‚ä¸ºè¶…æ—¶
        if task_mapping:
            print(f"[Cluster] âš ï¸ {len(task_mapping)} ä¸ªç« èŠ‚è¶…æ—¶æˆ–è¢«æš‚åœ")
            for task_uuid, idx in task_mapping.items():
                if idx not in completed:
                    results[idx] = {
                        'title': chapters[idx].get('name', f'ç¬¬{idx+1}ç« '),
                        'content': 'çˆ¬å–è¶…æ—¶æˆ–è¢«æš‚åœ'
                    }
                    completed.add(idx)
        
        print(f"[Cluster] ğŸ‰ é›†ç¾¤çˆ¬å–å®Œæˆ: {len(completed)}/{len(chapters)} ç« ")
        return results
    
    def _generate_txt(self, task, results):
        """ç”Ÿæˆ TXT æ–‡ä»¶"""
        filepath = os.path.join(DL_DIR, task['filename'])
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(f"{task['book_name']}\n")
            f.write("=" * 50 + "\n\n")
            
            for chapter in results:
                if chapter:
                    f.write(f"\n\n{'=' * 50}\n")
                    f.write(f"{chapter['title']}\n")
                    f.write(f"{'=' * 50}\n\n")
                    f.write(chapter['content'])
                    f.write("\n\n")
    
    def _generate_epub(self, task, results):
        """ç”Ÿæˆ EPUB æ–‡ä»¶ï¼ˆéœ€è¦ ebooklibï¼‰"""
        try:
            from ebooklib import epub
        except ImportError:
            raise Exception("éœ€è¦å®‰è£… ebooklib åº“: pip install ebooklib")
        
        book = epub.EpubBook()
        metadata = task.get('metadata', {})
        
        # è®¾ç½®å…ƒæ•°æ®
        book.set_identifier(hashlib.md5(task['book_name'].encode()).hexdigest())
        book.set_title(task['book_name'])
        book.set_language(metadata.get('language', 'zh'))
        
        if metadata.get('author'):
            book.add_author(metadata['author'])
        
        if metadata.get('description'):
            book.add_metadata('DC', 'description', metadata['description'])
        
        # æ·»åŠ å°é¢ï¼ˆå¦‚æœæä¾›ï¼‰
        if metadata.get('cover_path') and os.path.exists(metadata['cover_path']):
            with open(metadata['cover_path'], 'rb') as f:
                book.set_cover('cover.jpg', f.read())
        
        # åˆ›å»ºç« èŠ‚
        chapters_epub = []
        spine = ['nav']
        
        for i, chapter_data in enumerate(results):
            if not chapter_data:
                continue
                
            chapter = epub.EpubHtml(
                title=chapter_data['title'],
                file_name=f'chapter_{i+1}.xhtml',
                lang='zh'
            )
            
            # æ·»åŠ ç« èŠ‚å†…å®¹
            content = f'<h1>{chapter_data["title"]}</h1>'
            content += '<div>' + chapter_data['content'].replace('\n', '</p><p>') + '</div>'
            chapter.content = content
            
            book.add_item(chapter)
            chapters_epub.append(chapter)
            spine.append(chapter)
        
        # æ·»åŠ ç›®å½•
        book.toc = tuple(chapters_epub)
        
        # æ·»åŠ å¯¼èˆªæ–‡ä»¶
        book.add_item(epub.EpubNcx())
        book.add_item(epub.EpubNav())
        
        # è®¾ç½® spine
        book.spine = spine
        
        # å†™å…¥æ–‡ä»¶
        filepath = os.path.join(DL_DIR, task['filename'])
        epub.write_epub(filepath, book, {})
    
    def get_status(self, task_id):
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.exports.get(task_id)
class ClusterManager:
    def __init__(self):
        self.redis_url = os.environ.get('REDIS_URL')
        self.use_redis = False
        self.nodes = {} # å†…å­˜ fallback
        self.r = None

        if self.redis_url:
            try:
                self.r = redis.from_url(self.redis_url, decode_responses=True)
                self.r.ping() # æµ‹è¯•è¿æ¥
                self.use_redis = True
                print("âœ… [Cluster] Redis è¿æ¥æˆåŠŸï¼Œé›†ç¾¤æ¨¡å¼å·²å°±ç»ª")
            except Exception as e:
                print(f"âš ï¸ [Cluster] Redis è¿æ¥å¤±è´¥ ({e})ï¼Œé™çº§ä¸ºå†…å­˜æ¨¡å¼")
        else:
            print("â„¹ï¸ [Cluster] æœªé…ç½® REDIS_URLï¼Œä½¿ç”¨å†…å­˜æ¨¡å¼ (é‡å¯åèŠ‚ç‚¹ä¿¡æ¯ä¸¢å¤±)")
    # managers.py -> ClusterManager ç±»

    # ... (å‰é¢çš„æ–¹æ³•ä¿æŒä¸å˜) ...
    def record_latency(self, url, worker_uuid, latency_ms):
        """
        è‡ªé€‚åº”æƒé‡è®°å½•ï¼šEWMAå¹³æ»‘ + å¼‚å¸¸å€¼è¿‡æ»¤ + ç†”æ–­ä¿æŠ¤
        Redis Key: crawler:latency:{domain}  (Hashç»“æ„)
        """
        if not self.use_redis or not url: return
        
        try:
            from urllib.parse import urlparse
            import statistics
            
            domain = urlparse(url).netloc
            if not domain: return

            key = f"crawler:latency:{domain}"
            
            # === 1. å¼‚å¸¸å€¼è¿‡æ»¤ï¼ˆé˜²æ­¢å•æ¬¡è¶…æ—¶æ±¡æŸ“æƒé‡ï¼‰ ===
            # è·å–è¯¥åŸŸåä¸‹æ‰€æœ‰èŠ‚ç‚¹çš„å»¶è¿Ÿï¼Œç”¨äºç»Ÿè®¡åˆ†æ
            all_latencies_raw = self.r.hgetall(key)
            all_latencies = [float(v) for v in all_latencies_raw.values() if v]
            
            # å¦‚æœæœ‰è¶³å¤Ÿæ ·æœ¬ï¼ˆè‡³å°‘3ä¸ªèŠ‚ç‚¹ï¼‰ï¼Œè¿›è¡Œå¼‚å¸¸æ£€æµ‹
            if len(all_latencies) >= 3:
                mean = statistics.mean(all_latencies)
                try:
                    std = statistics.stdev(all_latencies)
                except:
                    std = mean * 0.3  # å¦‚æœæ ‡å‡†å·®è®¡ç®—å¤±è´¥ï¼Œç”¨30%ä½œä¸ºä¼°è®¡
                
                # æ£€æµ‹å¼‚å¸¸å€¼ï¼šè¶…è¿‡å‡å€¼+3å€æ ‡å‡†å·®è§†ä¸ºå¼‚å¸¸
                threshold = mean + 3 * std
                if latency_ms > threshold:
                    # é’³åˆ¶åˆ°å‡å€¼+2å€æ ‡å‡†å·®ï¼ˆä¿ç•™ä¸€å®šæƒ©ç½šï¼Œä½†ä¸è‡³äºè¿‡åº¦ï¼‰
                    clamped = mean + 2 * std
                    print(f"[Latency] å¼‚å¸¸å€¼è¿‡æ»¤: {domain} {worker_uuid} {latency_ms}ms -> {clamped:.0f}ms (å‡å€¼{mean:.0f})")
                    latency_ms = clamped
            
            # === 2. ç†”æ–­ä¿æŠ¤ï¼ˆè¶…æ—¶ç›´æ¥é™æƒï¼‰ ===
            if latency_ms > 15000:  # è¶…è¿‡15ç§’è§†ä¸ºä¸¥é‡è¶…æ—¶
                print(f"[Latency] ç†”æ–­è§¦å‘: {domain} {worker_uuid} {latency_ms}ms")
                latency_ms = 15000  # é’³åˆ¶åˆ°15ç§’ä¸Šé™
            
            # === 3. EWMAå¹³æ»‘å¤„ç†ï¼ˆæ ¸å¿ƒç®—æ³•ï¼‰ ===
            old_latency_str = self.r.hget(key, worker_uuid)
            
            if old_latency_str:
                old_latency = float(old_latency_str)
                # Î± = 0.15ï¼šå†å²å 85%ï¼Œæ–°æ•°æ®å 15%ï¼ˆä¿å®ˆç­–ç•¥ï¼Œé€‚åˆä¸ç¨³å®šç½‘ç»œï¼‰
                alpha = 0.15
                smoothed_latency = alpha * latency_ms + (1 - alpha) * old_latency
            else:
                # å†·å¯åŠ¨ï¼šç¬¬ä¸€æ¬¡è®°å½•ç›´æ¥ä½¿ç”¨
                smoothed_latency = latency_ms
            
            # === 4. ä¿å­˜åˆ°Redis ===
            self.r.hset(key, worker_uuid, int(smoothed_latency))
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´7å¤©ï¼ˆç½‘ç»œç¯å¢ƒä¼šå˜åŒ–ï¼‰
            self.r.expire(key, 7 * 86400)
            
            # è°ƒè¯•æ—¥å¿—ï¼ˆç”Ÿäº§ç¯å¢ƒå¯æ³¨é‡Šï¼‰
            # print(f"[Latency] {domain} {worker_uuid}: {latency_ms}ms -> {smoothed_latency:.0f}ms")
            
        except Exception as e:
            print(f"[Latency] è®°å½•å¤±è´¥: {e}")
    def _get_speed_coefficient(self, latency):
        """
        å»¶è¿Ÿè½¬æƒé‡ç³»æ•°ï¼ˆå¹³æ»‘æ›²çº¿ï¼Œé¿å…é˜¶æ¢¯å¼è·³å˜ï¼‰
        å…¬å¼: weight = baseline / max(latency, min_latency)
        """
        # é”™è¯¯ç†”æ–­ï¼šä¹‹å‰æŠ¥é”™è¿‡çš„èŠ‚ç‚¹ç»™æä½æƒé‡
        if latency < 0:
            return 0.05
        
        # åŸºå‡†å»¶è¿Ÿï¼š1000msï¼ˆ1ç§’ï¼‰è§†ä¸ºæ ‡å‡†æ°´å¹³
        baseline = 1000
        
        # æœ€å°å»¶è¿Ÿä¿æŠ¤ï¼šé¿å…é™¤é›¶ï¼Œæœ€å°æŒ‰100msè®¡ç®—
        safe_latency = max(latency, 100)
        
        # è®¡ç®—æƒé‡æ¯”ä¾‹ï¼ˆåæ¯”å…³ç³»ï¼šå»¶è¿Ÿè¶Šä½æƒé‡è¶Šé«˜ï¼‰
        ratio = baseline / safe_latency
        
        # é™åˆ¶åœ¨åˆç†åŒºé—´ [0.1, 3.0]
        # - æœ€å¿«èŠ‚ç‚¹ï¼ˆ100msï¼‰æœ€å¤š3å€æƒé‡
        # - æœ€æ…¢èŠ‚ç‚¹ï¼ˆ10s+ï¼‰æœ€ä½0.1å€æƒé‡
        coefficient = max(0.1, min(3.0, ratio))
        
        return coefficient
    def get_speed_multiplier(self, url, worker_uuid):
        """
        [æ–°å¢] è®¡ç®—é€Ÿåº¦åŠ æƒç³»æ•°
        """
        if not self.use_redis or not url: return 1.0
        
        try:
            from urllib.parse import urlparse
            domain = urlparse(url).netloc
            
            # è·å–è¯¥èŠ‚ç‚¹çš„å†å²å»¶è¿Ÿ
            latency_str = self.r.hget(f"crawler:latency:{domain}", worker_uuid)
            
            if not latency_str: return 1.0 # æ— è®°å½•ï¼Œä¸­ç«‹
            
            latency = int(latency_str)
            
            if latency < 0: return 0.1    # ä¹‹å‰æŠ¥é”™è¿‡ï¼Œæåˆ‘
            if latency < 200: return 2.0  # æé€Ÿ
            if latency < 500: return 1.5  # å¾ˆå¿«
            if latency < 1000: return 1.2 # è¿˜è¡Œ
            if latency < 2000: return 1.0 # ä¸€èˆ¬
            if latency < 5000: return 0.8 # æœ‰ç‚¹æ…¢
            return 0.5                    # å¤ªæ…¢äº†
            
        except: return 1.0
    def start_speed_test(self, url):
        """å‘å¸ƒæµ‹é€Ÿå¹¿æ’­"""
        if not self.use_redis: return None
        
        import uuid
        import time
        test_id = str(uuid.uuid4())
        
        # 1. å­˜å…¥æŒ‡ä»¤
        cmd = {
            "id": test_id,
            "url": url,
            "timestamp": time.time()
        }
        
        # å­˜å…¥å…ƒæ•°æ® (ç”¨äºå‰ç«¯åˆ¤æ–­è¿›åº¦)
        active_nodes = self.get_active_nodes()
        meta = {
            "total": len(active_nodes),
            "start_time": time.time(),
            "url": url
        }
        self.r.setex(f"crawler:speedtest:meta:{test_id}", 300, json.dumps(meta))
        
        # 2. å‘å¸ƒå¹¿æ’­æŒ‡ä»¤
        self.r.setex("crawler:cmd:speedtest", 60, json.dumps(cmd))
        
        # 3. [æ ¸å¿ƒä¿®å¤] æ¸…ç†æ‰€æœ‰ç›¸å…³çš„ Redis Key
        # dispatched: è®°å½•å·²é¢†ä»»åŠ¡çš„èŠ‚ç‚¹ (é˜²æ­¢é‡å¤é¢†)
        # results: è®°å½•ç»“æœæ•°æ®
        self.r.delete(f"crawler:speedtest:dispatched:{test_id}")
        self.r.delete(f"crawler:speedtest:results:{test_id}")
        
        return test_id
    def should_dispatch_speedtest(self, worker_uuid):
        """æ£€æŸ¥å¹¶åˆ†å‘æµ‹é€Ÿä»»åŠ¡ (é˜²æŠ–åŠ¨ç‰ˆ)"""
        if not self.use_redis: return None
        
        # 1. è·å–å…¨å±€æŒ‡ä»¤
        cmd_raw = self.r.get("crawler:cmd:speedtest")
        if not cmd_raw: return None
        
        cmd = json.loads(cmd_raw)
        test_id = cmd['id']
        
        # 2. [æ ¸å¿ƒä¿®å¤] æ£€æŸ¥â€œå·²ä¸‹å‘â€åå•ï¼Œè€Œä¸æ˜¯â€œå·²å®Œæˆâ€åå•
        dispatch_key = f"crawler:speedtest:dispatched:{test_id}"
        
        # å¦‚æœè¯¥èŠ‚ç‚¹å·²ç»åœ¨â€œå·²ä¸‹å‘â€åå•é‡Œï¼Œç›´æ¥å¿½ç•¥
        if self.r.sismember(dispatch_key, worker_uuid):
            return None 
            
        # 3. [æ ¸å¿ƒä¿®å¤] ç«‹å³æ ‡è®°ä¸ºâ€œå·²ä¸‹å‘â€ (å…ˆæ–©åå¥)
        # åœ¨ä»»åŠ¡å‘å‡ºçš„ä¸€ç¬é—´å°±æ ‡è®°ï¼Œé˜²æ­¢ Worker è¿˜æ²¡æµ‹å®Œåˆæ¥è¯·æ±‚
        self.r.sadd(dispatch_key, worker_uuid)
        self.r.expire(dispatch_key, 60) # 60ç§’åè‡ªåŠ¨è¿‡æœŸ
            
        return cmd
    def get_speed_test_results(self, test_id):
        """è·å–ç»“æœå¹¶åˆ¤æ–­çŠ¶æ€"""
        if not self.use_redis: return {"status": "error"}
        
        # 1. è·å–å…ƒæ•°æ®
        meta_json = self.r.get(f"crawler:speedtest:meta:{test_id}")
        if not meta_json:
            return {"state": "expired", "data": []}
            
        meta = json.loads(meta_json)
        total_expected = meta['total']
        start_time = meta['start_time']
        
        # 2. è·å–å½“å‰ç»“æœ
        raw_results = self.r.hgetall(f"crawler:speedtest:results:{test_id}")
        results = []
        for k, v in raw_results.items():
            try: results.append(json.loads(v))
            except: pass
            
        # 3. æ ¸å¿ƒï¼šåˆ¤æ–­çŠ¶æ€
        # çŠ¶æ€ï¼šrunning (è¿›è¡Œä¸­), finished (å…¨æ”¶é½), timeout (è¶…æ—¶)
        
        current_count = len(results)
        elapsed = time.time() - start_time
        
        if current_count >= total_expected:
            state = "finished" # å…¨é½äº†
        elif elapsed > 5:
            state = "timeout"  # è¶…è¿‡5ç§’äº†ï¼Œå¼ºåˆ¶ç»“æŸ
        else:
            state = "running"
            
        return {
            "state": state,
            "total": total_expected,
            "received": current_count,
            "elapsed": round(elapsed, 1),
            "data": results
        }
    def update_heartbeat(self, node_data, real_ip):
        """æ›´æ–°èŠ‚ç‚¹å¿ƒè·³"""
        uuid = node_data['uuid']
        
        # è‡ªåŠ¨è¡¥å…¨ IPï¼šå¦‚æœ Worker æ²¡é… public_urlï¼Œç”¨ real_ip è¡¥å…¨
        if not node_data['config'].get('public_url'):
            port = node_data['config']['port']
            node_data['config']['public_url'] = f"http://{real_ip}:{port}"
        
        # è®°å½•æœ€åæ›´æ–°æ—¶é—´
        node_data['last_seen'] = time.time()

        if self.use_redis:
            try:
                # 30ç§’è¿‡æœŸ
                self.r.setex(f"crawler:node:{uuid}", 30, json.dumps(node_data))
            except Exception as e:
                print(f"âŒ [Cluster] Redis Write Error: {e}")
        else:
            self.nodes[uuid] = node_data

    # managers.py -> ClusterManager ç±»

    def get_active_nodes(self):
        """è·å–æ‰€æœ‰èŠ‚ç‚¹å¹¶è¿›è¡Œåˆæ­¥æ¸…æ´—"""
        nodes = []
        if self.use_redis:
            try:
                # è·å–æ‰€æœ‰ crawler:node:* çš„é”®
                keys = self.r.keys("crawler:node:*")
                if keys:
                    # æ‰¹é‡è·å–
                    vals = self.r.mget(keys)
                    for v in vals:
                        if v:
                            nodes.append(json.loads(v))
            except Exception as e:
                print(f"âŒ [Cluster] Redis Read Error: {e}")
                return []
        else:
            # å†…å­˜æ¨¡å¼ï¼šæ¸…ç†è¿‡æœŸèŠ‚ç‚¹
            now = time.time()
            # è¿‡æ»¤æ‰è¶…è¿‡ 40 ç§’æ²¡å¿ƒè·³çš„èŠ‚ç‚¹ (ç»™ä¸€ç‚¹å®½å®¹åº¦)
            self.nodes = {k: v for k, v in self.nodes.items() if now - v.get('last_seen', 0) < 40}
            nodes = list(self.nodes.values())
        
        return nodes

    def select_best_node(self, target_url=None):
        """
        [é‡æ„] æ™ºèƒ½è·¯ç”±ç®—æ³• (è´Ÿè½½ + åŒºåŸŸ + åŸŸåçº§é€Ÿåº¦)
        """
        nodes = self.get_active_nodes()
        if not nodes: return None

        best_node = None
        highest_score = -9999
        
        # é¢„å…ˆæå–åŸŸå
        target_domain = None
        if target_url:
            try:
                from urllib.parse import urlparse
                target_domain = urlparse(target_url).netloc
            except: pass

        for node in nodes:
            cfg = node['config']
            status = node['status']
            uuid = node['uuid']
            
            # 1. ç†”æ–­æœºåˆ¶ï¼šæ»¡è½½ä¸æ¥å®¢
            if status['current_tasks'] >= cfg['max_tasks']: 
                continue

            # 2. åŸºç¡€èµ„æºåˆ† (0~100)
            # é€»è¾‘ï¼š(1 - è´Ÿè½½ç‡) * 100
            load_ratio = status['current_tasks'] / cfg['max_tasks']
            base_score = (1 - load_ratio) * 100
            
            # CPU æƒ©ç½š (å¦‚æœ CPU > 80%ï¼Œåˆ†æ•°å¤§å‡)
            if status['cpu'] > 80: base_score *= 0.5

            # 3. åŒºåŸŸåŠ æƒ (ç²—ç•¥ç­›é€‰)
            region_coef = 1.0
            if target_url:
                is_cn_site = any(x in target_url for x in ['.cn', 'biqu', 'gongzicp'])
                if is_cn_site and cfg['region'] == 'CN': region_coef = 1.2
                if not is_cn_site and cfg['region'] == 'GLOBAL': region_coef = 1.2
            
            # 4. [æ ¸å¿ƒ] åŸŸåçº§é€Ÿåº¦åŠ æƒ (ç²¾ç»†ç­›é€‰)
            speed_coef = 1.0
            if target_domain and self.use_redis:
                # æŸ¥ Redis: crawler:latency:www.google.com -> {uuid: 150}
                latency_str = self.r.hget(f"crawler:latency:{target_domain}", uuid)
                if latency_str:
                    speed_coef = self._get_speed_coefficient(int(latency_str))

            # === æœ€ç»ˆå¾—åˆ†å…¬å¼ ===
            # èµ„æºåˆ† * åŒºåŸŸç³»æ•° * é€Ÿåº¦ç³»æ•°
            final_score = base_score * region_coef * speed_coef
            
            # è°ƒè¯•æ—¥å¿— (å¼€å‘æ—¶å–æ¶ˆæ³¨é‡Š)
            # print(f"Node: {cfg['name']} | Base: {base_score:.0f} | Reg: {region_coef} | Spd: {speed_coef} ({target_domain}) -> Final: {final_score:.1f}")

            if final_score > highest_score:
                highest_score = final_score
                best_node = node

        return best_node


class TaskManager:
    def __init__(self):
        self.executor = ThreadPoolExecutor(max_workers=5)
        self.tasks = {} # task_id -> {status, result, error, created_at, message}
        self.redis_url = os.environ.get('REDIS_URL')
        self.use_redis = False
        self.r = None
        if self.redis_url:
            try:
                self.r = redis.from_url(self.redis_url, decode_responses=True)
                self.r.ping()
                self.use_redis = True
                print("âœ… [TaskMgr] Redis è¿æ¥æˆåŠŸï¼Œä»»åŠ¡çŠ¶æ€æŒä¹…åŒ–å¯ç”¨")
            except Exception as e:
                print(f"âš ï¸ [TaskMgr] Redis è¿æ¥å¤±è´¥ ({e})ï¼Œé™çº§ä¸ºå†…å­˜æ¨¡å¼")
        else:
            print("â„¹ï¸ [TaskMgr] æœªé…ç½® REDIS_URLï¼Œä½¿ç”¨å†…å­˜ä»»åŠ¡çŠ¶æ€")

    def _redis_key(self, task_id):
        return f"task:{task_id}"

    def _set_task(self, task_id, data, expire=3600):
        if not self.use_redis:
            self.tasks[task_id] = data
            return
        try:
            payload = data.copy()
            # åºåˆ—åŒ– result
            if 'result' in payload:
                payload['result'] = json.dumps(payload['result'], ensure_ascii=False) if payload['result'] is not None else ''
            self.r.hset(self._redis_key(task_id), mapping=payload)
            self.r.expire(self._redis_key(task_id), expire)
        except Exception as e:
            print(f"[TaskMgr] Redis å†™å…¥å¤±è´¥: {e}")
            self.tasks[task_id] = data

    def _get_task(self, task_id):
        if not self.use_redis:
            return self.tasks.get(task_id)
        try:
            data = self.r.hgetall(self._redis_key(task_id))
            if not data:
                return None
            # ååºåˆ—åŒ– result
            result = data.get('result', '')
            if result:
                try:
                    data['result'] = json.loads(result)
                except Exception:
                    data['result'] = result
            else:
                data['result'] = None
            # ç±»å‹ä¿®æ­£
            if 'progress' in data:
                try: data['progress'] = int(data['progress'])
                except: pass
            if 'created_at' in data:
                try: data['created_at'] = float(data['created_at'])
                except: pass
            return data
        except Exception as e:
            print(f"[TaskMgr] Redis è¯»å–å¤±è´¥: {e}")
            return self.tasks.get(task_id)

    def submit(self, func, *args, **kwargs):
        task_id = str(uuid.uuid4())
        task_data = {
            "status": "pending",
            "progress": 0,
            "result": None,
            "error": None,
            "message": "Waiting...",
            "created_at": time.time()
        }
        self._set_task(task_id, task_data)
        # å°† update_callback æ³¨å…¥åˆ° kwargs ä¸­ï¼Œä¾› func è°ƒç”¨
        kwargs['_task_update_cb'] = self._create_updater(task_id)
        self.executor.submit(self._worker, task_id, func, *args, **kwargs)
        return task_id

    def _create_updater(self, task_id):
        def updater(progress=None, msg=None, result_delta=None):
            data = self._get_task(task_id) or {}
            if progress is not None: data["progress"] = progress
            if msg is not None: data["message"] = msg
            # Support intermediate results (list only for now)
            if result_delta is not None:
                if data.get("result") is None: data["result"] = []
                if isinstance(data.get("result"), list) and isinstance(result_delta, list):
                    data["result"].extend(result_delta)
            self._set_task(task_id, data)
        return updater

    def _worker(self, task_id, func, *args, **kwargs):
        print(f"[Task] Starting {task_id}")
        callback = kwargs.pop('_task_update_cb', None)
        try:
            data = self._get_task(task_id) or {}
            data["status"] = "running"
            self._set_task(task_id, data)
            
            # Pass user_callback if the function accepts it
            # We assume func signature might allow **kwargs or explicit 'callback'
            # But to be safe, we only pass it if 'callback' is in kwargs, which we handle in submit wrapper?
            # actually, let's just pass it as 'callback' arg if the user function expects it.
            # However, for simplicity, I'll inject it into kwargs and let the target function `pop` it if it needs.
            
            # Re-inject for the function to use
            kwargs['callback'] = callback
            
            # Execute
            res = func(*args, **kwargs)
            
            # Only overwrite result if it's returned and task result is not managed incrementally
            # Or assume the function returns the FINAL COMPLETE result.
            data = self._get_task(task_id) or {}
            data["result"] = res
            data["status"] = "completed"
            data["progress"] = 100
            self._set_task(task_id, data)
            print(f"[Task] Completed {task_id}")
        except Exception as e:
            print(f"[Task Error] {task_id}: {e}")
            import traceback
            traceback.print_exc()
            data = self._get_task(task_id) or {}
            data["status"] = "failed"
            data["error"] = str(e)
            data["message"] = "Task failed"
            self._set_task(task_id, data)
    def get_status(self, task_id):
        # Debug: print keys if not found
        t = self._get_task(task_id)
        if not t:
            print(f"[TaskMgr] Checking {task_id} -> Not Found")
        return t

    def cleanup(self):
        # ç®€å•æ¸…ç†è¶…è¿‡1å°æ—¶çš„ä»»åŠ¡
        if self.use_redis:
            return
        now = time.time()
        to_del = [k for k,v in self.tasks.items() if now - v['created_at'] > 3600]
        for k in to_del: del self.tasks[k]

task_manager = TaskManager()

# å®ä¾‹åŒ–
cluster_manager = ClusterManager()
# ==========================================
# 6. åˆå§‹åŒ–æ‰€æœ‰å•ä¾‹
# ==========================================
role_manager = RoleManager()
offline_manager = OfflineBookManager()
cache = CacheManager()
db = IsolatedDB()
booklist_manager = IsolatedBooklistManager()
downloader = DownloadManager()
tag_manager = IsolatedTagManager()
stats_manager = IsolatedStatsManager()
history_manager = HistoryManager()
update_manager = UpdateManager()
update_sub_manager = UpdateRecordManager()
exporter = ExportManager()

# æ³¨å…¥åˆ° shared ä¾›è£…é¥°å™¨ä½¿ç”¨
shared.role_manager_instance = role_manager

class MemoManager:
    """æ¡Œé¢å¤‡å¿˜å½•ç®¡ç†"""
    
    def __init__(self):
        self._ensure_table()
    
    def _ensure_table(self):
        """ç¡®ä¿å¤‡å¿˜å½•è¡¨å­˜åœ¨"""
        try:
            with get_db() as conn:
                conn.execute('''CREATE TABLE IF NOT EXISTS user_memos (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    username TEXT NOT NULL,
                    title TEXT NOT NULL DEFAULT 'æœªå‘½åå¤‡å¿˜å½•',
                    content TEXT,
                    tags TEXT,  -- JSON æ•°ç»„å­˜å‚¨æ ‡ç­¾
                    is_pinned BOOLEAN DEFAULT 0,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )''')
                
                # å†å²ç‰ˆæœ¬è¡¨ï¼ˆæ¯æ¬¡ä¿å­˜è‡ªåŠ¨å¿«ç…§ï¼‰
                conn.execute('''CREATE TABLE IF NOT EXISTS memo_history (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    memo_id INTEGER NOT NULL,
                    content TEXT,
                    saved_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    FOREIGN KEY(memo_id) REFERENCES user_memos(id)
                )''')
                conn.commit()
        except Exception as e:
            print(f"[MemoManager] å»ºè¡¨å¤±è´¥: {e}")
    
    def get_all_memos(self, username):
        """è·å–ç”¨æˆ·æ‰€æœ‰å¤‡å¿˜å½•"""
        with get_db() as conn:
            rows = conn.execute(
                "SELECT * FROM user_memos WHERE username=? ORDER BY is_pinned DESC, updated_at DESC",
                (username,)
            ).fetchall()
            return [dict(row) for row in rows]
    
    def get_memo(self, memo_id):
        """è·å–å•æ¡å¤‡å¿˜å½•"""
        with get_db() as conn:
            row = conn.execute("SELECT * FROM user_memos WHERE id=?", (memo_id,)).fetchone()
            return dict(row) if row else None
    
    def save_memo(self, username, memo_id=None, title=None, content=None, tags=None):
        """ä¿å­˜å¤‡å¿˜å½•ï¼ˆæ–°å»ºæˆ–æ›´æ–°ï¼‰"""
        with get_db() as conn:
            if memo_id:
                # æ›´æ–°ç°æœ‰å¤‡å¿˜å½•
                conn.execute("""
                    UPDATE user_memos 
                    SET title=COALESCE(?, title), 
                        content=COALESCE(?, content),
                        tags=COALESCE(?, tags),
                        updated_at=CURRENT_TIMESTAMP
                    WHERE id=?
                """, (title, content, tags, memo_id))
                
                # ä¿å­˜å†å²ç‰ˆæœ¬
                if content:
                    conn.execute(
                        "INSERT INTO memo_history (memo_id, content) VALUES (?, ?)",
                        (memo_id, content)
                    )
            else:
                # æ–°å»ºå¤‡å¿˜å½•
                cursor = conn.execute("""
                    INSERT INTO user_memos (username, title, content, tags)
                    VALUES (?, ?, ?, ?)
                """, (username, title or 'æ–°å¤‡å¿˜å½•', content or '', tags or '[]'))
                memo_id = cursor.lastrowid
            
            conn.commit()
            return memo_id
    
    def delete_memo(self, memo_id):
        """åˆ é™¤å¤‡å¿˜å½•"""
        with get_db() as conn:
            conn.execute("DELETE FROM user_memos WHERE id=?", (memo_id,))
            conn.execute("DELETE FROM memo_history WHERE memo_id=?", (memo_id,))
            conn.commit()
    
    def toggle_pin(self, memo_id):
        """ç½®é¡¶/å–æ¶ˆç½®é¡¶"""
        with get_db() as conn:
            conn.execute("""
                UPDATE user_memos 
                SET is_pinned = NOT is_pinned 
                WHERE id=?
            """, (memo_id,))
            conn.commit()
    
    def search_memos(self, username, keyword):
        """æœç´¢å¤‡å¿˜å½•"""
        with get_db() as conn:
            rows = conn.execute("""
                SELECT * FROM user_memos 
                WHERE username=? AND (title LIKE ? OR content LIKE ?)
                ORDER BY updated_at DESC
            """, (username, f'%{keyword}%', f'%{keyword}%')).fetchall()
            return [dict(row) for row in rows]

# å…¨å±€å®ä¾‹
memo_manager = MemoManager()