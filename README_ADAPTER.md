
# ğŸ› ï¸ Smart NoteDB é€‚é…å™¨æ’ä»¶å¼€å‘æ‰‹å†Œ

## 1. æ¦‚è¿°
Smart NoteDB é‡‡ç”¨ **Adapter Pattern (é€‚é…å™¨æ¨¡å¼)** å¤„ç†ä¸åŒå°è¯´ç½‘ç«™çš„å·®å¼‚ã€‚
*   **é€šç”¨çˆ¬è™«é€»è¾‘**ï¼šè´Ÿè´£å¤„ç† 90% çš„æ ‡å‡†ç¬”è¶£é˜æ¨¡æ¿ç«™ç‚¹ã€‚
*   **é€‚é…å™¨æ’ä»¶**ï¼šä¸“é—¨å¤„ç†ç»“æ„å¥‡è‘©ã€æœ‰ç‰¹æ®Šå¹²æ‰°ç¬¦ï¼ˆå¦‚ `.la`ã€`www.xxx.com` å¹¿å‘Šè¯ï¼‰ã€æˆ–è€…ç¿»é¡µé€»è¾‘è¯¡å¼‚çš„â€œç¡¬éª¨å¤´â€ç½‘ç«™ã€‚

## 2. æ’ä»¶æœºåˆ¶
ä¸»ç¨‹åºä¼šè‡ªåŠ¨æ‰«æ `/adapters` ç›®å½•ä¸‹çš„æ‰€æœ‰ `.py` æ–‡ä»¶ã€‚
*   **è¯†åˆ«æ¡ä»¶**ï¼šç±»åä¸­åŒ…å« `Adapter` å…³é”®å­—ã€‚
*   **åŠ è½½æ–¹å¼**ï¼šåŠ¨æ€åŠ è½½ï¼Œä¿®æ”¹æ’ä»¶ä»£ç åé‡å¯æœåŠ¡å™¨å³å¯ç”Ÿæ•ˆï¼Œ**æ— éœ€æ”¹åŠ¨ä¸»ç¨‹åºé€»è¾‘**ã€‚

## 3. æ’ä»¶æ ‡å‡†æ¨¡æ¿
åœ¨ `/adapters` ç›®å½•ä¸‹æ–°å»º `xxx_adapter.py`ï¼Œå¤åˆ¶ä»¥ä¸‹ç»“æ„ï¼š

```python
import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup

class XxxSiteAdapter:
    """
    é€‚é…å™¨ç±»ï¼šè¯·ç¡®ä¿ç±»ååŒ…å« 'Adapter' å…³é”®å­—
    """

    def can_handle(self, url):
        """
        [å¿…é¡»å®ç°] å†³å®šå“ªäº› URL ç”±è¯¥æ’ä»¶æ¥ç®¡
        """
        return "xxxsite.com" in url

    def get_toc(self, crawler, toc_url):
        """
        [å¿…é¡»å®ç°] ä¸“ç”¨çš„ç›®å½•è§£æé€»è¾‘
        :param crawler: ä¼ å…¥çš„ NovelCrawler å®ä¾‹ï¼Œç”¨äºå¤ç”¨è¯·æ±‚æ–¹æ³•
        :param toc_url: ç›®å½•é¡µåœ°å€
        """
        html = crawler._fetch_page_smart(toc_url)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        
        title = "è§£æåˆ°çš„ä¹¦å"
        chapters = []
        # æ‰‹å†™è§£æé€»è¾‘ï¼šä¾‹å¦‚ soup.select('.list a')
        # æ¯é¡¹æ ¼å¼ï¼š{'title': 'ç« èŠ‚å', 'url': 'å®Œæ•´é“¾æ¥'}
        return {'title': title, 'chapters': chapters}

    def run(self, crawler, url):
        """
        [å¿…é¡»å®ç°] ä¸“ç”¨çš„æ­£æ–‡è§£æä¸åˆ†é¡µç¼åˆé€»è¾‘
        :param crawler: ä¼ å…¥çš„ NovelCrawler å®ä¾‹
        :param url: ç« èŠ‚èµ·å§‹åœ°å€
        """
        combined_content = []
        meta = {} # å­˜å‚¨ title, next, prev, toc_url
        
        # æ¨èé€»è¾‘ï¼š
        # 1. æ‰§è¡Œ while å¾ªç¯æˆ– for å¾ªç¯é€’å½’æŠ“å–åˆ†é¡µ (Page 1, 2, 3...)
        # 2. è°ƒç”¨ crawler._extract_content_smart(soup) è·å–æ­£æ–‡
        # 3. åœ¨æ’ä»¶å†…è¿›è¡Œé’ˆå¯¹æ€§çš„åƒåœ¾å­—ç¬¦æ¸…ç† (å¦‚ re.sub)
        # 4. è¯†åˆ«çœŸæ­£çš„â€˜ä¸‹ä¸€ç« â€™é“¾æ¥å¹¶è¿”å›
        
        return {
            'title': meta.get('title'),
            'content': combined_content, # ç¼åˆåçš„ List
            'prev': meta.get('prev'),
            'next': meta.get('next'),
            'toc_url': meta.get('toc_url')
        }
```

## 4. å¼€å‘æ ¸å¿ƒæŠ€å·§

### 4.1 å¤ç”¨ä¸»ç¨‹åºèƒ½åŠ›
ç¼–å†™æ’ä»¶æ—¶ï¼Œè¯·åŠ¡å¿…åˆ©ç”¨ `crawler` æä¾›çš„å†…ç½®å·¥å…·ï¼Œä»¥ä¿è¯åŠŸèƒ½ä¸€è‡´æ€§ï¼š
*   `crawler._fetch_page_smart(url)`ï¼š**å¿…é¡»ä½¿ç”¨**ã€‚å®ƒå†…ç½®äº†ä½ é…ç½®çš„**ç³»ç»Ÿä»£ç†**ã€**æµè§ˆå™¨æŒ‡çº¹ä¼ªè£…**å’Œ**æ™ºèƒ½ç¼–ç è¯†åˆ«**ã€‚
*   `crawler._clean_text_lines(list)`ï¼šé€šç”¨æ¸…æ´—ã€‚å¯ä»¥è¿‡æ»¤æ‰â€œä¸Šä¸€ç« ã€ä¸‹ä¸€ç« â€ç­‰åŸºç¡€å™ªéŸ³ã€‚
*   `crawler._get_smart_title(soup)`ï¼šé€šç”¨æ ‡é¢˜è¯†åˆ«ã€‚å¦‚æœè¯¥ç«™æ ‡é¢˜ä¸ç‰¹æ®Šï¼Œå¯ä»¥ç›´æ¥è°ƒç”¨ã€‚

### 4.2 å¤„ç†åˆ†é¡µç¼åˆçš„â€œå¥—è·¯â€
é’ˆå¯¹ä¸€ç« åˆ†å¤šé¡µçš„ç«™ç‚¹ï¼Œåœ¨ `run` æ–¹æ³•ä¸­ä½¿ç”¨ä»¥ä¸‹é€»è¾‘ï¼š
1.  **å½’ä¸€åŒ–**ï¼šå¦‚æœä¼ å…¥çš„ URL æ˜¯ `_2.html`ï¼Œå…ˆæ­£åˆ™æ›¿æ¢æˆ `.html` (ç¬¬ä¸€é¡µ)ã€‚
2.  **è·å–æœ¬ç«  ID**ï¼šä» URL æå–æ•°å­—ï¼Œåˆ¤æ–­ä¸‹ä¸€é¡µæŒ‰é’®çš„é“¾æ¥æ˜¯å¦è¿˜åŒ…å«è¿™ä¸ªæ•°å­—ã€‚å¦‚æœåŒ…å«ï¼Œè¯´æ˜æ˜¯åˆ†é¡µï¼›å¦‚æœä¸åŒ…å«ï¼Œè¯´æ˜æ˜¯çœŸæ­£çš„ä¸‹ä¸€ç« ã€‚
3.  **æ ‡é¢˜é”å®š**ï¼šè®°å½•ç¬¬ä¸€é¡µçš„æ ‡é¢˜ã€‚å¦‚æœåç»­é¡µé¢çš„æ ‡é¢˜å˜äº†ï¼Œç«‹å³åœæ­¢ç¼åˆã€‚

### 4.3 å¼ºåŠ›æ¸…æ´—â€œè„æ•°æ®â€
å¯¹äºç«™ç‚¹ç‰¹æœ‰çš„å¹²æ‰°ï¼ˆå¦‚ `xbqg77` çš„ `.la`ï¼‰ï¼š
```python
lines = crawler._clean_text_lines(raw_text)
# é’ˆå¯¹æ€§å¤„ç†
clean_lines = [line.replace('.la', '').strip() for line in lines]
```

## 5. è°ƒè¯•å»ºè®®
1.  **æ§åˆ¶å°æ—¥å¿—**ï¼šåœ¨æ’ä»¶çš„å…³é”®ä½ç½®å†™ `print(f"[Adapter] xxx")`ï¼Œå¯åŠ¨ Flask åå¯ä»¥åœ¨ç»ˆç«¯ç›´æ¥æŸ¥çœ‹åˆ°æ’ä»¶çš„è¿è¡ŒçŠ¶æ€ã€‚
2.  **ç‹¬ç«‹æµ‹è¯•**ï¼šå¯ä»¥å…ˆå†™ä¸€ä¸ªç®€å•çš„æµ‹è¯•è„šæœ¬ï¼Œæ‰‹åŠ¨å®ä¾‹åŒ–ä½ çš„ Adapter å¹¶ä¼ å…¥ä¸€ä¸ªæ¨¡æ‹Ÿçš„ crawler å¯¹è±¡è¿›è¡Œè§£ææµ‹è¯•ã€‚
