from flask import Blueprint, request, jsonify
import threading
from concurrent.futures import ThreadPoolExecutor, as_completed
from shared import pro_required, is_safe_url
from managers import offline_manager

# å‡è®¾ä½ æŠŠçˆ¬è™«é€»è¾‘ç§»åˆ°äº† spider_core.pyï¼Œå¹¶å®ä¾‹åŒ–äº† crawler_instance
# å¦‚æœæ²¡ç§»ï¼Œä½ éœ€è¦ä» dbserver import crawler (ä½†è¿™ä¼šå¯¼è‡´å¾ªç¯å¼•ç”¨)ï¼Œæ‰€ä»¥å¼ºçƒˆå»ºè®®ç§»å‡ºçˆ¬è™«ç±»
from spider_core import crawler_instance as crawler 

pro_bp = Blueprint('pro', __name__)

# routes/pro_bp.py

@pro_bp.route('/api/pro/download_book', methods=['POST'])
@pro_required
def api_pro_download_book():
    book_key = request.json.get('key')
    input_url = request.json.get('url') # è¿™æ˜¯ä½ å½“å‰çœ‹çš„æŸä¸€ç« 
    
    if not book_key or not input_url:
        return jsonify({"status": "error", "msg": "Missing params"})

    if not is_safe_url(input_url):
        return jsonify({"status": "error", "msg": "Illegal URL"}), 403

    def download_task(u_key, start_url):
        print(f"[Pro] å¯åŠ¨ç¦»çº¿ä»»åŠ¡: {u_key}")
        
        toc = None
        real_toc_url = None

        # 1. æ™ºèƒ½åˆ¤æ–­ï¼šå¦‚æœ URL ä»¥ .html ç»“å°¾ï¼Œå¤§æ¦‚ç‡æ˜¯ç« èŠ‚ï¼Œä¸æ˜¯ç›®å½•
        # æˆ–è€…å…ˆå°è¯•è§£æï¼Œå¦‚æœç« èŠ‚æ•°å¤ªå°‘ï¼Œä¹Ÿè®¤ä¸ºä¸å¯¹
        is_chapter_url = ".html" in start_url
        
        if not is_chapter_url:
            # çœ‹èµ·æ¥åƒç›®å½•ï¼Œå…ˆè¯•ç€æŠ“ä¸€ä¸‹
            toc = crawler.get_toc(start_url)
        
        # 2. æ ¡éªŒé€»è¾‘ï¼šå¦‚æœæ²¡æŠ“åˆ°ï¼Œæˆ–è€…æŠ“åˆ°çš„ç« èŠ‚å°‘äº 20 ç«  (é˜²æ­¢è¯¯åˆ¤â€œæœ€æ–°ç« èŠ‚åˆ—è¡¨â€)
        if not toc or len(toc['chapters']) < 20:
            print(f"[Pro] URL ä¼¼ä¹ä¸æ˜¯å…¨æœ¬ç›®å½• (ä»… {len(toc['chapters']) if toc else 0} ç« )ï¼Œå°è¯•å¯»æ‰¾çœŸå®ç›®å½•...")
            
            # è®¿é—®å½“å‰é¡µé¢ï¼Œå¯»æ‰¾â€œç›®å½•â€æŒ‰é’®çš„é“¾æ¥
            page_data = crawler.run(start_url)
            if page_data and page_data.get('toc_url'):
                real_toc_url = page_data['toc_url']
                print(f"[Pro] ğŸ¯ å®šä½åˆ°çœŸå®ç›®å½•: {real_toc_url}")
                # å†æ¬¡å°è¯•æŠ“å–ç›®å½•
                toc = crawler.get_toc(real_toc_url)
            else:
                print("[Pro] âŒ æ— æ³•å®šä½ç›®å½•é¡µï¼Œä»»åŠ¡ç»ˆæ­¢ã€‚")
                return

        if not toc or not toc['chapters']: 
            print("[Pro] âŒ ç›®å½•è§£æå¤±è´¥æˆ–ä¸ºç©º")
            return
        
        print(f"[Pro] âœ… ç›®å½•è·å–æˆåŠŸï¼Œå…± {len(toc['chapters'])} ç« ï¼Œå¼€å§‹å¹¶å‘ä¸‹è½½...")

        # 3. å¹¶å‘ä¸‹è½½å…¨ä¹¦
        full_data = {}
        # å»ºè®®æ ¹æ®æœåŠ¡å™¨é…ç½®è°ƒæ•´ max_workersï¼Œ10-15 æ˜¯æ¯”è¾ƒæ¿€è¿›ä½†é«˜æ•ˆçš„å€¼
        with ThreadPoolExecutor(max_workers=12) as exe:
            future_to_url = {exe.submit(crawler.run, c['url']): c['url'] for c in toc['chapters']}
            
            # è¿›åº¦è®¡æ•°
            total = len(toc['chapters'])
            done = 0
            
            for future in as_completed(future_to_url):
                url = future_to_url[future]
                try:
                    data = future.result()
                    if data: 
                        full_data[url] = data
                except: pass
                
                done += 1
                if done % 50 == 0:
                    print(f"[Pro] ä¸‹è½½è¿›åº¦: {done}/{total}")
        
        # 4. ä¿å­˜
        offline_manager.save_book(u_key, full_data)
        print(f"[Pro] ğŸ‰ ç¦»çº¿ä¸‹è½½å®Œæˆ: {u_key} (æœ€ç»ˆç¼“å­˜ {len(full_data)} ç« )")

    threading.Thread(target=download_task, args=(book_key, input_url)).start()
    return jsonify({"status": "success", "msg": "ğŸš€ å…¨æœ¬ç¦»çº¿ä»»åŠ¡å·²å¯åŠ¨ï¼Œæ­£åœ¨åå°é«˜é€Ÿä¸‹è½½..."})

# ==========================================
# ä¸‹è½½ç®¡ç†åŠŸèƒ½ï¼ˆPro ä¸“å±ï¼‰
# ==========================================
@pro_bp.route('/api/pro/list_downloads', methods=['GET'])
@pro_required
def list_downloads():
    """åˆ—å‡º downloads æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    import os
    from shared import DL_DIR
    
    try:
        files = []
        if os.path.exists(DL_DIR):
            for filename in os.listdir(DL_DIR):
                filepath = os.path.join(DL_DIR, filename)
                if os.path.isfile(filepath):
                    file_stat = os.stat(filepath)
                    files.append({
                        'filename': filename,
                        'size': file_stat.st_size,
                        'modified': file_stat.st_mtime
                    })
        
        # æŒ‰ä¿®æ”¹æ—¶é—´å€’åºæ’åˆ—
        files.sort(key=lambda x: x['modified'], reverse=True)
        return jsonify({"success": True, "files": files})
    
    except Exception as e:
        return jsonify({"success": False, "msg": str(e)})

@pro_bp.route('/api/pro/download_file', methods=['GET'])
@pro_required
def download_file():
    from flask import send_from_directory
    from shared import DL_DIR
    
    filename = request.args.get('filename')
    if not filename:
        return "Missing filename", 400
    
    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢è·¯å¾„éå†æ”»å‡»
    import os
    safe_filename = os.path.basename(filename)
    filepath = os.path.join(DL_DIR, safe_filename)
    
    if not os.path.exists(filepath):
        return "File not found", 404
    
    return send_from_directory(DL_DIR, safe_filename, as_attachment=True, conditional=False, max_age=0)

@pro_bp.route('/api/pro/delete_file', methods=['POST'])
@pro_required
def delete_file():
    """åˆ é™¤ downloads æ–‡ä»¶å¤¹ä¸­çš„æŒ‡å®šæ–‡ä»¶"""
    import os
    from shared import DL_DIR
    
    data = request.get_json()
    filename = data.get('filename')
    
    if not filename:
        return jsonify({"success": False, "msg": "ç¼ºå°‘æ–‡ä»¶å"})
    
    # å®‰å…¨æ£€æŸ¥
    safe_filename = os.path.basename(filename)
    filepath = os.path.join(DL_DIR, safe_filename)
    
    if not os.path.exists(filepath):
        return jsonify({"success": False, "msg": "æ–‡ä»¶ä¸å­˜åœ¨"})
    
    try:
        os.remove(filepath)
        return jsonify({"success": True, "msg": "åˆ é™¤æˆåŠŸ"})
    except Exception as e:
        return jsonify({"success": False, "msg": str(e)})
