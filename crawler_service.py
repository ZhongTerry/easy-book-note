import os
import time
import requests
import json
import logging
import sys
import psutil # ç¡®ä¿å®‰è£…äº† psutil
from unittest.mock import MagicMock
from dotenv import load_dotenv
import threading
load_dotenv('config.env') 

# === [é»‘é­”æ³•ï¼šç¯å¢ƒæ¨¡æ‹Ÿ] ===
mock_managers = MagicMock()
def configure_mock_manager(manager_mock):
    manager_mock.get.return_value = None
    manager_mock.get_val.return_value = None
    manager_mock.get_chapter.return_value = None
    manager_mock.get_toc.return_value = None
    manager_mock.find.return_value = None
    manager_mock.load.return_value = {}
    manager_mock.get_all.return_value = []
    manager_mock.list_all.return_value = {}
    manager_mock.get_history.return_value = []

known_managers = ['cache', 'db', 'offline_manager', 'booklist_manager', 'tag_manager', 'stats_manager', 'history_manager', 'update_manager', 'role_manager']
for name in known_managers:
    configure_mock_manager(getattr(mock_managers, name))

sys.modules['managers'] = mock_managers
sys.modules['managers.cache'] = mock_managers.cache
for name in known_managers:
    sys.modules[f'managers.{name}'] = getattr(mock_managers, name)

from spider_core import crawler_instance as crawler, searcher

# === é…ç½®åŒº ===
MASTER_URL = os.environ.get("MASTER_URL", "https://book.ztrztr.top")
AUTH_TOKEN = os.environ.get("REMOTE_CRAWLER_TOKEN", "my-secret-token-888")
NODE_NAME = os.environ.get("NODE_NAME", "Worker-Node")
# å¯¼å…¥ uuid
import uuid
# å…¨å±€ UUID (å¯åŠ¨ç”Ÿæˆä¸€æ¬¡ï¼Œä¸å˜)
NODE_UUID = str(uuid.uuid4())

# ä»»åŠ¡è®¡æ•°é”
CURRENT_TASKS = 0
TASK_LOCK = threading.Lock() # éœ€è¦å¯¼å…¥ threading
import threading

# === [æ–°å¢] çŠ¶æ€ç”Ÿæˆè¾…åŠ©å‡½æ•° ===
def get_node_payload():
    """ç”Ÿæˆå®Œæ•´çš„èŠ‚ç‚¹çŠ¶æ€æ•°æ®"""
    return {
        "uuid": NODE_UUID,
        "config": {
            "name": NODE_CONFIG['name'],
            "region": NODE_CONFIG['region'],
            "max_tasks": NODE_CONFIG['max_tasks'],
            "public_url": NODE_CONFIG['public_url'],
            "port": NODE_CONFIG['port']
        },
        "status": {
            "cpu": psutil.cpu_percent(interval=None),
            "memory": psutil.virtual_memory().percent,
            "current_tasks": CURRENT_TASKS,
            "timestamp": time.time()
        }
    }

# è¡¥å…¨é…ç½®å¯¹è±¡
NODE_CONFIG = {
    "name": NODE_NAME,
    "region": os.environ.get("NODE_REGION", "GLOBAL"),
    "max_tasks": int(os.environ.get("NODE_MAX_TASKS", 20)),
    "public_url": os.environ.get("NODE_PUBLIC_URL", ""),
    "port": int(os.environ.get("PORT", 12345))
}


def do_work(task):
    endpoint = task['endpoint']
    payload = task['payload']
    url = payload.get('url')
    print(f"âš¡ [Job] æ‰§è¡Œ: {endpoint} -> {url}")
    result = {"status": "failed", "msg": "Unknown error"}
    
    with TASK_LOCK: CURRENT_TASKS += 1
    try:
        data = None
        if endpoint == 'run': data = crawler.run(url)
        elif endpoint == 'toc': data = crawler.get_toc(url)
        elif endpoint == 'search': data = searcher.search_bing(payload.get('keyword'))
            
        if data:
            # ç®€å•æ¸…æ´—é˜²åºåˆ—åŒ–é”™è¯¯
            try: json.dumps(data)
            except: data = str(data)
            result = {"status": "success", "data": data}
        else:
            result = {"status": "failed", "msg": "Empty data"}
    except Exception as e:
        print(f"âŒ ä»»åŠ¡å‡ºé”™: {e}")
        result = {"status": "error", "msg": str(e)}
    finally:
        with TASK_LOCK: CURRENT_TASKS -= 1
        
    return result

def worker_loop():
    print(f"ğŸš€ Worker [{NODE_NAME}] å¯åŠ¨ (Hybrid Mode)")
    print(f"ğŸ†” UUID: {NODE_UUID}")
    print(f"ğŸ”— è¿æ¥ Master: {MASTER_URL}")
    
    session = requests.Session()
    session.headers.update({
        "Authorization": f"Bearer {AUTH_TOKEN}",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) SmartNoteDB-Worker"
    })
    
    while True:
        try:
            # === [æ ¸å¿ƒä¿®å¤] å–ä»»åŠ¡æ—¶ï¼Œæºå¸¦å®Œæ•´çš„çŠ¶æ€åŒ… ===
            # è¿™æ ·å³ä½¿å¿ƒè·³çº¿ç¨‹æŒ‚äº†ï¼Œåªè¦è¿˜åœ¨å–ä»»åŠ¡ï¼ŒçŠ¶æ€å°±èƒ½æ›´æ–°
            full_payload = get_node_payload()
            
            resp = session.post(f"{MASTER_URL}/api/cluster/fetch_task", json=full_payload, timeout=10)
            
            if resp.status_code == 403:
                print("ğŸ”’ Token é”™è¯¯")
                time.sleep(10); continue
            
            # å¤„ç†å“åº”
            try:
                res_json = resp.json()
            except:
                time.sleep(5); continue
            
            if res_json.get('status') == 'success':
                task = res_json['task']
                crawl_result = do_work(task)
                session.post(f"{MASTER_URL}/api/cluster/submit_result", json={
                    "task_id": task['id'], "result": crawl_result
                })
                print(f"âœ… [Job] å®Œæˆ")
            else:
                time.sleep(1) 
                
        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œæ³¢åŠ¨: {e}")
            time.sleep(5)

# ä¿ç•™å¿ƒè·³çº¿ç¨‹ä½œä¸ºç©ºé—²æ—¶çš„ä¿æ´»æ‰‹æ®µ
def heartbeat_thread():
    while True:
        try:
            # å¤ç”¨åŒä¸€ä¸ª payload ç”Ÿæˆå‡½æ•°
            requests.post(
                f"{MASTER_URL}/api/cluster/heartbeat", 
                json=get_node_payload(),
                headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
                timeout=5
            )
        except: pass
        time.sleep(10)

if __name__ == '__main__':
    threading.Thread(target=heartbeat_thread, daemon=True).start()
    worker_loop()