import os
import time
import requests
import json
import logging
import sys
import threading
import psutil
import uuid
from dotenv import load_dotenv

# åŠ è½½é…ç½®
load_dotenv('config.env')

# =========================================================
# ğŸ›¡ï¸ æ ¸å¿ƒä¿®å¤ï¼šæ„å»ºä¸€ä¸ªâ€œå“‘å·´â€ç¯å¢ƒ (Dumb Mock)
# =========================================================
# å®šä¹‰ä¸€ä¸ªåªä¼šè¿”å› None çš„ç©ºç±»ï¼Œé˜²æ­¢ MagicMock è‡ªåŠ¨ç”Ÿæˆå¯¹è±¡
class EmptyObject:
    def __getattr__(self, name):
        return None  # è®¿é—®ä»»ä½•å±æ€§éƒ½è¿”å› None
    def __call__(self, *args, **kwargs):
        return None  # è°ƒç”¨ä»»ä½•æ–¹æ³•éƒ½è¿”å› None
    def __getitem__(self, key):
        return None
    def get(self, key, default=None):
        return default
    def __bool__(self):
        return False # å…³é”®ï¼šè®© if obj: åˆ¤æ–­ä¸º False

# 1. åˆ›å»ºå‡çš„ managers
class MockManagers:
    # æ¨¡æ‹Ÿ cache
    class MockCache:
        def get(self, *args): return None  # æ ¸å¿ƒï¼šå¼ºåˆ¶æœªå‘½ä¸­ç¼“å­˜
        def set(self, *args): pass         # æ ¸å¿ƒï¼šå‡è£…å†™å…¥ç¼“å­˜ï¼Œå®é™…å•¥ä¹Ÿä¸å¹²
        def cleanup_expired(self): pass
        def _get_filename(self, *args): return "/tmp/dummy"

    # æ¨¡æ‹Ÿ db
    class MockDB:
        def get_val(self, *args): return None
        def list_all(self): return {"data": {}}
        def update(self, *args): return {"status": "success"}
    
    # æ¨¡æ‹Ÿå…¶ä»–ç»„ä»¶ (å…¨éƒ¨è¿”å› EmptyObject)
    class MockGeneric(EmptyObject):
        def load(self, *args): return {} # load è¿”å›ç©ºå­—å…¸ï¼Œæ–¹ä¾¿è¿­ä»£
        def get_all(self, *args): return {}

    # å®ä¾‹åŒ–
    cache = MockCache()
    db = MockDB()
    offline_manager = MockGeneric()
    booklist_manager = MockGeneric()
    tag_manager = MockGeneric()
    stats_manager = MockGeneric()
    history_manager = MockGeneric()
    update_manager = MockGeneric()
    role_manager = MockGeneric()

    # æ¨¡æ‹Ÿé…ç½®å˜é‡ (é˜²æ­¢æŠ¥é”™)
    USER_DATA_DIR = "/tmp"
    CACHE_DIR = "/tmp"
    DL_DIR = "/tmp"
    # æ¨¡æ‹Ÿ cluster_manager (é˜²æ­¢å»¶è¿Ÿå¯¼å…¥æŠ¥é”™)
    cluster_manager = MockGeneric() 

# 2. å¼ºè¡Œæ³¨å…¥ç³»ç»Ÿæ¨¡å—
# è¿™æ · spider_core å¯¼å…¥ managers æ—¶ï¼Œæ‹¿åˆ°çš„å°±æ˜¯æˆ‘ä»¬å®šä¹‰çš„è¿™ä¸ªâ€œå“‘å·´â€å¯¹è±¡
sys.modules['managers'] = MockManagers()
sys.modules['managers.cache'] = MockManagers.cache

# =========================================================
# å¯¼å…¥çˆ¬è™«æ ¸å¿ƒ (å¿…é¡»åœ¨æ³¨å…¥ä¹‹å)
# =========================================================
from spider_core import crawler_instance as crawler, searcher

# === é…ç½®åŒº ===
MASTER_URL = os.environ.get("MASTER_URL", "https://book.ztrztr.top")
AUTH_TOKEN = os.environ.get("REMOTE_CRAWLER_TOKEN", "my-secret-token-888")
NODE_NAME = os.environ.get("NODE_NAME", "Worker-Node")
NODE_UUID = str(uuid.uuid4())

CURRENT_TASKS = 0
TASK_LOCK = threading.Lock()

# è¡¥å…¨é…ç½®å¯¹è±¡
NODE_CONFIG = {
    "name": NODE_NAME,
    "region": os.environ.get("NODE_REGION", "GLOBAL"),
    "max_tasks": int(os.environ.get("NODE_MAX_TASKS", 20)),
    "public_url": os.environ.get("NODE_PUBLIC_URL", ""),
    "port": int(os.environ.get("PORT", 12345))
}

def get_node_payload():
    return {
        "uuid": NODE_UUID,
        "config": NODE_CONFIG,
        "status": {
            "cpu": psutil.cpu_percent(interval=None),
            "memory": psutil.virtual_memory().percent,
            "current_tasks": CURRENT_TASKS,
            "timestamp": time.time()
        }
    }
def run_speedtest_async(task):
    """
    [æ–°å¢] ç‹¬ç«‹çš„æµ‹é€Ÿçº¿ç¨‹å‡½æ•°
    """
    payload = task['payload']
    target_url = payload.get('url')
    print(f"ğŸš€ [SpeedTest] åå°å¯åŠ¨æµ‹é€Ÿ: {target_url}")
    
    try:
        import time
        t_start = time.time()
        status_code = 0
        error_msg = ""
        size = 0
        
        try:
            # ä½¿ç”¨ requests ç›´æ¥æµ‹é€Ÿ (ä¸èµ° curl_cffiï¼Œæ›´è½»é‡)
            # è®¾ç½®çŸ­è¶…æ—¶ï¼Œé˜²æ­¢å¡çº¿ç¨‹
            r = requests.get(
                target_url, 
                headers={'User-Agent': 'Mozilla/5.0'}, 
                timeout=10, 
                verify=False
            )
            status_code = r.status_code
            size = len(r.content)
        except Exception as req_e:
            error_msg = str(req_e)
        
        latency = int((time.time() - t_start) * 1000)
        print("[latency]", latency)
        # æ„é€ ç»“æœ
        result = {
            "is_speedtest": True,
            "worker_uuid": NODE_UUID,
            "worker_name": NODE_CONFIG['name'],
            "region": NODE_CONFIG['region'],
            "target": target_url,
            "latency": latency,
            "status_code": status_code,
            "size": size,
            "error": error_msg
        }
        
        # ç‹¬ç«‹å›ä¼ ç»“æœ (ä¸èµ°ä¸»å¾ªç¯)
        # é‡æ–°å»ºç«‹ä¸€ä¸ª session ä¹Ÿå¯ä»¥ï¼Œæˆ–è€…ç›´æ¥ post
        requests.post(
            f"{MASTER_URL}/api/cluster/submit_result",
            json={"task_id": task['id'], "result": result},
            headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
            timeout=5
        )
        # print(f"âœ… [SpeedTest] ç»“æœå·²å›ä¼ : {latency}ms")
        
    except Exception as e:
        print(f"âŒ [SpeedTest] çº¿ç¨‹å‡ºé”™: {e}")
def do_work(task):
    """æ‰§è¡Œå…·ä½“ä»»åŠ¡"""
    global CURRENT_TASKS # å£°æ˜å…¨å±€å˜é‡
    endpoint = task['endpoint']
    payload = task['payload']
    url = payload.get('url')
    
    print(f"âš¡ [Job] æ‰§è¡Œ: {endpoint} -> {url}")
    
    result = {"status": "failed", "msg": "Unknown error", "worker_uuid": NODE_UUID}
    
    with TASK_LOCK: CURRENT_TASKS += 1
    try:
        data = None
        # [å…³é”®ä¿®å¤] WorkerèŠ‚ç‚¹å¼ºåˆ¶æœ¬åœ°çˆ¬å–ï¼Œè·³è¿‡_remote_request
        # è®¾ç½®ç¯å¢ƒå˜é‡å‘Šè¯‰crawlerè·³è¿‡è¿œç¨‹è¯·æ±‚
        import os
        original_flag = os.environ.get('FORCE_LOCAL_CRAWL')
        os.environ['FORCE_LOCAL_CRAWL'] = '1'
        
        try:
            # å¼ºåˆ¶çˆ¬å–é€»è¾‘
            if endpoint == 'run':
                data = crawler.run(url)
            elif endpoint == 'toc':
                data = crawler.get_toc(url)
            elif endpoint == 'search':
                data = searcher.search_bing(payload.get('keyword'))
        finally:
            # æ¢å¤ç¯å¢ƒå˜é‡
            if original_flag is None:
                os.environ.pop('FORCE_LOCAL_CRAWL', None)
            else:
                os.environ['FORCE_LOCAL_CRAWL'] = original_flag
            
        if data:
            # æ•°æ®æ¸…æ´—ï¼šé˜²æ­¢ä»»ä½•éæ ‡å¯¹è±¡æ··å…¥
            # æœ‰æ—¶å€™ soup å¯¹è±¡æˆ–è€… lxml å¯¹è±¡ä¼šæ··è¿›æ¥ï¼Œå¯¼è‡´ JSON åºåˆ—åŒ–å¤±è´¥
            try:
                json.dumps(data) 
            except TypeError:
                print("âš ï¸ æ£€æµ‹åˆ°ä¸å¯åºåˆ—åŒ–æ•°æ®ï¼Œæ‰§è¡Œæ·±åº¦æ¸…æ´—...")
                data = clean_data(data)

            result = {"status": "success", "data": data, "worker_uuid": NODE_UUID}
        else:
            result = {"status": "failed", "msg": "Empty data from crawler", "worker_uuid": NODE_UUID}
            
    except Exception as e:
        print(f"âŒ ä»»åŠ¡å‡ºé”™: {e}")
        # import traceback; traceback.print_exc() # è°ƒè¯•ç”¨
        result = {"status": "error", "msg": str(e), "worker_uuid": NODE_UUID}
    finally:
        with TASK_LOCK: CURRENT_TASKS -= 1
        
    return result

def clean_data(obj):
    """é€’å½’æ¸…æ´—æ•°æ®ï¼ŒæŠŠæ‰€æœ‰éåŸºæœ¬ç±»å‹è½¬ä¸ºå­—ç¬¦ä¸²"""
    if isinstance(obj, dict):
        return {k: clean_data(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_data(v) for v in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:
        return str(obj) 

def worker_loop():
    print(f"ğŸš€ Worker [{NODE_NAME}] å¯åŠ¨ (Pull Mode)")
    print(f"ğŸ†” UUID: {NODE_UUID}")
    print(f"ğŸ”— è¿æ¥ Master: {MASTER_URL}")
    print(f"ğŸ›¡ï¸  Mockå±‚å·²å°±ç»ªï¼Œå…¨é‡å®æ—¶çˆ¬å–")
    
    session = requests.Session()
    session.headers.update({
        "Authorization": f"Bearer {AUTH_TOKEN}",
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) SmartNoteDB-Worker"
    })
    
    while True:
        try:
            # å–ä»»åŠ¡ (å¸¦çŠ¶æ€)
            resp = session.post(f"{MASTER_URL}/api/cluster/fetch_task", json=get_node_payload(), timeout=10)
            
            if resp.status_code == 403:
                print("ğŸ”’ Token é”™è¯¯")
                time.sleep(10); continue
            
            try:
                res_json = resp.json()
            except:
                time.sleep(5); continue
            
            if res_json.get('status') == 'success':
                task = res_json['task']
                task_id = task['id']
                endpoint = task.get('endpoint') # Master ä¼ å›æ¥çš„ endpoint
                
                # === [æ ¸å¿ƒé€»è¾‘] åˆ†æµå¤„ç† ===
                if endpoint == 'speedtest':
                    # 1. å¦‚æœæ˜¯æµ‹é€Ÿä»»åŠ¡ï¼Œå¯åŠ¨çº¿ç¨‹ï¼Œç«‹å³ç»§ç»­å¾ªç¯
                    threading.Thread(target=run_speedtest_async, args=(task,)).start()
                    # ä¸ sleepï¼Œç«‹å³å»å–ä¸‹ä¸€ä¸ªå¯èƒ½çš„çˆ¬è™«ä»»åŠ¡
                    continue 
                else:
                    # 2. å¦‚æœæ˜¯çˆ¬è™«ä»»åŠ¡ï¼Œé˜»å¡æ‰§è¡Œ (é˜²æ­¢å¹¶å‘è¿‡é«˜)
                    crawl_result = do_work(task)
                    
                    # å›ä¼ 
                    session.post(f"{MASTER_URL}/api/cluster/submit_result", json={
                        "task_id": task_id,
                        "result": crawl_result
                    })
                    print(f"âœ… [Job] ä»»åŠ¡ {task_id} å®Œæˆ")
            else:
                time.sleep(1) # æ²¡ä»»åŠ¡ï¼Œä¼‘æ¯
                
        except Exception as e:
            print(f"âš ï¸ ç½‘ç»œæ³¢åŠ¨: {e}")
            time.sleep(5)

# ç‹¬ç«‹å¿ƒè·³çº¿ç¨‹ (ä½œä¸ºè¡¥å……)
def heartbeat_thread():
    while True:
        try:
            requests.post(
                f"{MASTER_URL}/api/cluster/heartbeat", 
                json=get_node_payload(),
                headers={"Authorization": f"Bearer {AUTH_TOKEN}"},
                timeout=5
            )
        except: pass
        time.sleep(10)

if __name__ == '__main__':
    threading.Thread(target=heartbeat_thread, daemon=True).start()
    worker_loop()