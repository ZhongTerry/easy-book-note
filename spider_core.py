import time
import random
import re
import os
import importlib.util
import hashlib
from urllib.parse import urljoin, urlparse 
from urllib.request import getproxies
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
from lxml import html as lxml_html
from pypinyin import lazy_pinyin, Style
from concurrent.futures import ThreadPoolExecutor, as_completed
from ebooklib import epub
from werkzeug.utils import secure_filename
from shared import BASE_DIR, LIB_DIR
from curl_cffi import requests as cffi_requests, CurlHttpVersion

# ==========================================
# 0. è¾…åŠ©å·¥å…· (ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­— - å¢å¼ºç‰ˆ)
# ==========================================
def parse_chapter_id(text):
    if not text: return -1
    text = text.strip()
    
    # 1. ä¼˜å…ˆåŒ¹é…çº¯æ•°å­— (ä¾‹å¦‚: "49. ç« èŠ‚å" æˆ– "ç¬¬49ç« ")
    match_num = re.search(r'(?:ç¬¬)?\s*(\d+)\s*[ç« èŠ‚å›å¹•\.]', text)
    if match_num: 
        return int(match_num.group(1))
        
    # 2. åŒ¹é…ä¸­æ–‡æ•°å­— (ä¾‹å¦‚: "ç¬¬åä¸€ç« ")
    # æ³¨æ„ï¼šè¿™é‡ŒæŠŠä¸¤ã€åƒã€ä¸‡ç­‰éƒ½åŠ å…¨äº†
    match_cn = re.search(r'(?:ç¬¬)?\s*([é›¶ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)\s*[ç« èŠ‚å›å¹•]', text)
    if match_cn: 
        return _smart_convert_int(match_cn.group(1))
        
    # 3. å®åœ¨ä¸è¡Œï¼ŒåŒ¹é…å¼€å¤´çš„æ•°å­— (ä¾‹å¦‚ "123 ç« èŠ‚å")
    match_start = re.search(r'^(\d+)', text)
    if match_start: 
        return int(match_start.group(1))
        
    return -1

def _smart_convert_int(s):
    """
    å°†ä¸­æ–‡æ•°å­—è½¬æ¢ä¸ºé˜¿æ‹‰ä¼¯æ•°å­— (æ”¯æŒ: åä¸€ -> 11, ä¸€ç™¾é›¶äº” -> 105)
    """
    # å°è¯•ç›´æ¥è½¬æ•°å­— (é˜²æ­¢ä¼ å…¥çš„æ˜¯ "123")
    try: return int(s)
    except: pass

    # æ˜ å°„è¡¨
    cn_nums = {'é›¶': 0, 'ä¸€': 1, 'äºŒ': 2, 'ä¸¤': 2, 'ä¸‰': 3, 'å››': 4, 
               'äº”': 5, 'å…­': 6, 'ä¸ƒ': 7, 'å…«': 8, 'ä¹': 9}
    cn_units = {'å': 10, 'ç™¾': 100, 'åƒ': 1000, 'ä¸‡': 10000}

    # [æ ¸å¿ƒä¿®å¤] ç‰¹æ®Šå¤„ç†ä»¥"å"å¼€å¤´çš„æ•°å­— (å¦‚: åä¸€ => ä¸€åä¸€, åäº” => ä¸€åäº”)
    if s.startswith('å'):
        s = 'ä¸€' + s

    result = 0
    temp_val = 0 # æš‚å­˜å½“å‰è¯»å–çš„æ•°å­—
    
    for char in s:
        if char in cn_nums:
            temp_val = cn_nums[char]
        elif char in cn_units:
            unit = cn_units[char]
            if unit >= 10000:
                # å¤„ç†"ä¸‡"è¿™ç§å¤§å•ä½ï¼Œå…ˆç»“ç®—å‰é¢çš„
                result = (result + temp_val) * unit
                temp_val = 0
            else:
                # å¤„ç†"å/ç™¾/åƒ"
                result += temp_val * unit
                temp_val = 0
    
    # åŠ ä¸Šæœ€åå‰©ä¸‹çš„ä¸ªä½æ•°
    result += temp_val
    return result
    

# ==========================================
# 1. æ’ä»¶ç®¡ç†å™¨
# ==========================================
class AdapterManager:
    def __init__(self, folder="adapters"):
        self.folder = os.path.join(BASE_DIR, folder)
        self.adapters = []
        if not os.path.exists(self.folder): os.makedirs(self.folder)
        self.load_plugins()

    def load_plugins(self):
        self.adapters = []
        for f in os.listdir(self.folder):
            if f.endswith(".py") and f != "__init__.py":
                try:
                    spec = importlib.util.spec_from_file_location(f[:-3], os.path.join(self.folder, f))
                    mod = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(mod)
                    for n in dir(mod):
                        obj = getattr(mod, n)
                        if isinstance(obj, type) and "Adapter" in n: self.adapters.append(obj())
                except: pass
        print(f"[System] å·²åŠ è½½ {len(self.adapters)} ä¸ªç«™ç‚¹é€‚é…æ’ä»¶")

    def find_match(self, url):
        for a in self.adapters:
            if hasattr(a, 'can_handle') and a.can_handle(url): return a
        return None

plugin_mgr = AdapterManager()
from functools import lru_cache
import requests
# from curl_cffi import requests as CurlHttpVersion
# ==========================================
# 2. æœç´¢åŠ©æ‰‹
# ==========================================


# ==========================================
# 2. æœç´¢åŠ©æ‰‹ (è°ƒè¯•å¢å¼ºç‰ˆ)
# ==========================================import re
import time
from urllib.parse import urlparse, parse_qs, unquote, urljoin
from bs4 import BeautifulSoup
from curl_cffi import requests as cffi_requests
from pypinyin import lazy_pinyin, Style
from concurrent.futures import ThreadPoolExecutor, as_completed

class SearchHelper:
    def __init__(self):
        # [Owllook é…ç½®] æ¨¡æ‹Ÿ Chrome æŒ‡çº¹ï¼Œè¿™æ˜¯è¿‡ç›¾çš„å…³é”®
        self.impersonate = "chrome110" 
        self.timeout = 10
        
        # [Owllook ç§»æ¤] åŸŸåé»‘åå• (Black Domain)
        # æ¥æº: owllook/config/config.py
        self.black_domains = {
            'baidu.com', 'tieba.baidu.com', 'zhidao.baidu.com', 'wenku.baidu.com',
            # 'so.com', 'baike.so.com', 'wenda.so.com',
            'zhihu.com', 'douban.com', '163.com', 'qq.com', 'sina.com.cn',
            'amazon.cn', 'dangdang.com', 'jd.com', 'tmall.com', 'taobao.com',
            # 'qidian.com', 'zongheng.com', '17k.com', 'faloo.com', 'jjwxc.net',
            'facebook.com', 'twitter.com', 'youtube.com', 'bilibili.com'
        }
        self.plugins = []
        self._load_search_plugins()
        self.sites = [
            {
                "name": "ç¬”è¶£é˜.cc", 
                "url": "https://www.biquge.cc", 
                "search": "/search.php", 
                "param": "q", 
                "encoding": "gbk" # GBKç¼–ç ç«™ç‚¹
            },
            {
                "name": "ç¬”è¶£å¡", 
                "url": "https://www.bqgka.com", 
                "search": "/search.php", 
                "param": "q", 
                "encoding": "utf-8"
            },
            {
                "name": "52å°è¯´", 
                "url": "https://www.52bqg.cc", 
                "search": "/modules/article/search.php", 
                "param": "searchkey", 
                "encoding": "gbk"
            },
            {
                "name": "æ–°ç¬”è¶£é˜", 
                "url": "https://www.xbiquge.so", 
                "search": "/search.php", 
                "param": "keyword", 
                "encoding": "utf-8"
            },
            {
                "name": "23å°è¯´", 
                "url": "https://www.23us.so", 
                "search": "/files/article/search.html", 
                "param": "searchkey", 
                "encoding": "gbk"
            }
        ]

    def _search_single_site(self, site, keyword):
        """æœç´¢å•ä¸ªç«™ç‚¹"""
        results = []
        try:
            # 1. ç¼–ç å¤„ç†
            if site['encoding'] == 'gbk':
                # GBK ç«™ç‚¹é€šå¸¸éœ€è¦æ‰‹åŠ¨ç¼–ç å‚æ•°
                kw_val = keyword.encode('gbk')
            else:
                kw_val = keyword

            params = {site['param']: kw_val}
            
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Referer': site['url']
            }

            # 2. å‘èµ·è¯·æ±‚ (çŸ­è¶…æ—¶ï¼Œå¿«é€Ÿå¤±è´¥)
            resp = requests.get(
                f"{site['url']}{site['search']}", 
                params=params, 
                headers=headers, 
                timeout=6, 
                verify=False
            )
            
            # 3. å¼ºåˆ¶è®¾ç½®ç¼–ç é˜²æ­¢ä¹±ç 
            resp.encoding = site['encoding']
            
            # 4. é€šç”¨è§£æé€»è¾‘
            soup = BeautifulSoup(resp.text, 'html.parser')
            
            # å°è¯•åŒ¹é…å¸¸è§çš„ç¬”è¶£é˜åˆ—è¡¨ç»“æ„
            items = []
            # ç»“æ„A: .result-list .result-item (xbiqugeç±»)
            items.extend(soup.select('.result-list .result-item'))
            # ç»“æ„B: .grid tr (æ°å¥‡CMSç±»)
            items.extend(soup.select('tr')) 
            # ç»“æ„C: .novelslist2 li (éƒ¨åˆ†è€ç«™)
            items.extend(soup.select('li'))

            for item in items:
                try:
                    # å°è¯•å¯»æ‰¾é“¾æ¥
                    link = item.find('a', href=True)
                    if not link: continue
                    
                    href = link['href']
                    title = link.get_text(strip=True)
                    
                    # è¿‡æ»¤æ— æ•ˆé“¾æ¥
                    if not title or len(title) < 2: continue
                    if "å°è¯´" in title and len(title) > 20: continue # è¿‡æ»¤å¯¼èˆªæ 
                    
                    # æ¨¡ç³ŠåŒ¹é…ï¼šåªæœ‰åŒ…å«å…³é”®è¯æ‰æ”¶å½• (é˜²æ­¢è§£æåˆ°é¡µçœ‰é¡µè„š)
                    if keyword not in title: continue

                    # æå–ä½œè€… (å°è¯•æ‰¾é™„è¿‘çš„æ–‡æœ¬)
                    text_content = item.get_text()
                    author = "æœªçŸ¥"
                    if "ä½œè€…ï¼š" in text_content:
                        author = text_content.split("ä½œè€…ï¼š")[1].split()[0].strip()
                    elif item.find_next_sibling('td'): # è¡¨æ ¼ç»“æ„ä½œè€…åœ¨ä¸‹ä¸€åˆ—
                        author = item.find_next_sibling('td').get_text(strip=True)

                    # URL è¡¥å…¨
                    if not href.startswith('http'):
                        href = urljoin(site['url'], href)
                    
                    # ä¿®æ­£ç›®å½•é¡µ (éƒ¨åˆ†ç«™ç‚¹æœå‡ºæ¥æ˜¯è¯¦æƒ…é¡µ /book/123/ï¼Œéœ€è¦è½¬ /123/)
                    # è¿™é‡Œä¿æŒåŸæ ·ï¼Œäº¤ç»™çˆ¬è™«æ ¸å¿ƒå»çº é”™ï¼Œæˆ–è€…ç®€å•æ›¿æ¢
                    
                    results.append({
                        'title': title,
                        'url': href,
                        'source': f"{site['name']} ğŸ“š",
                        'description': f"ä½œè€…: {author}"
                    })
                    
                    if len(results) >= 3: break # æ¯ä¸ªç«™åªå–å‰3ä¸ª
                except: continue

        except Exception as e:
            # print(f"[Universal] {site['name']} Error: {e}")
            pass
            
        return results

    def search(self, keyword):
        print(f"[Plugin] ğŸš€ å¯åŠ¨ç¬”è¶£é˜èšåˆæœç´¢ ({len(self.sites)}ä¸ªæº)...")
        all_results = []
        
        # çº¿ç¨‹æ± å¹¶å‘æœç´¢æ‰€æœ‰æº
        with ThreadPoolExecutor(max_workers=5) as exe:
            futures = [exe.submit(self._search_single_site, site, keyword) for site in self.sites]
            
            for future in as_completed(futures):
                res = future.result()
                if res:
                    all_results.extend(res)
        
        return all_results

    def _load_search_plugins(self):
        """åŠ¨æ€åŠ è½½ search_plugins ç›®å½•ä¸‹çš„æ‰€æœ‰æ’ä»¶"""
        plugin_dir = os.path.join(BASE_DIR, 'search_plugins')
        if not os.path.exists(plugin_dir):
            os.makedirs(plugin_dir)
            return

        print(f"[System] æ­£åœ¨åŠ è½½æœç´¢æ’ä»¶...")
        for filename in os.listdir(plugin_dir):
            if filename.endswith(".py") and filename != "__init__.py":
                try:
                    # åŠ¨æ€å¯¼å…¥æ¨¡å—
                    module_name = filename[:-3]
                    file_path = os.path.join(plugin_dir, filename)
                    
                    spec = importlib.util.spec_from_file_location(module_name, file_path)
                    module = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(module)
                    
                    # å¯»æ‰¾æ’ä»¶ç±» (çº¦å®šç±»åä¸º SourceWorker)
                    if hasattr(module, 'SourceWorker'):
                        plugin_instance = module.SourceWorker()
                        self.plugins.append(plugin_instance)
                        print(f"  -> å·²åŠ è½½æº: {plugin_instance.source_name}")
                except Exception as e:
                    print(f"  -> æ’ä»¶ {filename} åŠ è½½å¤±è´¥: {e}")
        
        print(f"[System] å…±åŠ è½½ {len(self.plugins)} ä¸ªç›´è¿æœç´¢æº")
    
    def get_pinyin_key(self, text):
        clean = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
        clean = re.sub(r'(å°è¯´|ç¬”è¶£é˜|æœ€æ–°ç« èŠ‚|å…¨æ–‡é˜…è¯»)', '', clean)
        try:
            s = lazy_pinyin(clean, style=Style.FIRST_LETTER)
            k = ''.join(s).lower()
            return k[:15] if k else "temp"
        except: return "temp"

    def _clean_title(self, title):
        if not title: return "æœªçŸ¥æ ‡é¢˜"
        return re.split(r'(-|_|\|)', title)[0].strip()

    def _is_valid_result(self, title, url):
        """
        [Owllook ç§»æ¤] ç»“æœæ ¡éªŒé€»è¾‘
        """
        if not url or not url.startswith('http'): return False
        
        netloc = urlparse(url).netloc
        
        # 1. é»‘åå•æ ¡éªŒ
        for domain in self.black_domains:
            if domain in netloc: return False
            
        # 2. å¿…é¡»æ˜¯ html ç»“å°¾æˆ–è€…æ˜¯ç›®å½•é¡µ (Owllook åå¥½)
        # if '.html' not in url and not url.endswith('/'): return False
        
        # 3. å…³é”®è¯æ ¡éªŒ
        bad_keywords = ['ä¸‹è½½', 'txt', 'ç²¾æ ¡', 'ç™¾ç§‘', 'æ‰‹æ¸¸', 'è§†é¢‘', 'åœ¨çº¿è§‚çœ‹']
        if any(k in title.lower() for k in bad_keywords): return False
        
        return True

    def _get_real_url(self, url):
        """
        [Owllook ç§»æ¤] è§£æçœŸå® URL (Get Real URL)
        æ ¸å¿ƒï¼šå¤„ç†ç™¾åº¦å’Œ360çš„åŠ å¯†è·³è½¬é“¾æ¥
        """
        # å¦‚æœä¸æ˜¯åŠ å¯†é“¾ï¼Œç›´æ¥è¿”å›
        if "baidu.com/link" not in url and "so.com/link" not in url:
            return url
            
        try:
            # 1. å°è¯• HEAD è¯·æ±‚ (Owllook ç­–ç•¥: async with client.head...)
            # ç¦æ­¢è‡ªåŠ¨è·³è½¬ï¼Œåªçœ‹ Location
            resp = cffi_requests.head(
                url, 
                impersonate=self.impersonate, 
                timeout=5, 
                allow_redirects=False
            )
            
            if resp.status_code in [301, 302]:
                real_url = resp.headers.get('Location') or resp.headers.get('location')
                if real_url and "baidu.com" not in real_url and "so.com" not in real_url:
                    return real_url

            # 2. å¦‚æœ HEAD å¤±è´¥ï¼Œå°è¯• GET (é’ˆå¯¹ 360 çš„ JS è·³è½¬)
            resp = cffi_requests.get(
                url,
                impersonate=self.impersonate,
                timeout=8,
                allow_redirects=False
            )
            
            if resp.status_code == 200:
                html = resp.text
                # 360 ç‰¹æœ‰çš„ JS è·³è½¬æå–
                js_match = re.search(r"window\.location\.replace\(['\"](.+?)['\"]", html)
                if js_match: return js_match.group(1)
                
                meta_match = re.search(r'url=([^"]+)"', html, re.IGNORECASE)
                if meta_match: return meta_match.group(1)

        except Exception: 
            pass
            
        return url

    # ==========================================
    # å¼•æ“ 1: 360æœç´¢ (SoNovels)
    # ==========================================
    def _do_so_search(self, keyword):
        print(f"[Search] ğŸ” å¯åŠ¨ Owllook-360 å¼•æ“: {keyword}")
        url = "https://www.so.com/s"
        # Owllook å‚æ•°: ie=utf-8, src=noscript_home, shb=1
        params = {'q': keyword, 'ie': 'utf-8', 'src': 'noscript_home', 'shb': 1, 'pn': 1}
        
        try:
            res = []
            for i in range(1, 3) :
                params['pn'] = i
                resp = cffi_requests.get(url, params=params, impersonate=self.impersonate, timeout=self.timeout)
                soup = BeautifulSoup(resp.content, 'html.parser')
                
                raw_results = []
                # Owllook é€‰æ‹©å™¨: .res-list
                items = soup.select('.res-list')
                print(len(items))
                for item in items:
                    try:
                        title_tag = item.select_one('h3 a')
                        if not title_tag: continue
                        
                        title = title_tag.get_text(strip=True)
                        href = title_tag.get('href')
                        
                        # Owllook: é’ˆå¯¹ä¸åŒçš„è¯·æ±‚è¿›è¡Œ url çš„æå–
                        if "www.so.com/link?m=" in href:
                            href = title_tag.get('data-mdurl') or href
                        if "www.so.com/link?url=" in href:
                            qs = parse_qs(urlparse(href).query)
                            if 'url' in qs: href = qs['url'][0]
                        
                        # if self._is_valid_result(title, href):
                        if True:
                            raw_results.append({
                                'title': self._clean_title(title),
                                'url': href, # å¯èƒ½æ˜¯åŠ å¯†é“¾ï¼Œç¨åè§£æ
                                'suggested_key': self.get_pinyin_key(keyword),
                                'source': '360 (Owllook)'
                            })

                    except: continue
                    for item in raw_results :
                        res.append(item)
                    if len(raw_results) >= 10: break
            return self._concurrent_resolve(res)
        except Exception as e:
            print(f"[Search] So Error: {e}")
            return []
            

        
    def _resolve_real_url(self, url):
        """
        [æ ¸å¿ƒä¿®å¤] è§£æçœŸå® URL
        é’ˆå¯¹æœåŠ¡å™¨ IPï¼Œ360 ç»å¸¸è¿”å›ä¸€ä¸ª 200 OK çš„ä¸­é—´é¡µï¼Œè€Œä¸æ˜¯ 302 è·³è½¬
        """
        if "so.com/link" not in url and "baidu.com/link" not in url:
            return url
            
        try:
            # è¿™é‡Œä½¿ç”¨æ ‡å‡† requestsï¼Œå› ä¸ºå¤„ç†é‡å®šå‘å’Œ header æ¯”è¾ƒæ–¹ä¾¿ä¸”ç¨³å®š
            # timeout è®¾ç½®çŸ­ä¸€ç‚¹ï¼Œå¿«é€Ÿå¤±è´¥
            resp = requests.get(
                url, 
                headers={'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'},
                timeout=6, 
                allow_redirects=False, # ç¦æ­¢è‡ªåŠ¨è·³è½¬ï¼Œæˆ‘ä»¬è¦æ‹¦æˆªç¬¬ä¸€è·³
                verify=False
            )
            
            # æƒ…å†µ 1: æ ‡å‡† 302 è·³è½¬
            if resp.status_code in [301, 302]:
                return resp.headers.get('Location') or url
            
            # æƒ…å†µ 2: æœåŠ¡å™¨ IP å¸¸è§çš„ "æ­£åœ¨è·³è½¬..." ä¸­é—´é¡µ
            if resp.status_code == 200:
                html = resp.text
                # æå– window.location.replace("...")
                js_match = re.search(r"window\.location\.replace\(['\"](.+?)['\"]", html)
                if js_match: 
                    return js_match.group(1)
                
                # æå– <meta http-equiv="refresh" content="0;url=...">
                meta_match = re.search(r'url=([^"]+)"', html, re.IGNORECASE)
                if meta_match: 
                    return meta_match.group(1)

        except Exception:
            pass
            
        # è§£æå¤±è´¥è¿”å›åŸåŠ å¯†é“¾æ¥ï¼Œåç»­ä¼šè¢«æ¸…æ´—æ‰
        return url
    # ==========================================
    # å¼•æ“ 2: ç™¾åº¦æœç´¢ (BaiduNovels)
    # ==========================================
    def _do_baidu_search(self, keyword):
        print(f"[Search] ğŸ” å¯åŠ¨ Owllook-Baidu å¼•æ“: {keyword}")
        url = "https://www.baidu.com/s"
        
        # [Owllook å‚æ•°]
        # rn: æ¯é¡µæ¡æ•° (Owllook è®¾ä¸º 15ï¼Œæˆ‘ä»¬è®¾ 10)
        # vf_bl: 1 (è¿™ä¸ªå‚æ•°å¾ˆé‡è¦ï¼Œæœ‰æ—¶èƒ½å‡å°‘å¹¿å‘Š)
        params = {'wd': f"{keyword} å°è¯´ æœ€æ–°ç« èŠ‚", 'ie': 'utf-8', 'rn': 10, 'vf_bl': 1}
        
        try:
            # ç™¾åº¦åçˆ¬è¾ƒä¸¥ï¼Œå¿…é¡»å¸¦ Referer
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
                'Referer': 'https://www.baidu.com/'
            }
            # ä½¿ç”¨ curl_cffi æ¨¡æ‹ŸæŒ‡çº¹ï¼Œé€šè¿‡ç‡æ¯” requests é«˜
            resp = cffi_requests.get(url, params=params, headers=headers, impersonate=self.impersonate, timeout=8)
            
            if "å®‰å…¨éªŒè¯" in resp.text or "wappass" in resp.url:
                print("[Search] ç™¾åº¦è§¦å‘éªŒè¯ç ")
                return []
                
            soup = BeautifulSoup(resp.content, 'html.parser')
            raw_results = []
            
            # [Owllook é€‰æ‹©å™¨]
            # å…¼å®¹æ—§ç‰ˆ .result å’Œæ–°ç‰ˆ .c-container
            items = soup.select('div.result') or soup.select('div.c-container')
            
            for item in items:
                try:
                    # æå–æ ‡é¢˜é“¾æ¥ (h3.t a æ˜¯ç™¾åº¦ç»å…¸ç»“æ„)
                    title_tag = item.select_one('h3.t a') or item.select_one('h3 a') or item.select_one('a')
                    if not title_tag: continue
                    
                    title = title_tag.get_text(strip=True)
                    href = title_tag.get('href') # è¿™æ˜¯ä¸€ä¸ªåŠ å¯†é“¾æ¥
                    
                    if not href: continue

                    if self._is_valid_result(title, href):
                        raw_results.append({
                            'title': self._clean_title(title),
                            'url': href,
                            'suggested_key': self.get_pinyin_key(keyword),
                            'source': 'Baidu (Owllook)'
                        })
                except: continue
                if len(raw_results) >= 8: break
            
            # ç™¾åº¦é“¾æ¥å…¨æ˜¯åŠ å¯†çš„ï¼Œå¿…é¡»å¹¶å‘è§£å¯†
            return self._concurrent_resolve(raw_results)

        except Exception as e:
            print(f"[Search] Baidu Error: {e}")
            return []
    # ==========================================
    # å¼•æ“ 3: å¿…åº”æœç´¢ (BingNovels)
    # ==========================================
    def _do_bing_search(self, keyword):
        print(f"[Search] ğŸ” å¯åŠ¨ Owllook-Bing å¼•æ“: {keyword}")
        url = "https://www.bing.com/search"
        
        # [Owllook å‚æ•°]
        # ensearch=0: å¼ºåˆ¶ä¸­æ–‡æœç´¢é€»è¾‘
        params = {'q': f"{keyword} å°è¯´ ç›®å½•", 'ensearch': 0}
        
        try:
            # Bing éœ€è¦ Referer
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36',
                'Referer': 'https://www.bing.com/'
            }
            resp = cffi_requests.get(url, params=params, headers=headers, impersonate=self.impersonate, timeout=10)
            soup = BeautifulSoup(resp.content, 'html.parser')
            results = []
            
            # [Owllook é€‰æ‹©å™¨]
            # .b_algo æ˜¯ Bing æœç´¢ç»“æœçš„æ ‡å‡†å®¹å™¨
            items = soup.select('li.b_algo')
            
            for item in items:
                try:
                    title_tag = item.select_one('h2 a')
                    if not title_tag: continue
                    
                    title = title_tag.get_text(strip=True)
                    href = title_tag.get('href')
                    
                    if not href: continue

                    # è¿‡æ»¤æ‰ç™¾åº¦ç™¾ç§‘ç­‰åœ¨ Bing ä¸­çš„ç»“æœ
                    if "baike.baidu.com" in href: continue

                    if self._is_valid_result(title, href):
                        results.append({
                            'title': self._clean_title(title),
                            'url': href,
                            'suggested_key': self.get_pinyin_key(keyword),
                            'source': 'Bing (Owllook)'
                        })
                except: continue
                if len(results) >= 8: break
            
            return results

        except Exception as e:
            print(f"[Search] Bing Error: {e}")
            return []
    def _do_direct_source_search(self, keyword):
        if not self.plugins:
            return []
            
        print(f"[Search] ğŸ§± å¯åŠ¨ç›´è¿æ’ä»¶æœç´¢ (å…±{len(self.plugins)}ä¸ª): {keyword}")
        all_results = []
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘è°ƒç”¨æ‰€æœ‰æ’ä»¶
        with ThreadPoolExecutor(max_workers=len(self.plugins)) as exe:
            future_to_plugin = {
                exe.submit(plugin.search, keyword): plugin 
                for plugin in self.plugins
            }
            
            for future in as_completed(future_to_plugin):
                plugin = future_to_plugin[future]
                try:
                    res = future.result()
                    if res:
                        # ç»™ç»“æœè¡¥ä¸Š pinyin_key (æ’ä»¶é‡Œå¯èƒ½æ²¡åŠ )
                        for item in res:
                            if 'suggested_key' not in item:
                                item['suggested_key'] = self.get_pinyin_key(keyword)
                        all_results.extend(res)
                        print(f"  -> {plugin.source_name} è´¡çŒ®äº† {len(res)} æ¡ç»“æœ")
                except Exception as e:
                    print(f"  -> {plugin.source_name} è¿è¡Œæ—¶å¼‚å¸¸: {e}")

        return all_results
    # ==========================================
    # è¾…åŠ©: å¹¶å‘è§£æçœŸå®åœ°å€
    # ==========================================
    def _concurrent_resolve(self, raw_results):
        if not raw_results: return []
        print(f"[Search] å¹¶å‘è§£æ {len(raw_results)} ä¸ªé“¾æ¥...")
        
        final_results = []
        with ThreadPoolExecutor(max_workers=8) as exe:
            future_to_item = {
                exe.submit(self._resolve_real_url, item['url']): item 
                for item in raw_results
            }
            for future in as_completed(future_to_item):
                item = future_to_item[future]
                try:
                    real_url = future.result()
                    # ç¡®ä¿è§£æå‡ºæ¥çš„æ˜¯ http ä¸”ä¸æ˜¯åŠ å¯†é“¾
                    if (real_url.startswith('http') and 
                        "baidu.com/link" not in real_url and 
                        "so.com/link" not in real_url and 
                        self._is_valid_result(item['title'], real_url)):
                        
                        item['url'] = real_url
                        final_results.append(item)
                except: pass
        
        return final_results
    # ==========================================
    # ç»Ÿä¸€å…¥å£
    # ==========================================
    # ... (å‰é¢çš„ _do_so_search, _do_baidu_search ç­‰ä¿æŒä¸å˜) ...

    # === [æ ¸å¿ƒå‡çº§] å…¨ç½‘å¹¶å‘èšåˆæœç´¢ (Aggregated Search) ===
    def search_bing(self, keyword):
        print(f"\n[Search] ğŸš€ å¯åŠ¨å…¨ç½‘å¹¶å‘èšåˆæœç´¢: {keyword}")
        start_time = time.time()
        
        # 1. å®šä¹‰å‚èµ›é€‰æ‰‹ (æ‰€æœ‰æœç´¢å¼•æ“ä¸€èµ·ä¸Š)
        search_funcs = [
            self._do_direct_source_search,
            self._do_so_search,             # 360 (ä¸»åŠ›)
            # self._do_baidu_search,          # ç™¾åº¦ (äº’è¡¥)
            # self.search,
              # ç›´è¿ (å…œåº•+é«˜è´¨é‡)
            # self._do_bing_search            # Bing (å›½é™…æº)
        ]

        # å¦‚æœæœ‰ä»£ç†ï¼ŒæŠŠ DDG ä¹ŸåŠ ä¸Š
        # if self.proxies:
            # search_funcs.insert(0, self._do_ddg_search)

        all_results = []
        seen_urls = set()  # ç”¨äº URL å»é‡
        
        # 2. å¼€å¯çº¿ç¨‹æ± ï¼Œæœ€å¤§å¹¶å‘æ•° = å¼•æ“æ•°é‡
        # æ³¨æ„ï¼šè¿™é‡Œä¸ä»…æœç´¢å¼•æ“å¹¶å‘ï¼Œå†…éƒ¨è§£æçœŸå®é“¾æ¥ä¹Ÿæ˜¯å¹¶å‘çš„(åµŒå¥—å¹¶å‘)ï¼Œé€Ÿåº¦æå¿«
        with ThreadPoolExecutor(max_workers=len(search_funcs)) as exe:
            # æäº¤æ‰€æœ‰æœç´¢ä»»åŠ¡
            future_to_name = {
                exe.submit(func, keyword): func.__name__ 
                for func in search_funcs
            }
            
            # 3. æ”¶é›†ç»“æœ (è°å…ˆå›æ¥è°å…ˆä¸Šæ¦œï¼Œæˆ–è€…ç­‰å…¨éƒ¨å›æ¥)
            for future in as_completed(future_to_name):
                engine_name = future_to_name[future]
                try:
                    results = future.result()
                    if results:
                        print(f"  [Aggregator] {engine_name} è´¡çŒ®äº† {len(results)} æ¡ç»“æœ")
                        
                        for item in results:
                            url = item['url']
                            # ç®€å•å»é‡é€»è¾‘ (å»æ‰åè®®å¤´å’Œå°¾éƒ¨æ–œæ è¿›è¡Œæ¯”å¯¹)
                            clean_url = url.replace('https://', '').replace('http://', '').rstrip('/')
                            
                            if clean_url not in seen_urls:
                                seen_urls.add(clean_url)
                                all_results.append(item)
                                
                except Exception as e:
                    print(f"  [Aggregator] {engine_name} å¼‚å¸¸: {e}")

        # 4. ç»“æœæ’åºä¼˜åŒ– (å¯é€‰)
        # ç›®å‰æ˜¯æŒ‰â€œè°å¿«è°æ’å‰é¢â€çš„è‡ªç„¶é¡ºåºã€‚
        # å¦‚æœä½ æƒ³è®©ç›´è¿æº (XBiquge) å§‹ç»ˆæ’åœ¨å‰é¢ï¼Œå¯ä»¥åœ¨è¿™é‡Œå¯¹ all_results sort ä¸€ä¸‹
        # ä¾‹å¦‚: all_results.sort(key=lambda x: 0 if 'XBiquge' in x['source'] else 1)

        print(f"[Search] èšåˆå®Œæˆï¼Œè€—æ—¶ {time.time() - start_time:.2f}sï¼Œå…±è·å– {len(all_results)} ä¸ªæœ‰æ•ˆæº\n")
        return all_results


class SearchHelperOld:
    def __init__(self):
        self.impersonate = "chrome110"
        self.timeout = 10
        self.proxies = self._get_proxies()
    
    def _get_proxies(self):
        try: return getproxies()
        except: return None

    def get_pinyin_key(self, text):
        clean = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
        clean = re.sub(r'(å°è¯´|ç¬”è¶£é˜|æœ€æ–°ç« èŠ‚|å…¨æ–‡é˜…è¯»)', '', clean)
        try:
            s = lazy_pinyin(clean, style=Style.FIRST_LETTER)
            k = ''.join(s).lower()
            return k[:15] if k else "temp"
        except: return "temp"
    # === [æ ¸å¿ƒæ–°å¢] Owllook èšåˆæœç´¢ (åŸºäº HTML è§£æ) ===
    # === Owllook èšåˆæœç´¢ (æ ‡å‡† Requests ç‰ˆ) ===
    def _do_owllook_search(self, keyword):
        print(f"[Search] ğŸ¦‰ å°è¯• Owllook èšåˆæœç´¢: {keyword}")
        url = "https://www1.owlook.com.cn/search"
        params = {'wd': keyword}
        
        try:
            # ä½¿ç”¨æ ‡å‡† requestsï¼Œæ¨¡æ‹Ÿæ™®é€šæµè§ˆå™¨å¤´
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8',
                'Accept-Language': 'zh-CN,zh;q=0.9',
                'Connection': 'keep-alive'
            }
            
            # verify=False å¯ä»¥é˜²æ­¢å› ä¸ºè¯ä¹¦é—®é¢˜å¯¼è‡´çš„è¿æ¥ä¸­æ–­
            resp = requests.get(
                url, 
                params=params, 
                headers=headers,
                timeout=15,
                verify=False 
            )
            
            # ç¼–ç å¤„ç†
            resp.encoding = 'utf-8'
            
            soup = BeautifulSoup(resp.text, 'html.parser')
            results = []
            
            # ... (ä¸‹é¢çš„è§£æé€»è¾‘å®Œå…¨ä¿æŒä¸å˜) ...
            items = soup.select('.result_item')
            
            for item in items:
                try:
                    # 1. æå–çœŸå®æºé“¾æ¥
                    source_link_tag = item.select_one('.netloc a[href^="http"]')
                    if not source_link_tag: continue
                    
                    href = source_link_tag.get('href')
                    
                    # 2. æå–æ ‡é¢˜
                    main_link = item.select_one('li a')
                    if not main_link: continue
                    
                    full_text = main_link.get_text(strip=True)
                    parts = full_text.split('--')
                    title = parts[1] if len(parts) >= 2 else full_text
                    
                    clean_title = self._clean_title(title)
                    
                    if not href or self._is_junk(clean_title, href): continue
                    if not self._is_valid_novel_site(href): continue

                    results.append({
                        'title': clean_title,
                        'url': href,
                        'suggested_key': self.get_pinyin_key(keyword),
                        'source': 'Owllook ğŸ¦‰'
                    })
                    
                except Exception: continue
                if len(results) >= 10: break
            
            return results

        except Exception as e:
            print(f"[Search] Owllook Error: {e}")
            return []
    def _is_valid_novel_site(self, url):
        """
        [æ–°å¢] ç™½åå•æ ¡éªŒï¼šåªå…è®¸é•¿å¾—åƒå°è¯´ç«™çš„ URL é€šè¿‡
        ç”¨äºå¯¹æŠ— Bing å›½å†…ç‰ˆçš„åƒåœ¾ç»“æœ
        """
        u = url.lower()
        # 1. å¿…é¡»åŒ…å« http
        if not u.startswith('http'): return False
        
        # 2. æ’é™¤çŸ¥ååƒåœ¾ç«™
        bad_domains = ['zhihu', 'douban', 'baidu', 'bilibili', 'video', 'news', '163.com', 'qq.com', 'sohu']
        if any(d in u for d in bad_domains): return False
        
        # 3. [æ ¸å¿ƒ] å¿…é¡»åŒ…å«å°è¯´ç«™å¸¸è§ç‰¹å¾
        valid_signs = ['book', 'novel', 'read', 'shu', 'biqu', 'bqg', 'txt', '88', 'wx', 'du', 'yuedu', 'chapter']
        # æˆ–è€… URL ç»“æ„åŒ…å«æ•°å­— (é€šå¸¸æ˜¯ä¹¦ID)
        has_id = bool(re.search(r'\d+', u))
        
        if any(s in u for s in valid_signs) or has_id:
            return True
        return False
    def _is_junk(self, title, url):
        t = title.lower()
        u = url.lower()
        bad_domains = ['facebook', 'twitter', 'zhihu', 'douban', 'baidu', 'baike', 'csdn', 'cnblogs', 'youtube', 'bilibili', '52pojie', '163.com', 'sohu', 'microsoft', 'google', 'apple', 'amazon']
        if any(d in u for d in bad_domains): return True
        bad_keywords = ['å·¥å…·', 'ç ´è§£', 'è½¯ä»¶', 'ä¸‹è½½', 'æ•™ç¨‹', 'è§†é¢‘', 'å‰§é€', 'ç™¾ç§‘', 'èµ„è®¯', 'æ‰‹æ¸¸', 'å®˜ç½‘', 'APP']
        if any(k in t for k in bad_keywords): return True
        return False

    def _do_ddg_search(self, keyword):
        url = "https://html.duckduckgo.com/html/"
        data = {'q': f"{keyword} ç¬”è¶£é˜"}
        try:
            resp = cffi_requests.post(url, data=data, impersonate=self.impersonate, timeout=self.timeout, proxies=self.proxies)
            soup = BeautifulSoup(resp.content, 'html.parser')
            results = []
            for link in soup.find_all('a', class_='result__a'):
                title = link.get_text(strip=True)
                href = link.get('href')
                if not href.startswith('http'): continue
                if self._is_junk(title, href): continue
                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'DuckDuckGo ğŸ¦†'
                })
                if len(results) >= 8: break
            return results
        except: return None
    def _do_bing_cn_search(self, keyword):
        """
        [æ–°å¢] Bing å›½å†…ç‰ˆä¸“ç”¨å¼•æ“ (ç›´è¿å¯ç”¨)
        """
        print(f"[Search] Trying Bing CN (Direct): {keyword}")
        # å…³é”®è¯å¼ºåˆ¶åŠ ä¸Š "ç¬”è¶£é˜"ï¼Œè¿™åœ¨å›½å†…æœ€å¥½ç”¨
        query = f"{keyword} ç¬”è¶£é˜ åœ¨çº¿é˜…è¯»"
        url = "https://cn.bing.com/search"
        params = {'q': query}
        
        try:
            # æ³¨æ„ï¼šä¸ä½¿ç”¨ proxiesï¼Œå¼ºåˆ¶ç›´è¿
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                timeout=8
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # å®½å®¹è§£æ
            links = soup.select('li.b_algo h2 a') or soup.select('h2 a')
            results = []
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                # ä¸¥æ ¼çš„ç™½åå•è¿‡æ»¤
                if not self._is_valid_novel_site(href):
                    continue

                results.append({
                    'title': self._clean_title(title),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Bing CN ğŸ‡¨ğŸ‡³'
                })
                if len(results) >= 8: break
            return results
        except Exception as e:
            print(f"[Search] Bing CN Error: {e}")
            return []
    def _do_360_search(self, keyword):
        """
        [ä¸»åŠ›] 360æœç´¢ + å¤šçº¿ç¨‹å¹¶å‘è§£å¯†
        """
        print(f"[Search] ğŸ” [è°ƒè¯•æ¨¡å¼] ä»…å°è¯• 360æœç´¢: {keyword}")
        url = "https://www.so.com/s"
        # å…³é”®è¯åŠ â€œç›®å½•â€ï¼Œç»“æœæ›´ç²¾å‡†
        params = {'q': f"{keyword} å…è´¹é˜…è¯» ç›®å½•"} 
        
        try:
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                timeout=self.timeout
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            raw_results = []
            # 360 ç»“æœé€‰æ‹©å™¨
            links = soup.select('ul.result li.res-list h3 a')
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('data-url') or link.get('href')
                
                if not href: continue
                
                # å°è¯•ä» URL å‚æ•°æå– (æŸäº› 360 é“¾æ¥æ˜¯ ...?url=http%3A%2F%2F...)
                if "so.com/link" in href:
                    try:
                        from urllib.parse import parse_qs, urlparse
                        qs = parse_qs(urlparse(href).query)
                        if 'url' in qs: href = qs['url'][0]
                    except: pass

                if self._is_junk(title, href): continue
                
                # å…ˆå­˜ä¸‹æ¥ï¼Œç¨åå¹¶å‘è§£å¯†
                raw_results.append({
                    'title': self._clean_title(title),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': '360 ğŸŸ¢'
                })
                if len(raw_results) >= 8: break
            
            if not raw_results:
                print("[Search] 360 æœªæ‰¾åˆ°åˆæ­¥ç»“æœ")
                return []

            # å¤šçº¿ç¨‹å¹¶å‘è§£å¯†çœŸå® URL
            print(f"[Search] æ­£åœ¨å¹¶å‘è§£æ {len(raw_results)} ä¸ª 360 é“¾æ¥...")
            final_results = []
            
            # ... å‰é¢çš„ä»£ç ä¿æŒä¸å˜ ...
            
            if not raw_results:
                print("[Search] 360 æœªæ‰¾åˆ°åˆæ­¥ç»“æœ")
                return []

            # [ä¿®æ”¹] æ”¹ä¸ºå•çº¿ç¨‹ä¸²è¡Œè§£æ
            print(f"[Search] æ­£åœ¨é¡ºåºè§£æ {len(raw_results)} ä¸ª 360 é“¾æ¥...")
            final_results = []
            
            for item in raw_results:
                try:
                    # ç›´æ¥è°ƒç”¨å‡½æ•°ï¼Œè€Œä¸æ˜¯æäº¤ç»™çº¿ç¨‹æ± 
                    real_url = self._resolve_real_url(item['url'])
                    # print(111)
                    # [é‡è¦] åªæœ‰å½“ URL ä¸å†åŒ…å« "so.com/link" æ—¶ï¼Œæ‰ç®—è§£ææˆåŠŸ
                    # å¹¶ä¸”è¦ç¬¦åˆå°è¯´ç«™ç™½åå•
                    if "so.com/link" not in real_url and self._is_valid_novel_site(real_url):
                        item['url'] = real_url
                        final_results.append(item)
                        # print(f"[Search] è§£ææˆåŠŸ: {real_url}") # è°ƒè¯•ç”¨
                    else:
                        # print(f"[Search] ä¸¢å¼ƒæ— æ•ˆé“¾æ¥: {real_url}") # è°ƒè¯•ç”¨
                        pass
                except Exception as e:
                    print(f"[Search] å•é¡¹è§£æå‡ºé”™: {e}")
                    pass
            
            return final_results

        except Exception as e:
            print(f"[Search] 360 Error: {e}")
            return []
    # def _resolve_real_url(url) :
        # print("[fff]")
        # return url
    def _resolve_real_url(self, url):
        # print("1111111")
        """
        [å¢å¼ºç‰ˆ] è§£æ 360/ç™¾åº¦çš„åŠ å¯†è·³è½¬é“¾æ¥
        æ”¯æŒï¼š302 Header è·³è½¬ã€Meta Refresh è·³è½¬ã€JS Window.location è·³è½¬
        """
        # å¦‚æœæœ¬èº«å°±æ˜¯ç›´é“¾ï¼Œç›´æ¥è¿”å›
        if "so.com" not in url:
            return url
            
        try:
            print("111")
            # 1. ç¬¬ä¸€æ¬¡å°è¯•ï¼šç¦æ­¢é‡å®šå‘ï¼Œçœ‹ Header
            # è¿™é‡Œçš„ timeout è®¾ç½®ç¨é•¿ä¸€ç‚¹ï¼Œé˜²æ­¢ç½‘ç»œæ³¢åŠ¨
            resp = cffi_requests.get(
                url, 
                impersonate=self.impersonate, 
                timeout=8, 
                allow_redirects=False 
            )
            
            # æƒ…å†µ A: æ ‡å‡† 301/302 è·³è½¬
            if resp.status_code in [301, 302]:
                real_url = resp.headers.get('Location') or resp.headers.get('location')
                print(real_url)
                if real_url:
                    print(f"[Resolve] 302è·³è½¬æˆåŠŸ: {real_url[:40]}...")
                    return real_url
            
            # æƒ…å†µ B: 200 OKï¼Œä½†æ˜¯æ˜¯ä¸€ä¸ªä¸­é—´è·³è½¬é¡µ (360 ç»å¸¸å¹²è¿™ä¸ª)
            if resp.status_code == 200:
                html = resp.text
                # B1. å°è¯•æå– JS è·³è½¬: window.location.replace("...")
                # 360 çš„ç‰¹å¾é€šå¸¸æ˜¯ window.location.replace
                import re
                js_match = re.search(r"window\.location\.replace\(['\"](.+?)['\"]", html)
                if js_match:
                    real_url = js_match.group(1)
                    print(f"[Resolve] JSæå–æˆåŠŸ: {real_url[:40]}...")
                    return real_url
                
                # B2. å°è¯•æå– Meta Refresh: <meta http-equiv="refresh" content="0;url=...">
                meta_match = re.search(r'url=([^"]+)"', html, re.IGNORECASE)
                if meta_match:
                    real_url = meta_match.group(1)
                    print(f"[Resolve] Metaæå–æˆåŠŸ: {real_url[:40]}...")
                    return real_url

        except Exception as e:
            print(f"[Resolve] è§£æå‡ºé”™: {e}")
            pass
            
        # å¦‚æœæ‰€æœ‰æ‰‹æ®µéƒ½å¤±æ•ˆï¼Œä¸ºäº†é˜²æ­¢å‰ç«¯æŠ¥é”™ï¼Œè¿˜æ˜¯è¿”å›åŸé“¾æ¥
        # ä½†å¤§æ¦‚ç‡è¿™ä¸ªé“¾æ¥å‰ç«¯ä¹Ÿæ‰“ä¸å¼€ï¼Œæ‰€ä»¥æœ€å¥½æ˜¯åœ¨ _do_360_search é‡Œè¿‡æ»¤æ‰
        return url
    def _do_sogou_search(self, keyword):
        print(f"[Search] ğŸš€ Bing å¤±è´¥ï¼Œæ­£åœ¨å°è¯•æœç‹—æœç´¢: {keyword}")
        query = f"{keyword} ç¬”è¶£é˜"
        url = "https://www.sogou.com/web"
        params = {'query': query}
        
        try:
            # æœç‹—éœ€è¦ä¸€ä¸ªæ¯”è¾ƒçœŸå®çš„ Referer
            headers = {
                "Referer": "https://www.sogou.com/",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
            }
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                headers=headers,
                timeout=10
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # æœç‹—çš„ç»“æ„æ¯”è¾ƒç‰¹æ®Šï¼Œé€šå¸¸åœ¨ .rb-tit a æˆ– h3 a
            links = soup.select('.rb-tit a') or soup.select('h3 a')
            results = []
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                # æœç‹—çš„ href å¾€å¾€æ˜¯ç»è¿‡æ··æ·†çš„ /link?url=...
                if not href: continue
                if not href.startswith('http'):
                    href = urljoin("https://www.sogou.com", href)

                # ç®€å•è¿‡æ»¤åƒåœ¾ç»“æœ
                if self._is_junk(title, href): continue
                if not self._is_valid_novel_site(href): continue

                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Sogou ğŸ¶'
                })
                if len(results) >= 8: break
            return results
        except Exception as e:
            print(f"[Search] Sogou Error: {e}")
            return []
    def _do_bing_search(self, keyword):
        url = "https://www.bing.com/search"
        params = {'q': f"{keyword} ç¬”è¶£é˜", 'setmkt': 'en-US'}
        try:
            resp = cffi_requests.get(url, params=params, impersonate=self.impersonate, timeout=self.timeout, proxies=self.proxies)
            soup = BeautifulSoup(resp.content, 'html.parser')
            links = soup.select('li.b_algo h2 a') or soup.select('li h2 a') or soup.select('h2 a')
            results = []
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                if not href.startswith('http') or self._is_junk(title, href): continue
                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Bing ğŸŒ'
                })
                if len(results) >= 8: break
            return results
        except: return []
    def _clean_title(self, title):
        """
        æ¸…æ´—æ ‡é¢˜ï¼šå»é™¤ç±»ä¼¼ "- ç¬”è¶£é˜", "_æ— å¼¹çª—" ç­‰åç¼€
        """
        if not title: return "æœªçŸ¥æ ‡é¢˜"
        # ä½¿ç”¨æ­£åˆ™åˆ†å‰² - _ | ç­‰ç¬¦å·ï¼Œåªå–ç¬¬ä¸€éƒ¨åˆ†
        return re.split(r'(-|_|\|)', title)[0].strip()

    def _is_junk(self, title, url):
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåƒåœ¾ç»“æœï¼ˆéå°è¯´å†…å®¹ï¼‰
        """
        t = title.lower()
        u = url.lower()
        
        # 1. æ’é™¤çŸ¥åéå°è¯´åŸŸå
        bad_domains = ['zhihu.com', 'douban.com', 'baike.baidu.com', 'csdn.net', 'cnblogs.com', 'bilibili.com', 'tieba.baidu.com', '163.com', 'sohu.com', 'sina.com']
        if any(d in u for d in bad_domains): return True
        
        # 2. æ’é™¤æ˜æ˜¾éå°è¯´æ ‡é¢˜å…³é”®è¯
        bad_keywords = ['ä¸‹è½½', 'txtä¸‹è½½', 'ç²¾æ ¡ç‰ˆ', 'æ•™ç¨‹', 'ç™¾ç§‘', 'èµ„è®¯', 'æ‰‹æ¸¸', 'æ”»ç•¥', 'è§†é¢‘', 'åœ¨çº¿è§‚çœ‹']
        if any(k in t for k in bad_keywords): return True
        
        return False
    # def search_bing(self, keyword):
    #     # 1. ç­–ç•¥ Aï¼šå¦‚æœæœ‰ä»£ç†ï¼Œé¦–é€‰ DuckDuckGo å’Œ Bing å›½é™…ç‰ˆ
    #     # (è¿™ä¸¤ä¸ªç»“æœæœ€å¹²å‡€ï¼Œä¼˜å…ˆçº§æœ€é«˜)
    #     if self.proxies:
    #         res = self._do_ddg_search(keyword)
    #         if res: return res
            
    #         res = self._do_bing_search(keyword)
    #         if res: return res
            
    #     # 2. ç­–ç•¥ Bï¼šå›½å†…ç›´è¿ç­–ç•¥ (Bing CN -> 360 -> ç™¾åº¦)
        
    #     # ä¼˜å…ˆçº§ 1: Bing å›½å†…ç‰ˆ (cn.bing.com)
    #     # å°è¯•ç›´è¿ Bingï¼Œå¦‚æœæœåŠ¡å™¨ IP æ²¡è¢«å¾®è½¯æ‹‰é»‘ï¼Œè¿™ä¸ªç»“æœæœ€å¥½
    #     res = self._do_bing_cn_search(keyword)
    #     if res and len(res) > 0:
    #         return res

    #     # ä¼˜å…ˆçº§ 2: 360æœç´¢ (So.com)
    #     # å¦‚æœ Bing æŒ‚äº†ï¼ˆè¿”å›ç©ºï¼‰ï¼Œå°è¯• 360ï¼ˆå¸¦å¤šçº¿ç¨‹è§£å¯†ï¼Œæœºæˆ¿IPé€šè¿‡ç‡é«˜ï¼‰
    #     res = self._do_360_search(keyword)
    #     if res: return res
        
    #     # ä¼˜å…ˆçº§ 3: ç™¾åº¦æœç´¢ (Baidu)
    #     # æœ€åå…œåº•ï¼Œæ”¶å½•å…¨ä½†å¯èƒ½æœ‰å¹¿å‘Šæˆ–éªŒè¯ç 
    #     return self._do_baidu_search(keyword)
    @lru_cache(maxsize=100) 
    def search_bing_cached(self, keyword):
        """å¸¦ç¼“å­˜çš„æœç´¢å…¥å£ï¼Œé¿å…é‡å¤è”ç½‘"""
        print(f"[Search Cache] Miss, fetching: {keyword}")
        return self.search_bing(keyword)
    def search_bing(self, keyword):
        # 1. ä¼˜å…ˆå°è¯• Owllook (èšåˆæºï¼Œè´¨é‡æœ€é«˜ï¼Œä¸”æä¾›ç›´é“¾)
        res = self._do_owllook_search(keyword)
        if res: return res
        # 2. 360
        res = self._do_360_search(keyword)
        if res: return res
        # 3. ç™¾åº¦/Bing...
        return self._do_bing_cn_search(keyword)
    # def _resolve_real_url(self, url):
    #     """
    #     [æ–°å¢] è§£æ 360/ç™¾åº¦çš„åŠ å¯†è·³è½¬é“¾æ¥
    #     åŸç†ï¼šå‘é€è¯·æ±‚ä½†ä¸è·Ÿéšè·³è½¬ (allow_redirects=False)ï¼Œç›´æ¥è¯»å– Location å¤´
    #     """
    #     # å¦‚æœä¸æ˜¯åŠ å¯†é“¾æ¥ï¼Œç›´æ¥è¿”å›
    #     if "so.com/link" not in url and "baidu.com/link" not in url:
    #         return url
            
    #     try:
    #         # å¿…é¡»ç¦æ­¢è‡ªåŠ¨è·³è½¬ï¼Œå¦åˆ™ä¼šä¸‹è½½æ•´ä¸ªç›®æ ‡ç½‘é¡µï¼Œæµªè´¹æµé‡å’Œæ—¶é—´
    #         resp = cffi_requests.get(
    #             url, 
    #             impersonate=self.impersonate, 
    #             timeout=5, 
    #             allow_redirects=False 
    #         )
            
    #         # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦ä¸º 301/302 é‡å®šå‘
    #         if resp.status_code in [301, 302]:
    #             # è·å–çœŸå®åœ°å€ (Location å¤´)
    #             real_url = resp.headers.get('Location') or resp.headers.get('location')
    #             if real_url:
    #                 return real_url
    #     except Exception as e: # <--- è¿™é‡ŒåŠ äº†ç©ºæ ¼ï¼Œä¿®å¤äº†è¯­æ³•é”™è¯¯
    #         print(f"[Search] è§£æè·³è½¬å¤±è´¥: {e}")
    #         pass
            
    #     # å¦‚æœè§£æå¤±è´¥ï¼Œä¸ºäº†ä¸è®©ç¨‹åºå´©æºƒï¼ŒåŸæ ·è¿”å›åŠ å¯†é“¾æ¥
    #     # è™½ç„¶è¿™ä¼šå¯¼è‡´å‰ç«¯å¯èƒ½æ‰“ä¸å¼€ï¼Œä½†æ€»æ¯”æ²¡æœ‰å¥½
    #     return url

    # === [æ ¸å¿ƒæ–°å¢ 2] ç™¾åº¦æœç´¢ (Baidu) - æ”¶å½•æœ€å…¨ï¼Œä½œä¸ºå¤‡ç”¨ ===
    def _do_baidu_search(self, keyword):
        print(f"[Search] ğŸ” å°è¯• ç™¾åº¦æœç´¢: {keyword}")
        url = "https://www.baidu.com/s"
        # æŠ€å·§ï¼šwd å¿…é¡»å¸¦ "æœ€æ–°ç« èŠ‚"ï¼Œå¦åˆ™å…¨æ˜¯è´´å§
        params = {'wd': f"{keyword} å°è¯´ æœ€æ–°ç« èŠ‚"}
        
        try:
            # ç™¾åº¦å¯¹ User-Agent éå¸¸æ•æ„Ÿï¼Œä¸”å¯¹ Referer æœ‰æ ¡éªŒ
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                "Referer": "https://www.baidu.com/"
            }
            # å¿…é¡»ä¸å¸¦ä»£ç†è®¿é—®ç™¾åº¦å›½å†…ç‰ˆï¼Œå¦åˆ™å¯èƒ½è·³åˆ°éªŒè¯ç 
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate,
                headers=headers,
                timeout=6
            )
            
            # æ£€æµ‹æ˜¯å¦è¢«ç™¾åº¦æ‹¦æˆª
            if "wappass.baidu.com" in resp.url or "éªŒè¯ç " in resp.text:
                print("[Search] âš ï¸ è§¦å‘ç™¾åº¦éªŒè¯ç ï¼Œè·³è¿‡")
                return []

            soup = BeautifulSoup(resp.content, 'html.parser')
            results = []
            
            # ç™¾åº¦çš„ç»“æœå—é€šå¸¸æ˜¯ c-container
            containers = soup.select('div.c-container')
            
            for box in containers:
                try:
                    # æå–æ ‡é¢˜é“¾æ¥
                    title_elem = box.select_one('h3 a') or box.select_one('a')
                    if not title_elem: continue
                    
                    title = title_elem.get_text(strip=True)
                    href = title_elem.get('href') # è¿™æ˜¯ç™¾åº¦çš„åŠ å¯†é“¾æ¥
                    
                    # æå–ä¸‹æ–¹æ˜¾ç¤ºçš„çœŸå®åŸŸå (è¾…åŠ©åˆ¤æ–­)
                    footer_text = box.get_text()
                    
                    # å¼ºåŠ›è¿‡æ»¤
                    if self._is_junk(title, ""): continue # URLæ˜¯åŠ å¯†çš„ï¼Œæš‚æ—¶åªèƒ½æ£€æŸ¥æ ‡é¢˜
                    
                    # ç™¾åº¦ç‰¹è‰²ï¼šå¹¿å‘Šé€šå¸¸æœ‰ 'å¹¿å‘Š' å­—æ ·
                    if "å¹¿å‘Š" in footer_text: continue

                    # æ—¢ç„¶æ‹¿ä¸åˆ°çœŸå®URLï¼ˆéœ€è¦å†æ¬¡è¯·æ±‚è§£å¯†ï¼Œå¤ªæ…¢ï¼‰ï¼Œ
                    # æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªå¤§èƒ†çš„ç­–ç•¥ï¼š
                    # ç›´æ¥è¿”å›è¿™ä¸ªåŠ å¯†é“¾æ¥ã€‚
                    # å› ä¸ºä½ çš„ NovelCrawler.run() èƒ½å¤Ÿå¤„ç† 302 è·³è½¬ï¼
                    
                    results.append({
                        'title': self._clean_title(title),
                        'url': href, # è¿™æ˜¯ä¸€ä¸ª http://www.baidu.com/link?url=...
                        'suggested_key': self.get_pinyin_key(keyword),
                        'source': 'Baidu ğŸ”µ'
                    })
                    if len(results) >= 6: break
                except: pass
                
            return results
        except Exception as e:
            print(f"[Search] Baidu Error: {e}")
            return []
# ==========================================
# 3. å°è¯´çˆ¬è™« (NovelCrawler - ä¿®å¤KeyErrorç‰ˆ)
# ==========================================
class NovelCrawler:
    def __init__(self):
        self.impersonate = "chrome110"
        self.timeout = 15
        self.proxies = getproxies()
    # spider_core.py -> NovelCrawler ç±»å†…éƒ¨
    # ==========================================
    # [æ–°å¢] æ™ºèƒ½æ¢æºæ ¸å¿ƒé€»è¾‘
    # ==========================================
    # === [è°ƒè¯•å¢å¼ºç‰ˆ] æœç´¢å¹¶è¿”å›å¯ç”¨æºåˆ—è¡¨ ===
    def search_alternative_sources(self, book_name, target_chapter_id):
        print(f"\n[Switch] ğŸš€ æé€Ÿæ¢æº: ã€Š{book_name}ã€‹ (ID: {target_chapter_id})")
        
        # 1. æœç´¢ (å¸¦ç¼“å­˜)
        from spider_core import searcher 
        # ä½¿ç”¨ search_bing_cached è€Œä¸æ˜¯ search_bing
        search_results = searcher.search_bing_cached(book_name)
        
        if not search_results:
            return []

        print(f"[Switch] ğŸ” ç¼“å­˜/æœç´¢è¿”å› {len(search_results)} ä¸ªæºï¼Œå¼€å§‹æé€ŸéªŒè¯...")
        valid_sources = []
        
        # 2. éªŒè¯ä»»åŠ¡ (æé€Ÿç‰ˆ)
        def check_source(result):
            toc_url = result['url']
            domain = urlparse(toc_url).netloc
            
            try:
                # [å…³é”®] å¼€å¯ fast_mode=True
                # è¶…æ—¶ 5ç§’ï¼Œä¸é‡è¯•ã€‚å¦‚æœ 5ç§’æ²¡æ‹‰ä¸‹æ¥ç›®å½•ï¼Œè¯´æ˜è¿™ä¸ªæºå¤ªæ…¢ï¼Œç›´æ¥ä¸¢å¼ƒï¼
                toc = self.get_toc(toc_url, fast_mode=True)
                
                if not toc or not toc.get('chapters'):
                    return None
                
                # å€’åºæŸ¥æ‰¾ï¼Œæ•ˆç‡æ›´é«˜
                for chap in reversed(toc['chapters']):
                    if chap.get('id') == target_chapter_id:
                        # print(f"[Switch] âœ… å‘½ä¸­: {domain}")
                        return {
                            "source": domain,
                            "url": chap['url'],
                            "title": chap['name'],
                            "toc_url": toc_url
                        }
            except: pass
            return None

        # 3. å¹¶å‘éªŒè¯ (æœ€å¤§ 8 çº¿ç¨‹)
        # åªå–å‰ 5 ä¸ªç»“æœéªŒè¯ï¼Œå› ä¸ºåé¢çš„é€šå¸¸è´¨é‡ä½ä¸”æµªè´¹æ—¶é—´
        candidates = search_results[:5] 
        
        with ThreadPoolExecutor(max_workers=8) as exe:
            futures = [exe.submit(check_source, res) for res in candidates]
            for future in as_completed(futures):
                res = future.result()
                if res:
                    valid_sources.append(res)
        
        print(f"[Switch] ğŸ è€—æ—¶æ“ä½œç»“æŸï¼Œæ‰¾åˆ° {len(valid_sources)} ä¸ªæœ‰æ•ˆæº")
        return valid_sources
    def _get_book_name(self, soup):
        """
        é€šç”¨çš„å°è¯´åè¯†åˆ«é€»è¾‘
        """
        # 1. å°è¯•ä»å¸¸è§é¢åŒ…å±‘å¯¼èˆªä¸­æå–
        # åŒ¹é…åŒ…å« 'path', 'breadcrumb', 'crumb' çš„ class æˆ– id
        path_box = soup.find(class_=re.compile(r'path|crumb|breadcrumb', re.I)) or \
                   soup.find(id=re.compile(r'path|crumb|breadcrumb', re.I))
        
        if path_box:
            links = path_box.find_all('a')
            # é€»è¾‘ï¼šé¦–é¡µ > åˆ†ç±» > ä¹¦å > ç« èŠ‚åï¼Œé€šå¸¸å€’æ•°ç¬¬äºŒä¸ªæˆ–ç¬¬ä¸‰ä¸ªæ˜¯ä¹¦å
            if len(links) >= 3:
                # é’ˆå¯¹ä¹¦é¦™é˜è¿™ç§ï¼šé¦–é¡µ(0) > åˆ†ç±»(1) > ä¹¦å(2) > ç« èŠ‚
                return links[2].get_text(strip=True)
            elif len(links) == 2:
                return links[1].get_text(strip=True)

        # 2. å°è¯•ä» Meta Keywords æå– (ç¬¬ä¸€ä¸ªè¯é€šå¸¸æ˜¯ä¹¦å)
        meta_kw = soup.find('meta', attrs={'name': 'keywords'})
        if meta_kw:
            kw = meta_kw.get('content', '').split(',')[0]
            if kw and len(kw) < 20: return kw

        # 3. å°è¯•ä» Title æ ‡ç­¾æ‹†åˆ†
        if soup.title:
            t_text = soup.title.get_text(strip=True)
            # å¸¸è§æ ¼å¼ï¼šç« èŠ‚å_ä¹¦å_ç«™ç‚¹å æˆ– ä¹¦å_ç« èŠ‚å
            if "_" in t_text:
                parts = t_text.split('_')
                for p in parts:
                    if "ç¬¬" not in p and "ç« " not in p and "èŠ‚" not in p:
                        # å‰”é™¤å¸¸è§çš„åç¼€
                        name = re.sub(r'(å°è¯´|å…¨æ–‡|é˜…è¯»|æœ€æ–°ç« èŠ‚|ç¬”è¶£é˜).*', '', p)
                        if len(name) > 1: return name.strip()

        return "æœªçŸ¥ä¹¦å"
    def search_and_switch_source(self, book_name, target_chapter_id):
        """
        æ ¹æ®ä¹¦åå’Œç›®æ ‡ç« èŠ‚IDï¼Œå…¨ç½‘æœç´¢å¤‡é€‰æºï¼Œå¹¶å¯»æ‰¾åŒ¹é…çš„ç« èŠ‚é“¾æ¥
        """
        print(f"[Switch] æ­£åœ¨ä¸ºã€Š{book_name}ã€‹ç¬¬ {target_chapter_id} ç« å¯»æ‰¾æ–°æº...")
        
        # 1. å…¨ç½‘æœç´¢å¤‡é€‰æº (å¤ç”¨ SearchHelper)
        # æœç´¢å…³é”®è¯åŠ ä¸Š "ç›®å½•"ï¼Œæé«˜å‘½ä¸­ç‡
        from spider_core import searcher # ç¡®ä¿å¼•ç”¨
        search_results = searcher.search_bing(book_name)
        
        if not search_results:
            print("[Switch] æœªæœç´¢åˆ°ä»»ä½•ç»“æœ")
            return None

        # 2. å®šä¹‰å•ä¸ªæºçš„éªŒè¯ä»»åŠ¡
        def check_source(result):
            toc_url = result['url']
            domain = urlparse(toc_url).netloc
            
            # ç®€å•è¿‡æ»¤ï¼šå¦‚æœæ˜¯å½“å‰æ­£åœ¨ä½¿ç”¨çš„æº(ç•¥)ï¼Œæˆ–è€…æ˜æ˜¾ä¸æ˜¯å°è¯´ç«™çš„ï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿‡æ»¤
            # è¿™é‡Œå…ˆä¸åšå¤æ‚è¿‡æ»¤ï¼Œä¿¡ä»» SearchHelper çš„é»‘åå•
            
            try:
                # æŠ“å–ç›®å½• (å¤ç”¨ get_tocï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œ ID è§£æå’Œæ’åº)
                toc = self.get_toc(toc_url)
                if not toc or not toc.get('chapters'):
                    return None
                
                # 3. åœ¨ç›®å½•ä¸­äºŒåˆ†æŸ¥æ‰¾æˆ–éå†å¯»æ‰¾ç›®æ ‡ ID
                # å› ä¸ºæˆ‘ä»¬å·²ç»æ’å¥½åºäº†ï¼Œç†è®ºä¸ŠäºŒåˆ†æ›´å¿«ï¼Œä½†åˆ—è¡¨ä¸é•¿ï¼Œéå†ä¹Ÿè¡Œ
                for chap in toc['chapters']:
                    if chap.get('id') == target_chapter_id:
                        print(f"[Switch] âœ… åœ¨ [{domain}] æ‰¾åˆ°åŒ¹é…ç« èŠ‚: {chap['name']}")
                        return {
                            "new_url": chap['url'],
                            "source_name": domain,
                            "chapter_title": chap['name']
                        }
            except Exception as e:
                # print(f"[Switch] æ£€æŸ¥æº {domain} å¤±è´¥: {e}")
                pass
            return None

        # 3. å¹¶å‘éªŒè¯ (é€Ÿåº¦è‡³ä¸Š)
        # æˆ‘ä»¬åŒæ—¶æ£€æŸ¥å‰ 5 ä¸ªæœç´¢ç»“æœ
        candidates = search_results[:6] 
        found_target = None
        
        with ThreadPoolExecutor(max_workers=6) as exe:
            futures = [exe.submit(check_source, res) for res in candidates]
            
            for future in as_completed(futures):
                res = future.result()
                if res:
                    found_target = res
                    # åªè¦æ‰¾åˆ°ä¸€ä¸ªèƒ½ç”¨çš„ï¼Œç«‹é©¬åœæ­¢å…¶ä»–ä»»åŠ¡ï¼ˆè™½ç„¶çº¿ç¨‹æ± æ²¡æ³•ç«‹åˆ»killï¼Œä½†æˆ‘ä»¬å¯ä»¥breakè¿”å›ï¼‰
                    # å®é™…ä¸Šä¸ºäº†æœ€å¿«å“åº”ï¼Œè°å…ˆè¿”å›å°±ç”¨è°
                    break
        
        return found_target
    def resolve_start_url(self, url):
        """
        [æ–°å¢] æ™ºèƒ½å…¥å£è§£æï¼šå¦‚æœç»™çš„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨è½¬ä¸ºç¬¬ä¸€ç« 
        """
        if url.startswith('epub:'):
            return url
        print(f"[SmartURL] Analyzing: {url}")
        
        # 1. ç‰¹å¾é¢„åˆ¤ï¼šå¦‚æœ URL ä»¥ .html ç»“å°¾ä¸”åŒ…å«æ•°å­—ï¼Œå¤§æ¦‚ç‡æ˜¯ç« èŠ‚ï¼Œç›´æ¥è¿”å›
        # (è¿™èƒ½èŠ‚çœä¸€æ¬¡ç½‘ç»œè¯·æ±‚)
        if re.search(r'\d+\.html$', url) and "index" not in url:
            return url
            
        # 2. çˆ¬å–é¡µé¢åˆ†æ
        # è¿™é‡Œçš„ run ä¼šè‡ªåŠ¨è¯†åˆ«ç›®å½•é“¾æ¥ (toc_url)
        # æˆ‘ä»¬åˆ©ç”¨ get_toc æ–¹æ³•ï¼Œçœ‹çœ‹å®ƒæ˜¯ä¸æ˜¯ä¸€ä¸ªç›®å½•é¡µ
        
        try:
            # å°è¯•å½“åšç›®å½•æŠ“å–
            toc_data = self.get_toc(url)
            
            # å¦‚æœæŠ“åˆ°äº†å¤§é‡ç« èŠ‚ï¼Œè¯´æ˜å®ƒç¡®å®æ˜¯ç›®å½•
            if toc_data and len(toc_data['chapters']) > 5:
                first_chap = toc_data['chapters'][0]['url']
                print(f"[SmartURL] æ£€æµ‹åˆ°ç›®å½•é¡µï¼Œè‡ªåŠ¨è·³è½¬ç¬¬ä¸€ç« : {first_chap}")
                return first_chap
                
            # å¦‚æœä¸æ˜¯ç›®å½•ï¼Œè¯´æ˜å¯èƒ½æ˜¯ä¸€ä¸ªä¸å¸¦ .html åç¼€çš„ç« èŠ‚é¡µ (å¦‚ xbqg77)
            # æˆ–è€…çˆ¬è™«æ²¡è§£æå¯¹ï¼Œä¸ºäº†å®‰å…¨ï¼ŒåŸæ ·è¿”å›
            return url
            
        except Exception as e:
            print(f"[SmartURL] Resolve Error: {e}")
            return url
    def _fetch_page_smart(self, url, retry=None, timeout=None):
        """
        åŸºç¡€è¯·æ±‚ï¼šæ”¯æŒè‡ªå®šä¹‰é‡è¯•æ¬¡æ•°å’Œè¶…æ—¶æ—¶é—´
        é…åˆ get_toc çš„ fast_mode ä½¿ç”¨
        """
        # 1. å‚æ•°å†³æ–­ï¼šå¦‚æœæœªä¼ å…¥ï¼Œåˆ™ä½¿ç”¨å®ä¾‹å˜é‡æˆ–é»˜è®¤å€¼
        # è¿™æ ·è®¾è®¡æ˜¯ä¸ºäº†è®© get_toc ä¸­ä¸´æ—¶ä¿®æ”¹ self.timeout èƒ½ç”Ÿæ•ˆ
        current_retry = retry if retry is not None else 3
        current_timeout = timeout if timeout is not None else self.timeout

        for i in range(current_retry):
            try:
                headers = {
                    "Referer": url, 
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
                }
                
                # å‘èµ·è¯·æ±‚
                resp = cffi_requests.get(
                    url, 
                    impersonate=self.impersonate, 
                    timeout=current_timeout,  # <--- å…³é”®ï¼šä½¿ç”¨åŠ¨æ€è¶…æ—¶
                    headers=headers, 
                    allow_redirects=True, 
                    proxies=self.proxies
                )
                
                # === ç¼–ç æ™ºèƒ½è¯†åˆ«é€»è¾‘ ===
                
                # A. å°è¯• lxml è§£æ meta æ ‡ç­¾ (æœ€å‡†)
                try:
                    tree = lxml_html.fromstring(resp.content, parser=lxml_html.HTMLParser(encoding='utf-8'))
                    charset = tree.xpath('//meta[contains(@content, "charset")]/@content') or tree.xpath('//meta/@charset')
                    enc = 'utf-8'
                    if charset:
                        match = re.search(r'charset=([\w-]+)', str(charset[0]), re.I)
                        enc = match.group(1) if match else charset[0]
                    return resp.content.decode(enc)
                except Exception:
                    pass
                
                # B. æš´åŠ›å°è¯•å¸¸è§ä¸­æ–‡ç¼–ç 
                for e in ['utf-8', 'gb18030', 'gbk', 'big5']:
                    try: return resp.content.decode(e)
                    except: continue
                
                # C. æœ€åå…œåº•
                return resp.content.decode('utf-8', errors='replace')

            except Exception as e: 
                # åªæœ‰ä¸æ˜¯æœ€åä¸€æ¬¡é‡è¯•æ—¶æ‰ sleep
                if i == current_retry - 1: 
                    # print(f"[Fetch] æœ€ç»ˆå¤±è´¥: {url} | Err: {e}")
                    return None 
                time.sleep(1)
        
        return None

    def _get_smart_title(self, soup):
        h1_title = soup.find('h1', class_=re.compile(r'title|chapter|book|name', re.I))
        if h1_title: return h1_title.get_text(strip=True)
        h1s = soup.find_all('h1')
        for h in h1s:
            txt = h.get_text(strip=True)
            if len(txt) <= 4 or any(x in txt for x in ["ç¬”è¶£é˜", "å°è¯´ç½‘", "é˜…è¯»å™¨"]):
                if "logo" in str(h.get('class', '')).lower(): continue
                if h.find_parent(['nav', 'header']): continue
            return txt
        if soup.title: return re.split(r'[_â€”|-]', soup.title.get_text(strip=True))[0].strip()
        return "æœªçŸ¥ç« èŠ‚"

    def _clean_text_lines(self, text):
        if not text: return []
        junk = [r"ä¸€ç§’è®°ä½", r"æœ€æ–°ç« èŠ‚", r"ç¬”è¶£é˜", r"ä¸Šä¸€ç« ", r"ä¸‹ä¸€ç« ", r"åŠ å…¥ä¹¦ç­¾", r"æŠ•æ¨èç¥¨", r"æœ¬ç« æœªå®Œ", r"æœªå®Œå¾…ç»­", r"ps:"]
        lines = []
        for line in text.split('\n'):
            line = line.replace('\xa0', ' ').strip()
            if not line or len(line) < 2: continue
            if len(line) < 50 and any(re.search(p, line, re.I) for p in junk): continue
            if "{" in line and "function" in line: continue
            lines.append(line)
        return lines

    def _extract_content_smart(self, soup):
        for cid in ['txt', 'content', 'chaptercontent', 'BookText', 'showtxt', 'nr1', 'read-content']:
            div = soup.find(id=cid)
            if div:
                for a in div.find_all('a'): a.decompose()
                return self._clean_text_lines(div.get_text('\n'))
        best_div, max_score = None, 0
        for div in soup.find_all('div'):
            if div.get('id') and re.search(r'(nav|foot|header|menu)', str(div.get('id')), re.I): continue
            txt = div.get_text(strip=True)
            score = len(txt) - (len(div.find_all('a')) * 5)
            if score > max_score: max_score, best_div = score, div
        return self._clean_text_lines(best_div.get_text('\n')) if best_div else ["æ­£æ–‡è§£æå¤±è´¥"]

    def _parse_chapters_from_soup(self, soup, base_url):
        links = []
        max_valid_links = 0
        containers = soup.find_all(['div', 'ul', 'dl', 'tbody'])
        if not containers: containers = [soup.body]
        
        junk_keywords = ['æœ€æ–°ç« èŠ‚', 'å…¨æ–‡é˜…è¯»', 'æ— å¼¹çª—', 'å°è¯´', 'ç¬”è¶£é˜', 'åŠ å…¥ä¹¦æ¶', 'æŠ•æ¨èç¥¨', 'ä½œå®¶', 'ä½œè€…']

        for container in containers:
            if container.get('class') and any(x in str(container.get('class')) for x in ['nav', 'footer', 'header', 'hot', 'recommend']): continue
            temp_links = []
            for a in container.find_all('a'):
                raw_text = a.get_text(strip=True)
                href = a.get('href')
                if not href: continue
                if any(k in raw_text for k in junk_keywords) and not re.search(r'\d', raw_text): continue
                
                chap_id = parse_chapter_id(raw_text)
                is_valid = False
                if chap_id > 0: is_valid = True
                elif len(raw_text) > 2 and any(x in raw_text for x in ['ç« ', 'èŠ‚', 'å›', 'å¹•']) and not any(k in raw_text for k in junk_keywords): is_valid = True
                
                if is_valid:
                    full_url = urljoin(base_url, href)
                    match_name = re.search(r'(?:ç¬¬)?\s*[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*[ç« èŠ‚å›](.*)', raw_text)
                    pure_name = match_name.group(1).strip() if match_name else raw_text
                    if full_url:
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”Ÿæˆå­—å…¸æ—¶ä¸å¸¦ 'title' é”®ï¼Œç»Ÿä¸€ç”± _standardize_chapters å¤„ç†
                        temp_links.append({'id': chap_id, 'raw_title': raw_text, 'name': pure_name, 'url': full_url})
            
            if len(temp_links) > max_valid_links: max_valid_links = len(temp_links); links = temp_links
        return links

    def _standardize_chapters(self, raw_chapters):
        unique = {c['url']: c for c in raw_chapters}
        processed_list = []
        for c in unique.values():
            raw_title = c.get('title') or c.get('raw_title') or ""
            if any(x in raw_title for x in ['æœ€æ–°ç« èŠ‚', 'å…¨æ–‡é˜…è¯»', 'æ— å¼¹çª—', 'txtä¸‹è½½']) and not re.search(r'\d', raw_title): continue
            chap_id = parse_chapter_id(raw_title)
            pure_name = re.sub(r'^(?:ç¬¬)?\s*[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*[ç« èŠ‚å›]', '', raw_title).strip()
            pure_name = re.sub(r'^\d+\s*\.?\s*', '', pure_name).strip()
            
            c['id'] = chap_id
            c['name'] = pure_name or raw_title
            c['raw_title'] = raw_title
            c['title'] = raw_title # [æ ¸å¿ƒä¿®å¤] è¡¥ä¸Šè¿™ä¸ªé”®ï¼Œé˜²æ­¢åç«¯æŠ¥é”™
            processed_list.append(c)
            
        numbered = [c for c in processed_list if c['id'] > 0]
        others = [c for c in processed_list if c['id'] <= 0]
        numbered.sort(key=lambda x: x['id'])
        
        if len(numbered) > 10:
            final_chapters = numbered
            prologues = [c for c in others if "åº" in c['raw_title'] or "å¼•" in c['raw_title']]
            final_chapters = prologues + final_chapters
        else: final_chapters = others + numbered
        return final_chapters

    def get_toc(self, toc_url, fast_mode=False):
        """
        fast_mode=True: ä¸é‡è¯•ï¼Œè¶…æ—¶çŸ­ï¼Œä¸“ç”¨äºæ¢æºæ£€æµ‹
        """
        # å‚æ•°è®¾ç½®
        if toc_url.startswith('epub:'):
            return None
        timeout = 5 if fast_mode else 15
        retry = 1 if fast_mode else 3

        adapter = plugin_mgr.find_match(toc_url)
        if adapter: 
            # æ³¨æ„ï¼šå¦‚æœé€‚é…å™¨é‡Œçš„ get_toc è°ƒç”¨äº† _fetch_page_smartï¼Œ
            # æˆ‘ä»¬éœ€è¦ä¿®æ”¹é€‚é…å™¨æ‰èƒ½ç”Ÿæ•ˆï¼Œæˆ–è€…æˆ‘ä»¬åœ¨è¿™é‡Œ monkey patch ä¸€ä¸‹ï¼Ÿ
            # ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å‡è®¾é€‚é…å™¨è°ƒç”¨çš„æ˜¯ self._fetch_page_smart
            # æˆ‘ä»¬å¯ä»¥ä¸´æ—¶æŠŠ self.timeout æ”¹äº†ï¼Œè™½ç„¶ä¸ä¼˜é›…ä½†æœ‰æ•ˆ
            
            old_timeout = self.timeout
            self.timeout = timeout # ä¸´æ—¶ä¿®æ”¹å…¨å±€è¶…æ—¶
            try:
                data = adapter.get_toc(self, toc_url)
            finally:
                self.timeout = old_timeout # æ¢å¤
        else: 
            # é€šç”¨é€»è¾‘ï¼Œç›´æ¥ä¼ å‚
            # æˆ‘ä»¬éœ€è¦ä¿®æ”¹ _general_toc_logic æ¥å—å‚æ•°ï¼Œæˆ–è€…åƒä¸Šé¢ä¸€æ ·æ”¹ self.timeout
             # è¿™é‡Œå¤ç”¨ä¸Šé¢çš„é€»è¾‘ä¿®æ”¹ timeout å±æ€§æœ€ç¨³å¦¥
             pass
        
        # ä¸ºäº†ä¸ä¿®æ”¹æ‰€æœ‰é€‚é…å™¨ä»£ç ï¼Œæˆ‘ä»¬é‡‡ç”¨ä¿®æ”¹å®ä¾‹å±æ€§çš„æ–¹å¼æ¥å®ç° Fast Mode
        # ä¸Šé¢çš„é€»è¾‘å…¶å®åªå¯¹ adapter æœ‰æ•ˆï¼Œå¯¹é€šç”¨é€»è¾‘éœ€è¦ä¸‹é¢è¿™æ®µï¼š
        
        # é‡æ–°å†™ä¸€æ®µé€šç”¨çš„ get_toc è°ƒç”¨é€»è¾‘ï¼š
        old_timeout = self.timeout
        self.timeout = timeout
        
        try:
             # è¿™é‡Œè°ƒç”¨åŸæ¥çš„é€»è¾‘
             if adapter: 
                 data = adapter.get_toc(self, toc_url)
             else:
                 # ä¿®æ”¹ _general_toc_logic å†…éƒ¨è°ƒç”¨çš„ _fetch_page_smart
                 # ç”±äº _fetch_page_smart ç°åœ¨ç”¨çš„æ˜¯å‚æ•°é»˜è®¤å€¼ï¼Œæˆ‘ä»¬éœ€è¦å®ƒè¯»å– self.timeout
                 # è¯·ç¡®ä¿ä½ çš„ _fetch_page_smart é»˜è®¤ timeout=self.timeout
                 
                 # æˆ–è€…æˆ‘ä»¬ç®€å•ç²—æš´é‡å†™ _fetch_page_smart è®©ä»–ä¼˜å…ˆç”¨å‚æ•°ï¼Œæ²¡æœ‰å‚æ•°ç”¨ self.timeout
                 data = self._general_toc_logic(toc_url)
        except Exception:
            return None
        finally:
            self.timeout = old_timeout # æ¢å¤é»˜è®¤ 15s

        if not data or not data.get('chapters'): return None
        if data.get('manual_sort') is True: return data
        final_chapters = self._standardize_chapters(data['chapters'])
        return {'title': data['title'], 'chapters': final_chapters}

    def _general_toc_logic(self, toc_url):
        html = self._fetch_page_smart(toc_url)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        raw_chapters = self._parse_chapters_from_soup(soup, toc_url)
        
        pages = set()
        for s in soup.find_all('select'):
            for o in s.find_all('option'):
                v = o.get('value')
                if v:
                    f = urljoin(toc_url, v)
                    if f.rstrip('/') != toc_url.rstrip('/'): pages.add(f)
        if pages:
            with ThreadPoolExecutor(max_workers=5) as exe:
                results = exe.map(lambda u: self._parse_chapters_from_soup(BeautifulSoup(self._fetch_page_smart(u) or "", 'html.parser'), toc_url), sorted(list(pages)))
                for sub in results: raw_chapters.extend(sub)
        return {'title': self._get_smart_title(soup), 'chapters': raw_chapters}

    def get_latest_chapter(self, toc_url):
        toc_data = self.get_toc(toc_url)
        if not toc_data or not toc_data.get('chapters'): return None
        chapters = toc_data['chapters']
        last_chapter = chapters[-1]
        # å…¼å®¹æ€§å¤„ç†
        return {
            "title": last_chapter.get('name', last_chapter.get('raw_title', 'æœªçŸ¥ç« èŠ‚')),
            "url": last_chapter['url'],
            "id": last_chapter.get('id', -1),
            "total_chapters": len(chapters)
        }

    def run(self, url):
        print(f"\n[Run] ğŸš€ å¼€å§‹å¤„ç† URL: {url}")
        
        # 1. å°è¯•åŒ¹é…æ’ä»¶
        adapter = plugin_mgr.find_match(url)
        if adapter:
            print(f"[Run] âœ¨ åŒ¹é…åˆ°é€‚é…å™¨: {adapter.__class__.__name__}")
            result = adapter.run(self, url)
            # æ‰“å°æ’ä»¶è¿”å›çš„ä¹¦å
            print(f"[Run] ğŸ“¦ æ’ä»¶è¿”å›ä¹¦å: {result.get('book_name', 'æœªè·å–')}")
            return result
        
        print(f"[Run] ğŸŒ æœªæ‰¾åˆ°æ’ä»¶ï¼Œä½¿ç”¨é€šç”¨é€»è¾‘...")
        # 2. å¦‚æœæ²¡æ’ä»¶ï¼Œæ‰§è¡Œé€šç”¨é€»è¾‘
        return self._general_run_logic(url)
    
    def _general_run_logic(self, url):
        base_url = url
        if "_" in url:
            normalized = re.sub(r'_\d+\.html', '.html', url)
            if normalized != url: base_url = normalized
        combined_content = []
        first_page_meta = None
        current_url = base_url
        visited_urls = {url, base_url}
        max_pages, page_count = 8, 0
        original_title = ""
        chap_id_match = re.search(r'/(\d+)(?:_\d+)?\.html', base_url)
        current_chap_id = chap_id_match.group(1) if chap_id_match else ""
        while page_count < max_pages:
            html = self._fetch_page_smart(current_url)
            if not html: break
            soup = BeautifulSoup(html, 'html.parser')
            current_title = self._get_smart_title(soup)
            if page_count == 0: original_title = current_title
            elif current_title != original_title and len(current_title) > 3: break
            content = self._extract_content_smart(soup)
            if content and original_title in content[0]: content = content[1:]
            combined_content.extend(content)
            next_page_url, next_chapter_url, prev_chapter_url, toc_url = None, None, None, None
            for a in soup.find_all('a'):
                txt = a.get_text(strip=True).replace(' ', '')
                href = a.get('href')
                if not href or href.startswith('javascript'): continue
                full = urljoin(current_url, href)
                if "ä¸‹ä¸€é¡µ" in txt or "ä¸‹â€”é¡µ" in txt or re.search(r'\(\d+/\d+\)', txt):
                    if current_chap_id and current_chap_id in href: next_page_url = full
                    else: next_chapter_url = full
                elif "ä¸‹ä¸€ç« " in txt or "ä¸‹ç« " in txt: next_chapter_url = full
                if page_count == 0:
                    if "ä¸Šä¸€ç« " in txt or "ä¸Šç« " in txt: prev_chapter_url = full
                    elif "ä¸Šä¸€é¡µ" in txt or "ä¸Šé¡µ" in txt:
                        if current_chap_id and current_chap_id not in href: prev_chapter_url = full
                if "ç›®å½•" in txt: toc_url = full
            for aid in ['pb_prev', 'prev_url', 'pb_next', 'next_url', 'pb_mulu']:
                tag = soup.find(id=aid)
                if not tag or not tag.get('href'): continue
                t_url = urljoin(current_url, tag['href'])
                if 'prev' in aid and page_count == 0 and not prev_chapter_url:
                    if current_chap_id and current_chap_id not in tag['href']: prev_chapter_url = t_url
                elif 'next' in aid and not next_chapter_url:
                    if current_chap_id and current_chap_id in tag['href']: next_page_url = t_url
                    else: next_chapter_url = t_url
                elif 'mulu' in aid and not toc_url: toc_url = t_url
            if page_count == 0: first_page_meta = {'title': original_title, 'prev': prev_chapter_url, 'toc_url': toc_url}
            if next_page_url and next_page_url not in visited_urls:
                current_url = next_page_url
                visited_urls.add(next_page_url)
                page_count += 1
            else:
                first_page_meta['next'] = next_chapter_url
                break
        if first_page_meta:
            first_page_meta['content'] = combined_content
            return first_page_meta
        return None

    def get_first_chapter(self, toc_url):
        res = self.get_toc(toc_url)
        return res['chapters'][0]['url'] if res and res['chapters'] else None

# ... (EpubHandler ä¿æŒä¸å˜) ...
# spider_core.py

class EpubHandler:
    def __init__(self):
        self.lib_dir = LIB_DIR
        if not os.path.exists(self.lib_dir): os.makedirs(self.lib_dir)
        # åˆ†é¡µé˜ˆå€¼ï¼šæ¯é¡µå¤§çº¦ 3000 å­—
        self.CHUNK_SIZE = 3000 

    def save_file(self, file_obj):
        filename = secure_filename(file_obj.filename)
        if not filename: filename = f"book_{int(time.time())}.epub"
        filepath = os.path.join(self.lib_dir, filename)
        file_obj.save(filepath)
        return filename

    def _flatten_toc(self, toc, flat_list=None):
        """é€’å½’å±•å¹³ TOC ç»“æ„"""
        if flat_list is None: flat_list = []
        for item in toc:
            if isinstance(item, (list, tuple)):
                # è¿™æ˜¯ä¸€ä¸ªç« èŠ‚èŠ‚ç‚¹
                section = item[0]
                children = item[1] if len(item) > 1 else []
                
                # è·å– href (ebooklib çš„å¯¹è±¡æ¯”è¾ƒå¤æ‚ï¼Œéœ€è¦æå– href)
                href = section.href if hasattr(section, 'href') else ''
                title = section.title if hasattr(section, 'title') else 'æ— æ ‡é¢˜'
                
                if href:
                    flat_list.append({'title': title, 'href': href})
                
                # é€’å½’å¤„ç†å­ç« èŠ‚
                if children:
                    self._flatten_toc(children, flat_list)
            elif hasattr(item, 'href'):
                # ç®€å•èŠ‚ç‚¹
                flat_list.append({'title': item.title, 'href': item.href})
        return flat_list

    def get_toc(self, filename):
        filepath = os.path.join(self.lib_dir, filename)
        if not os.path.exists(filepath): return None
        try:
            book = epub.read_epub(filepath)
            title = book.get_metadata('DC', 'title')[0][0] if book.get_metadata('DC', 'title') else filename
            
            # 1. å°è¯•è§£æé€»è¾‘ç›®å½• (NCX/TOC)
            raw_toc = book.toc
            chapters = []
            
            if raw_toc:
                # ä½¿ç”¨é€’å½’å±•å¹³çš„ç›®å½•
                flat_toc = self._flatten_toc(raw_toc)
                # æ˜ å°„åˆ°æˆ‘ä»¬çš„ URL æ ¼å¼: epub:filename:href:page_index
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”¨ href ä½œä¸ºæ ‡è¯†ç¬¦ï¼Œè€Œä¸æ˜¯ spine indexï¼Œå› ä¸º spine index ä¸ç›´è§‚
                for i, item in enumerate(flat_toc):
                    chapters.append({
                        'title': item['title'], 
                        # ä½¿ç”¨ href ä½œä¸ºå®šä½ç¬¦
                        'url': f"epub:{filename}:{item['href']}:0" 
                    })
            else:
                # å…œåº•ï¼šå¦‚æœæ²¡æœ‰ TOCï¼Œè¿˜æ˜¯ç”¨ Spine
                for i, item in enumerate(book.spine):
                    chapters.append({
                        'title': f"ç¬¬ {i+1} èŠ‚", 
                        'url': f"epub:{filename}:{item[0]}:0" # item[0] æ˜¯ item_id
                    })

            return {'title': title, 'chapters': chapters}
        except Exception as e: 
            print(f"EPUB TOC Error: {e}")
            return None

    def get_chapter_content(self, filename, item_identifier, page_index=0):
        """
        :param item_identifier: å¯ä»¥æ˜¯ href (å¦‚ chapter1.html) æˆ– item_id
        :param page_index: åˆ†é¡µç´¢å¼•ï¼Œ0 å¼€å§‹
        """
        filepath = os.path.join(self.lib_dir, filename)
        try:
            book = epub.read_epub(filepath)
            
            # 1. å¯»æ‰¾å¯¹åº”çš„ Item
            target_item = None
            # å…ˆå°è¯•é€šè¿‡ href æ‰¾
            for item in book.get_items():
                if item.get_name() == item_identifier:
                    target_item = item
                    break
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œå°è¯•é€šè¿‡ ID æ‰¾
            if not target_item:
                target_item = book.get_item_with_id(item_identifier)
            
            if not target_item:
                return {'title': 'é”™è¯¯', 'content': ['æœªæ‰¾åˆ°è¯¥ç« èŠ‚å†…å®¹']}

            # 2. è§£æå†…å®¹
            soup = BeautifulSoup(target_item.get_content(), 'html.parser')
            
            # å°è¯•è·å–ç« èŠ‚æ ‡é¢˜
            title_tag = soup.find(['h1', 'h2'])
            current_title = title_tag.get_text(strip=True) if title_tag else "æœªçŸ¥ç« èŠ‚"
            
            # æå–æ­£æ–‡å¹¶æ¸…æ´—
            raw_lines = [p.get_text(strip=True) for p in soup.find_all(['p', 'div']) if p.get_text(strip=True)]
            
            # 3. [æ ¸å¿ƒ] æ‰§è¡Œé•¿ç« èŠ‚åˆ†é¡µé€»è¾‘
            # å°†æ‰€æœ‰è¡Œåˆå¹¶æˆå¤§æ–‡æœ¬ï¼Œå†é‡æ–°åˆ‡åˆ†ï¼Œæˆ–è€…ç›´æ¥æŒ‰è¡Œæ•°åˆ‡åˆ†
            # è¿™é‡Œé‡‡ç”¨â€œæŒ‰å­—ç¬¦æ•°èšåˆååˆ‡åˆ†â€çš„ç­–ç•¥ï¼Œä½“éªŒæ›´å¥½
            full_text = "\n".join(raw_lines)
            total_len = len(full_text)
            
            # å¦‚æœå†…å®¹éå¸¸çŸ­ï¼Œä¸åˆ†é¡µ
            if total_len <= self.CHUNK_SIZE:
                chunks = [raw_lines]
            else:
                # ç®€å•ç²—æš´åˆ†é¡µï¼šæŒ‰è¡Œç´¯åŠ ï¼Œè¶…è¿‡é˜ˆå€¼å°±åˆ‡
                chunks = []
                current_chunk = []
                current_count = 0
                for line in raw_lines:
                    current_chunk.append(line)
                    current_count += len(line)
                    if current_count >= self.CHUNK_SIZE:
                        chunks.append(current_chunk)
                        current_chunk = []
                        current_count = 0
                if current_chunk: chunks.append(current_chunk)

            # 4. æ ¡éªŒé¡µç 
            if page_index >= len(chunks): page_index = len(chunks) - 1
            if page_index < 0: page_index = 0
            
            final_content = chunks[page_index]
            
            # 5. æ„å»ºä¸Šä¸€é¡µ/ä¸‹ä¸€é¡µé“¾æ¥
            # é€»è¾‘ï¼š
            # - å¦‚æœè¿˜æœ‰ä¸‹ä¸€é¡µ (sub-page)ï¼ŒNext æŒ‡å‘ page_index + 1
            # - å¦‚æœæ²¡æœ‰ä¸‹ä¸€é¡µï¼ŒNext æŒ‡å‘ ä¸‹ä¸€ä¸ªæ–‡ä»¶çš„ç¬¬ 0 é¡µ (éœ€è¦è®¡ç®— TOC é¡ºåº)
            
            # è¿™é‡Œç®€åŒ–å¤„ç†ï¼šæˆ‘ä»¬åªå¤„ç†å†…éƒ¨ç¿»é¡µã€‚è·¨ç« ç¿»é¡µéœ€è¦çŸ¥é“ TOC çš„é¡ºåºã€‚
            # ä¸ºäº†å®ç°è·¨ç« ï¼Œæˆ‘ä»¬éœ€è¦é‡æ–°è·å–ä¸€æ¬¡ TOC åˆ—è¡¨æ¥å®šä½
            
            prev_url = None
            next_url = None
            
            # å†…éƒ¨ç¿»é¡µ
            if page_index > 0:
                prev_url = f"epub:{filename}:{item_identifier}:{page_index-1}"
            if page_index < len(chunks) - 1:
                next_url = f"epub:{filename}:{item_identifier}:{page_index+1}"
            
            # è·¨æ–‡ä»¶ç¿»é¡µ (å¦‚æœå†…éƒ¨æ²¡ç¿»é¡µäº†)
            if not prev_url or not next_url:
                toc_data = self.get_toc(filename) # è¿™æ­¥å¯èƒ½ç•¥è€—æ—¶ï¼Œä½†ä¸ºäº†å‡†ç¡®æ€§å¿…é¡»åš
                if toc_data:
                    chapters = toc_data['chapters']
                    # æ‰¾åˆ°å½“å‰ç« èŠ‚åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•
                    # æ„é€ å½“å‰çš„ URL å‰ç¼€è¿›è¡ŒåŒ¹é…
                    current_base = f"epub:{filename}:{item_identifier}"
                    
                    curr_idx = -1
                    for i, chap in enumerate(chapters):
                        if chap['url'].startswith(current_base):
                            curr_idx = i
                            break
                    
                    if curr_idx != -1:
                        # è·¨ç« ä¸Šä¸€é¡µ
                        if not prev_url and curr_idx > 0:
                            # ä¸Šä¸€ç« çš„é“¾æ¥ (é»˜è®¤è·³åˆ°ç¬¬0é¡µï¼Œå¦‚æœæƒ³è·³åˆ°æœ€åä¸€é¡µæ¯”è¾ƒéº»çƒ¦ï¼Œæš‚å®šç¬¬0é¡µ)
                            prev_url = chapters[curr_idx - 1]['url']
                        
                        # è·¨ç« ä¸‹ä¸€é¡µ
                        if not next_url and curr_idx < len(chapters) - 1:
                            next_url = chapters[curr_idx + 1]['url']

            return {
                'title': f"{current_title} ({page_index+1}/{len(chunks)})" if len(chunks)>1 else current_title,
                'content': final_content,
                'prev': prev_url,
                'next': next_url,
                'toc_url': f"epub:{filename}:toc"
            }

        except Exception as e: 
            return {'title': 'Error', 'content': [f"EPUB Error: {str(e)}"]}
# å®ä¾‹åŒ–å¯¹è±¡
crawler_instance = NovelCrawler()
searcher = SearchHelper()
epub_handler = EpubHandler()