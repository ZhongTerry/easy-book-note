import time
import random
import re
import os
import importlib.util
import hashlib
from urllib.parse import urljoin, urlparse 
from urllib.request import getproxies
from curl_cffi import requests as cffi_requests
from bs4 import BeautifulSoup
from lxml import html as lxml_html
from pypinyin import lazy_pinyin, Style
from concurrent.futures import ThreadPoolExecutor, as_completed
from ebooklib import epub
from werkzeug.utils import secure_filename
from shared import BASE_DIR, LIB_DIR

# ==========================================
# 0. è¾…åŠ©å·¥å…·
# ==========================================
def parse_chapter_id(text):
    if not text: return -1
    text = text.strip()
    match = re.search(r'(?:ç¬¬)?\s*([0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+)\s*[ç« èŠ‚å›å¹•]', text)
    if match: return _smart_convert_int(match.group(1))
    match = re.search(r'^(\d+)', text)
    if match: return int(match.group(1))
    return -1

def _smart_convert_int(s):
    try: return int(s)
    except: pass
    common_map = {'é›¶':0, 'ä¸€':1, 'äºŒ':2, 'ä¸‰':3, 'å››':4, 'äº”':5, 'å…­':6, 'ä¸ƒ':7, 'å…«':8, 'ä¹':9, 'å':10, 'ç™¾':100, 'åƒ':1000, 'ä¸‡':10000, 'ä¸¤':2}
    if len(s) == 1 and s in common_map: return common_map[s]
    res = 0
    unit = 1
    temp = 0
    for char in reversed(s):
        if char in common_map:
            val = common_map[char]
            if val >= 10:
                if val > unit: unit = val
                else: unit *= val
            else: temp += val * unit
    if temp == 0 and 'å' in s: temp = 10
    return temp if temp > 0 else 0

# ==========================================
# 1. æ’ä»¶ç®¡ç†å™¨
# ==========================================
class AdapterManager:
    def __init__(self, folder="adapters"):
        self.folder = os.path.join(BASE_DIR, folder)
        self.adapters = []
        if not os.path.exists(self.folder): os.makedirs(self.folder)
        self.load_plugins()

    def load_plugins(self):
        self.adapters = []
        for f in os.listdir(self.folder):
            if f.endswith(".py") and f != "__init__.py":
                try:
                    spec = importlib.util.spec_from_file_location(f[:-3], os.path.join(self.folder, f))
                    mod = importlib.util.module_from_spec(spec)
                    spec.loader.exec_module(mod)
                    for n in dir(mod):
                        obj = getattr(mod, n)
                        if isinstance(obj, type) and "Adapter" in n: self.adapters.append(obj())
                except: pass
        print(f"[System] å·²åŠ è½½ {len(self.adapters)} ä¸ªç«™ç‚¹é€‚é…æ’ä»¶")

    def find_match(self, url):
        for a in self.adapters:
            if hasattr(a, 'can_handle') and a.can_handle(url): return a
        return None

plugin_mgr = AdapterManager()

# ==========================================
# 2. æœç´¢åŠ©æ‰‹
# ==========================================
class SearchHelper:
    def __init__(self):
        self.impersonate = "chrome110"
        self.timeout = 10
        self.proxies = self._get_proxies()
    
    def _get_proxies(self):
        try: return getproxies()
        except: return None

    def get_pinyin_key(self, text):
        clean = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', '', text)
        clean = re.sub(r'(å°è¯´|ç¬”è¶£é˜|æœ€æ–°ç« èŠ‚|å…¨æ–‡é˜…è¯»)', '', clean)
        try:
            s = lazy_pinyin(clean, style=Style.FIRST_LETTER)
            k = ''.join(s).lower()
            return k[:15] if k else "temp"
        except: return "temp"
    def _is_valid_novel_site(self, url):
        """
        [æ–°å¢] ç™½åå•æ ¡éªŒï¼šåªå…è®¸é•¿å¾—åƒå°è¯´ç«™çš„ URL é€šè¿‡
        ç”¨äºå¯¹æŠ— Bing å›½å†…ç‰ˆçš„åƒåœ¾ç»“æœ
        """
        u = url.lower()
        # 1. å¿…é¡»åŒ…å« http
        if not u.startswith('http'): return False
        
        # 2. æ’é™¤çŸ¥ååƒåœ¾ç«™
        bad_domains = ['zhihu', 'douban', 'baidu', 'bilibili', 'video', 'news', '163.com', 'qq.com', 'sohu']
        if any(d in u for d in bad_domains): return False
        
        # 3. [æ ¸å¿ƒ] å¿…é¡»åŒ…å«å°è¯´ç«™å¸¸è§ç‰¹å¾
        valid_signs = ['book', 'novel', 'read', 'shu', 'biqu', 'bqg', 'txt', '88', 'wx', 'du', 'yuedu', 'chapter']
        # æˆ–è€… URL ç»“æ„åŒ…å«æ•°å­— (é€šå¸¸æ˜¯ä¹¦ID)
        has_id = bool(re.search(r'\d+', u))
        
        if any(s in u for s in valid_signs) or has_id:
            return True
        return False
    def _is_junk(self, title, url):
        t = title.lower()
        u = url.lower()
        bad_domains = ['facebook', 'twitter', 'zhihu', 'douban', 'baidu', 'baike', 'csdn', 'cnblogs', 'youtube', 'bilibili', '52pojie', '163.com', 'sohu', 'microsoft', 'google', 'apple', 'amazon']
        if any(d in u for d in bad_domains): return True
        bad_keywords = ['å·¥å…·', 'ç ´è§£', 'è½¯ä»¶', 'ä¸‹è½½', 'æ•™ç¨‹', 'è§†é¢‘', 'å‰§é€', 'ç™¾ç§‘', 'èµ„è®¯', 'æ‰‹æ¸¸', 'å®˜ç½‘', 'APP']
        if any(k in t for k in bad_keywords): return True
        return False

    def _do_ddg_search(self, keyword):
        url = "https://html.duckduckgo.com/html/"
        data = {'q': f"{keyword} ç¬”è¶£é˜"}
        try:
            resp = cffi_requests.post(url, data=data, impersonate=self.impersonate, timeout=self.timeout, proxies=self.proxies)
            soup = BeautifulSoup(resp.content, 'html.parser')
            results = []
            for link in soup.find_all('a', class_='result__a'):
                title = link.get_text(strip=True)
                href = link.get('href')
                if not href.startswith('http'): continue
                if self._is_junk(title, href): continue
                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'DuckDuckGo ğŸ¦†'
                })
                if len(results) >= 8: break
            return results
        except: return None
    def _do_bing_cn_search(self, keyword):
        """
        [æ–°å¢] Bing å›½å†…ç‰ˆä¸“ç”¨å¼•æ“ (ç›´è¿å¯ç”¨)
        """
        print(f"[Search] Trying Bing CN (Direct): {keyword}")
        # å…³é”®è¯å¼ºåˆ¶åŠ ä¸Š "ç¬”è¶£é˜"ï¼Œè¿™åœ¨å›½å†…æœ€å¥½ç”¨
        query = f"{keyword} ç¬”è¶£é˜ åœ¨çº¿é˜…è¯»"
        url = "https://cn.bing.com/search"
        params = {'q': query}
        
        try:
            # æ³¨æ„ï¼šä¸ä½¿ç”¨ proxiesï¼Œå¼ºåˆ¶ç›´è¿
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                timeout=8
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # å®½å®¹è§£æ
            links = soup.select('li.b_algo h2 a') or soup.select('h2 a')
            results = []
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                # ä¸¥æ ¼çš„ç™½åå•è¿‡æ»¤
                if not self._is_valid_novel_site(href):
                    continue

                results.append({
                    'title': self._clean_title(title),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Bing CN ğŸ‡¨ğŸ‡³'
                })
                if len(results) >= 8: break
            return results
        except Exception as e:
            print(f"[Search] Bing CN Error: {e}")
            return []
    def _do_360_search(self, keyword):
        """
        [ä¸»åŠ›] 360æœç´¢ + å¤šçº¿ç¨‹å¹¶å‘è§£å¯†
        """
        print(f"[Search] ğŸ” [è°ƒè¯•æ¨¡å¼] ä»…å°è¯• 360æœç´¢: {keyword}")
        url = "https://www.so.com/s"
        # å…³é”®è¯åŠ â€œç›®å½•â€ï¼Œç»“æœæ›´ç²¾å‡†
        params = {'q': f"{keyword} å…è´¹é˜…è¯» ç›®å½•"} 
        
        try:
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                timeout=self.timeout
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            raw_results = []
            # 360 ç»“æœé€‰æ‹©å™¨
            links = soup.select('ul.result li.res-list h3 a')
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('data-url') or link.get('href')
                
                if not href: continue
                
                # å°è¯•ä» URL å‚æ•°æå– (æŸäº› 360 é“¾æ¥æ˜¯ ...?url=http%3A%2F%2F...)
                if "so.com/link" in href:
                    try:
                        from urllib.parse import parse_qs, urlparse
                        qs = parse_qs(urlparse(href).query)
                        if 'url' in qs: href = qs['url'][0]
                    except: pass

                if self._is_junk(title, href): continue
                
                # å…ˆå­˜ä¸‹æ¥ï¼Œç¨åå¹¶å‘è§£å¯†
                raw_results.append({
                    'title': self._clean_title(title),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': '360 ğŸŸ¢'
                })
                if len(raw_results) >= 8: break
            
            if not raw_results:
                print("[Search] 360 æœªæ‰¾åˆ°åˆæ­¥ç»“æœ")
                return []

            # å¤šçº¿ç¨‹å¹¶å‘è§£å¯†çœŸå® URL
            print(f"[Search] æ­£åœ¨å¹¶å‘è§£æ {len(raw_results)} ä¸ª 360 é“¾æ¥...")
            final_results = []
            
            with ThreadPoolExecutor(max_workers=8) as exe:
                future_to_item = {
                    exe.submit(self._resolve_real_url, item['url']): item 
                    for item in raw_results
                }
                
                for future in as_completed(future_to_item):
                    item = future_to_item[future]
                    try:
                        real_url = future.result()
                        # å†æ¬¡æ ¡éªŒè§£å¯†åçš„ URL æ˜¯å¦ä¸ºå°è¯´ç«™
                        if self._is_valid_novel_site(real_url):
                            item['url'] = real_url
                            final_results.append(item)
                    except: pass
            
            return final_results

        except Exception as e:
            print(f"[Search] 360 Error: {e}")
            return []
    def _resolve_real_url(self, url):
        """
        [æ–°å¢] è§£æ 360/ç™¾åº¦çš„åŠ å¯†è·³è½¬é“¾æ¥
        åŸç†ï¼šå‘é€è¯·æ±‚ä½†ä¸è·Ÿéšè·³è½¬ (allow_redirects=False)ï¼Œç›´æ¥è¯»å– Location å¤´
        """
        # å¦‚æœä¸æ˜¯åŠ å¯†é“¾æ¥ï¼Œç›´æ¥è¿”å›
        if "so.com/link" not in url and "baidu.com/link" not in url:
            return url
            
        try:
            # å¿…é¡»ç¦æ­¢è‡ªåŠ¨è·³è½¬ï¼Œå¦åˆ™ä¼šä¸‹è½½æ•´ä¸ªç›®æ ‡ç½‘é¡µï¼Œæµªè´¹æµé‡å’Œæ—¶é—´
            resp = cffi_requests.get(
                url, 
                impersonate=self.impersonate, 
                timeout=5, 
                allow_redirects=False 
            )
            
            # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦ä¸º 301/302 é‡å®šå‘
            if resp.status_code in [301, 302]:
                # è·å–çœŸå®åœ°å€ (Location å¤´)
                real_url = resp.headers.get('Location') or resp.headers.get('location')
                if real_url:
                    return real_url
        except Exception as e: # <--- è¿™é‡ŒåŠ äº†ç©ºæ ¼ï¼Œä¿®å¤äº†è¯­æ³•é”™è¯¯
            print(f"[Search] è§£æè·³è½¬å¤±è´¥: {e}")
            pass
            
        # å¦‚æœè§£æå¤±è´¥ï¼Œä¸ºäº†ä¸è®©ç¨‹åºå´©æºƒï¼ŒåŸæ ·è¿”å›åŠ å¯†é“¾æ¥
        # è™½ç„¶è¿™ä¼šå¯¼è‡´å‰ç«¯å¯èƒ½æ‰“ä¸å¼€ï¼Œä½†æ€»æ¯”æ²¡æœ‰å¥½
        return url
    def _do_sogou_search(self, keyword):
        print(f"[Search] ğŸš€ Bing å¤±è´¥ï¼Œæ­£åœ¨å°è¯•æœç‹—æœç´¢: {keyword}")
        query = f"{keyword} ç¬”è¶£é˜"
        url = "https://www.sogou.com/web"
        params = {'query': query}
        
        try:
            # æœç‹—éœ€è¦ä¸€ä¸ªæ¯”è¾ƒçœŸå®çš„ Referer
            headers = {
                "Referer": "https://www.sogou.com/",
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
            }
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate, 
                headers=headers,
                timeout=10
            )
            soup = BeautifulSoup(resp.content, 'html.parser')
            
            # æœç‹—çš„ç»“æ„æ¯”è¾ƒç‰¹æ®Šï¼Œé€šå¸¸åœ¨ .rb-tit a æˆ– h3 a
            links = soup.select('.rb-tit a') or soup.select('h3 a')
            results = []
            
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                
                # æœç‹—çš„ href å¾€å¾€æ˜¯ç»è¿‡æ··æ·†çš„ /link?url=...
                if not href: continue
                if not href.startswith('http'):
                    href = urljoin("https://www.sogou.com", href)

                # ç®€å•è¿‡æ»¤åƒåœ¾ç»“æœ
                if self._is_junk(title, href): continue
                if not self._is_valid_novel_site(href): continue

                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Sogou ğŸ¶'
                })
                if len(results) >= 8: break
            return results
        except Exception as e:
            print(f"[Search] Sogou Error: {e}")
            return []
    def _do_bing_search(self, keyword):
        url = "https://www.bing.com/search"
        params = {'q': f"{keyword} ç¬”è¶£é˜", 'setmkt': 'en-US'}
        try:
            resp = cffi_requests.get(url, params=params, impersonate=self.impersonate, timeout=self.timeout, proxies=self.proxies)
            soup = BeautifulSoup(resp.content, 'html.parser')
            links = soup.select('li.b_algo h2 a') or soup.select('li h2 a') or soup.select('h2 a')
            results = []
            for link in links:
                title = link.get_text(strip=True)
                href = link.get('href')
                if not href.startswith('http') or self._is_junk(title, href): continue
                results.append({
                    'title': re.split(r'(-|_|\|)', title)[0].strip(),
                    'url': href,
                    'suggested_key': self.get_pinyin_key(keyword),
                    'source': 'Bing ğŸŒ'
                })
                if len(results) >= 8: break
            return results
        except: return []
    def _clean_title(self, title):
        """
        æ¸…æ´—æ ‡é¢˜ï¼šå»é™¤ç±»ä¼¼ "- ç¬”è¶£é˜", "_æ— å¼¹çª—" ç­‰åç¼€
        """
        if not title: return "æœªçŸ¥æ ‡é¢˜"
        # ä½¿ç”¨æ­£åˆ™åˆ†å‰² - _ | ç­‰ç¬¦å·ï¼Œåªå–ç¬¬ä¸€éƒ¨åˆ†
        return re.split(r'(-|_|\|)', title)[0].strip()

    def _is_junk(self, title, url):
        """
        åˆ¤æ–­æ˜¯å¦ä¸ºåƒåœ¾ç»“æœï¼ˆéå°è¯´å†…å®¹ï¼‰
        """
        t = title.lower()
        u = url.lower()
        
        # 1. æ’é™¤çŸ¥åéå°è¯´åŸŸå
        bad_domains = ['zhihu.com', 'douban.com', 'baike.baidu.com', 'csdn.net', 'cnblogs.com', 'bilibili.com', 'tieba.baidu.com', '163.com', 'sohu.com', 'sina.com']
        if any(d in u for d in bad_domains): return True
        
        # 2. æ’é™¤æ˜æ˜¾éå°è¯´æ ‡é¢˜å…³é”®è¯
        bad_keywords = ['ä¸‹è½½', 'txtä¸‹è½½', 'ç²¾æ ¡ç‰ˆ', 'æ•™ç¨‹', 'ç™¾ç§‘', 'èµ„è®¯', 'æ‰‹æ¸¸', 'æ”»ç•¥', 'è§†é¢‘', 'åœ¨çº¿è§‚çœ‹']
        if any(k in t for k in bad_keywords): return True
        
        return False
    # def search_bing(self, keyword):
    #     # 1. ç­–ç•¥ Aï¼šå¦‚æœæœ‰ä»£ç†ï¼Œé¦–é€‰ DuckDuckGo å’Œ Bing å›½é™…ç‰ˆ
    #     # (è¿™ä¸¤ä¸ªç»“æœæœ€å¹²å‡€ï¼Œä¼˜å…ˆçº§æœ€é«˜)
    #     if self.proxies:
    #         res = self._do_ddg_search(keyword)
    #         if res: return res
            
    #         res = self._do_bing_search(keyword)
    #         if res: return res
            
    #     # 2. ç­–ç•¥ Bï¼šå›½å†…ç›´è¿ç­–ç•¥ (Bing CN -> 360 -> ç™¾åº¦)
        
    #     # ä¼˜å…ˆçº§ 1: Bing å›½å†…ç‰ˆ (cn.bing.com)
    #     # å°è¯•ç›´è¿ Bingï¼Œå¦‚æœæœåŠ¡å™¨ IP æ²¡è¢«å¾®è½¯æ‹‰é»‘ï¼Œè¿™ä¸ªç»“æœæœ€å¥½
    #     res = self._do_bing_cn_search(keyword)
    #     if res and len(res) > 0:
    #         return res

    #     # ä¼˜å…ˆçº§ 2: 360æœç´¢ (So.com)
    #     # å¦‚æœ Bing æŒ‚äº†ï¼ˆè¿”å›ç©ºï¼‰ï¼Œå°è¯• 360ï¼ˆå¸¦å¤šçº¿ç¨‹è§£å¯†ï¼Œæœºæˆ¿IPé€šè¿‡ç‡é«˜ï¼‰
    #     res = self._do_360_search(keyword)
    #     if res: return res
        
    #     # ä¼˜å…ˆçº§ 3: ç™¾åº¦æœç´¢ (Baidu)
    #     # æœ€åå…œåº•ï¼Œæ”¶å½•å…¨ä½†å¯èƒ½æœ‰å¹¿å‘Šæˆ–éªŒè¯ç 
    #     return self._do_baidu_search(keyword)
    def search_bing(self, keyword):
        return self._do_360_search(keyword)
    def _resolve_real_url(self, url):
        """
        [æ–°å¢] è§£æ 360/ç™¾åº¦çš„åŠ å¯†è·³è½¬é“¾æ¥
        åŸç†ï¼šå‘é€è¯·æ±‚ä½†ä¸è·Ÿéšè·³è½¬ (allow_redirects=False)ï¼Œç›´æ¥è¯»å– Location å¤´
        """
        # å¦‚æœä¸æ˜¯åŠ å¯†é“¾æ¥ï¼Œç›´æ¥è¿”å›
        if "so.com/link" not in url and "baidu.com/link" not in url:
            return url
            
        try:
            # å¿…é¡»ç¦æ­¢è‡ªåŠ¨è·³è½¬ï¼Œå¦åˆ™ä¼šä¸‹è½½æ•´ä¸ªç›®æ ‡ç½‘é¡µï¼Œæµªè´¹æµé‡å’Œæ—¶é—´
            resp = cffi_requests.get(
                url, 
                impersonate=self.impersonate, 
                timeout=5, 
                allow_redirects=False 
            )
            
            # æ£€æŸ¥çŠ¶æ€ç æ˜¯å¦ä¸º 301/302 é‡å®šå‘
            if resp.status_code in [301, 302]:
                # è·å–çœŸå®åœ°å€ (Location å¤´)
                real_url = resp.headers.get('Location') or resp.headers.get('location')
                if real_url:
                    return real_url
        except Exception as e: # <--- è¿™é‡ŒåŠ äº†ç©ºæ ¼ï¼Œä¿®å¤äº†è¯­æ³•é”™è¯¯
            print(f"[Search] è§£æè·³è½¬å¤±è´¥: {e}")
            pass
            
        # å¦‚æœè§£æå¤±è´¥ï¼Œä¸ºäº†ä¸è®©ç¨‹åºå´©æºƒï¼ŒåŸæ ·è¿”å›åŠ å¯†é“¾æ¥
        # è™½ç„¶è¿™ä¼šå¯¼è‡´å‰ç«¯å¯èƒ½æ‰“ä¸å¼€ï¼Œä½†æ€»æ¯”æ²¡æœ‰å¥½
        return url

    # === [æ ¸å¿ƒæ–°å¢ 2] ç™¾åº¦æœç´¢ (Baidu) - æ”¶å½•æœ€å…¨ï¼Œä½œä¸ºå¤‡ç”¨ ===
    def _do_baidu_search(self, keyword):
        print(f"[Search] ğŸ” å°è¯• ç™¾åº¦æœç´¢: {keyword}")
        url = "https://www.baidu.com/s"
        # æŠ€å·§ï¼šwd å¿…é¡»å¸¦ "æœ€æ–°ç« èŠ‚"ï¼Œå¦åˆ™å…¨æ˜¯è´´å§
        params = {'wd': f"{keyword} å°è¯´ æœ€æ–°ç« èŠ‚"}
        
        try:
            # ç™¾åº¦å¯¹ User-Agent éå¸¸æ•æ„Ÿï¼Œä¸”å¯¹ Referer æœ‰æ ¡éªŒ
            headers = {
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36",
                "Referer": "https://www.baidu.com/"
            }
            # å¿…é¡»ä¸å¸¦ä»£ç†è®¿é—®ç™¾åº¦å›½å†…ç‰ˆï¼Œå¦åˆ™å¯èƒ½è·³åˆ°éªŒè¯ç 
            resp = cffi_requests.get(
                url, params=params, 
                impersonate=self.impersonate,
                headers=headers,
                timeout=6
            )
            
            # æ£€æµ‹æ˜¯å¦è¢«ç™¾åº¦æ‹¦æˆª
            if "wappass.baidu.com" in resp.url or "éªŒè¯ç " in resp.text:
                print("[Search] âš ï¸ è§¦å‘ç™¾åº¦éªŒè¯ç ï¼Œè·³è¿‡")
                return []

            soup = BeautifulSoup(resp.content, 'html.parser')
            results = []
            
            # ç™¾åº¦çš„ç»“æœå—é€šå¸¸æ˜¯ c-container
            containers = soup.select('div.c-container')
            
            for box in containers:
                try:
                    # æå–æ ‡é¢˜é“¾æ¥
                    title_elem = box.select_one('h3 a') or box.select_one('a')
                    if not title_elem: continue
                    
                    title = title_elem.get_text(strip=True)
                    href = title_elem.get('href') # è¿™æ˜¯ç™¾åº¦çš„åŠ å¯†é“¾æ¥
                    
                    # æå–ä¸‹æ–¹æ˜¾ç¤ºçš„çœŸå®åŸŸå (è¾…åŠ©åˆ¤æ–­)
                    footer_text = box.get_text()
                    
                    # å¼ºåŠ›è¿‡æ»¤
                    if self._is_junk(title, ""): continue # URLæ˜¯åŠ å¯†çš„ï¼Œæš‚æ—¶åªèƒ½æ£€æŸ¥æ ‡é¢˜
                    
                    # ç™¾åº¦ç‰¹è‰²ï¼šå¹¿å‘Šé€šå¸¸æœ‰ 'å¹¿å‘Š' å­—æ ·
                    if "å¹¿å‘Š" in footer_text: continue

                    # æ—¢ç„¶æ‹¿ä¸åˆ°çœŸå®URLï¼ˆéœ€è¦å†æ¬¡è¯·æ±‚è§£å¯†ï¼Œå¤ªæ…¢ï¼‰ï¼Œ
                    # æˆ‘ä»¬è¿™é‡Œåšä¸€ä¸ªå¤§èƒ†çš„ç­–ç•¥ï¼š
                    # ç›´æ¥è¿”å›è¿™ä¸ªåŠ å¯†é“¾æ¥ã€‚
                    # å› ä¸ºä½ çš„ NovelCrawler.run() èƒ½å¤Ÿå¤„ç† 302 è·³è½¬ï¼
                    
                    results.append({
                        'title': self._clean_title(title),
                        'url': href, # è¿™æ˜¯ä¸€ä¸ª http://www.baidu.com/link?url=...
                        'suggested_key': self.get_pinyin_key(keyword),
                        'source': 'Baidu ğŸ”µ'
                    })
                    if len(results) >= 6: break
                except: pass
                
            return results
        except Exception as e:
            print(f"[Search] Baidu Error: {e}")
            return []
# ==========================================
# 3. å°è¯´çˆ¬è™« (NovelCrawler - ä¿®å¤KeyErrorç‰ˆ)
# ==========================================
class NovelCrawler:
    def __init__(self):
        self.impersonate = "chrome110"
        self.timeout = 15
        self.proxies = getproxies()
    # spider_core.py -> NovelCrawler ç±»å†…éƒ¨
    # ==========================================
    # [æ–°å¢] æ™ºèƒ½æ¢æºæ ¸å¿ƒé€»è¾‘
    # ==========================================
    # === [è°ƒè¯•å¢å¼ºç‰ˆ] æœç´¢å¹¶è¿”å›å¯ç”¨æºåˆ—è¡¨ ===
    def search_alternative_sources(self, book_name, target_chapter_id):
        print(f"\n[Switch] ğŸš€ å¯åŠ¨æ¢æºæµç¨‹")
        print(f"[Switch] ç›®æ ‡ä¹¦å:ã€Š{book_name}ã€‹ (å¦‚æœè¿™æ˜¯æ‹¼éŸ³ï¼Œæœç´¢ç»å¯¹ä¼šå¤±è´¥ï¼)")
        print(f"[Switch] ç›®æ ‡ç« èŠ‚ID: {target_chapter_id}")
        
        # 1. æœç´¢
        from spider_core import searcher 
        search_results = searcher.search_bing(book_name)
        
        if not search_results:
            print("[Switch] âŒ æœç´¢å¼•æ“è¿”å› 0 ä¸ªç»“æœã€‚è¯·æ£€æŸ¥ä¹¦åæ˜¯å¦æ­£ç¡®ã€‚")
            return []
            
        print(f"[Switch] ğŸ” æœç´¢å¼•æ“è¿”å›äº† {len(search_results)} ä¸ªå¤‡é€‰æº")
        for i, res in enumerate(search_results):
            print(f"   [{i+1}] {res['title']} -> {res['url']}")

        valid_sources = []
        
        # 2. å®šä¹‰éªŒè¯ä»»åŠ¡ (å¸¦è¯¦ç»†æ—¥å¿—)
        def check_source(result):
            toc_url = result['url']
            domain = urlparse(toc_url).netloc
            print(f"[Switch] âš¡ å¼€å§‹æ£€æŸ¥æº: {domain} ...")
            
            try:
                # æŠ“å–ç›®å½•
                toc = self.get_toc(toc_url)
                if not toc or not toc.get('chapters'):
                    print(f"[Switch] âš ï¸ æº {domain} ç›®å½•è§£æå¤±è´¥æˆ–ä¸ºç©º")
                    return None
                
                # 3. å¯»æ‰¾åŒ¹é… ID
                # å€’åºæŸ¥æ‰¾
                # print(f"[Switch] æº {domain} å…±æœ‰ {len(toc['chapters'])} ç« ï¼Œæ­£åœ¨æ¯”å¯¹ ID...")
                
                # æ—¢ç„¶æˆ‘ä»¬å·²ç»æœ‰äº† parse_chapter_idï¼Œæˆ‘ä»¬ç›´æ¥çœ‹èƒ½ä¸èƒ½å¯¹ä¸Š
                # ä¸ºäº†è°ƒè¯•ï¼Œæˆ‘ä»¬æ‰“å°ä¸€ä¸‹è¯¥æºæœ€åä¸€ç« çš„ IDï¼Œçœ‹çœ‹åç¦»å¤šè¿œ
                last_chap = toc['chapters'][-1]
                # print(f"   -> {domain} æœ€åä¸€ç« : ID={last_chap.get('id')} ({last_chap.get('name')})")

                for chap in reversed(toc['chapters']):
                    if chap.get('id') == target_chapter_id:
                        print(f"[Switch] âœ… å‘½ä¸­ç›®æ ‡! [{domain}] -> {chap['name']}")
                        return {
                            "source": domain,
                            "url": chap['url'],
                            "title": chap['name'],
                            "toc_url": toc_url
                        }
            except Exception as e:
                print(f"[Switch] âŒ æ£€æŸ¥æº {domain} æ—¶å‘ç”Ÿå¼‚å¸¸: {e}")
            return None

        # 3. å¹¶å‘éªŒè¯
        candidates = search_results[:6]
        print(f"[Switch] æ­£åœ¨å¹¶å‘æ£€æŸ¥å‰ {len(candidates)} ä¸ªç»“æœ...")
        
        with ThreadPoolExecutor(max_workers=6) as exe:
            futures = [exe.submit(check_source, res) for res in candidates]
            for future in as_completed(futures):
                res = future.result()
                if res:
                    valid_sources.append(res)
        
        print(f"[Switch] ğŸ æµç¨‹ç»“æŸï¼Œå…±æ‰¾åˆ° {len(valid_sources)} ä¸ªå¯ç”¨æº")
        return valid_sources
    def _get_book_name(self, soup):
        """
        é€šç”¨çš„å°è¯´åè¯†åˆ«é€»è¾‘
        """
        # 1. å°è¯•ä»å¸¸è§é¢åŒ…å±‘å¯¼èˆªä¸­æå–
        # åŒ¹é…åŒ…å« 'path', 'breadcrumb', 'crumb' çš„ class æˆ– id
        path_box = soup.find(class_=re.compile(r'path|crumb|breadcrumb', re.I)) or \
                   soup.find(id=re.compile(r'path|crumb|breadcrumb', re.I))
        
        if path_box:
            links = path_box.find_all('a')
            # é€»è¾‘ï¼šé¦–é¡µ > åˆ†ç±» > ä¹¦å > ç« èŠ‚åï¼Œé€šå¸¸å€’æ•°ç¬¬äºŒä¸ªæˆ–ç¬¬ä¸‰ä¸ªæ˜¯ä¹¦å
            if len(links) >= 3:
                # é’ˆå¯¹ä¹¦é¦™é˜è¿™ç§ï¼šé¦–é¡µ(0) > åˆ†ç±»(1) > ä¹¦å(2) > ç« èŠ‚
                return links[2].get_text(strip=True)
            elif len(links) == 2:
                return links[1].get_text(strip=True)

        # 2. å°è¯•ä» Meta Keywords æå– (ç¬¬ä¸€ä¸ªè¯é€šå¸¸æ˜¯ä¹¦å)
        meta_kw = soup.find('meta', attrs={'name': 'keywords'})
        if meta_kw:
            kw = meta_kw.get('content', '').split(',')[0]
            if kw and len(kw) < 20: return kw

        # 3. å°è¯•ä» Title æ ‡ç­¾æ‹†åˆ†
        if soup.title:
            t_text = soup.title.get_text(strip=True)
            # å¸¸è§æ ¼å¼ï¼šç« èŠ‚å_ä¹¦å_ç«™ç‚¹å æˆ– ä¹¦å_ç« èŠ‚å
            if "_" in t_text:
                parts = t_text.split('_')
                for p in parts:
                    if "ç¬¬" not in p and "ç« " not in p and "èŠ‚" not in p:
                        # å‰”é™¤å¸¸è§çš„åç¼€
                        name = re.sub(r'(å°è¯´|å…¨æ–‡|é˜…è¯»|æœ€æ–°ç« èŠ‚|ç¬”è¶£é˜).*', '', p)
                        if len(name) > 1: return name.strip()

        return "æœªçŸ¥ä¹¦å"
    def search_and_switch_source(self, book_name, target_chapter_id):
        """
        æ ¹æ®ä¹¦åå’Œç›®æ ‡ç« èŠ‚IDï¼Œå…¨ç½‘æœç´¢å¤‡é€‰æºï¼Œå¹¶å¯»æ‰¾åŒ¹é…çš„ç« èŠ‚é“¾æ¥
        """
        print(f"[Switch] æ­£åœ¨ä¸ºã€Š{book_name}ã€‹ç¬¬ {target_chapter_id} ç« å¯»æ‰¾æ–°æº...")
        
        # 1. å…¨ç½‘æœç´¢å¤‡é€‰æº (å¤ç”¨ SearchHelper)
        # æœç´¢å…³é”®è¯åŠ ä¸Š "ç›®å½•"ï¼Œæé«˜å‘½ä¸­ç‡
        from spider_core import searcher # ç¡®ä¿å¼•ç”¨
        search_results = searcher.search_bing(book_name)
        
        if not search_results:
            print("[Switch] æœªæœç´¢åˆ°ä»»ä½•ç»“æœ")
            return None

        # 2. å®šä¹‰å•ä¸ªæºçš„éªŒè¯ä»»åŠ¡
        def check_source(result):
            toc_url = result['url']
            domain = urlparse(toc_url).netloc
            
            # ç®€å•è¿‡æ»¤ï¼šå¦‚æœæ˜¯å½“å‰æ­£åœ¨ä½¿ç”¨çš„æº(ç•¥)ï¼Œæˆ–è€…æ˜æ˜¾ä¸æ˜¯å°è¯´ç«™çš„ï¼Œå¯ä»¥åœ¨è¿™é‡Œè¿‡æ»¤
            # è¿™é‡Œå…ˆä¸åšå¤æ‚è¿‡æ»¤ï¼Œä¿¡ä»» SearchHelper çš„é»‘åå•
            
            try:
                # æŠ“å–ç›®å½• (å¤ç”¨ get_tocï¼Œå®ƒä¼šè‡ªåŠ¨è¿›è¡Œ ID è§£æå’Œæ’åº)
                toc = self.get_toc(toc_url)
                if not toc or not toc.get('chapters'):
                    return None
                
                # 3. åœ¨ç›®å½•ä¸­äºŒåˆ†æŸ¥æ‰¾æˆ–éå†å¯»æ‰¾ç›®æ ‡ ID
                # å› ä¸ºæˆ‘ä»¬å·²ç»æ’å¥½åºäº†ï¼Œç†è®ºä¸ŠäºŒåˆ†æ›´å¿«ï¼Œä½†åˆ—è¡¨ä¸é•¿ï¼Œéå†ä¹Ÿè¡Œ
                for chap in toc['chapters']:
                    if chap.get('id') == target_chapter_id:
                        print(f"[Switch] âœ… åœ¨ [{domain}] æ‰¾åˆ°åŒ¹é…ç« èŠ‚: {chap['name']}")
                        return {
                            "new_url": chap['url'],
                            "source_name": domain,
                            "chapter_title": chap['name']
                        }
            except Exception as e:
                # print(f"[Switch] æ£€æŸ¥æº {domain} å¤±è´¥: {e}")
                pass
            return None

        # 3. å¹¶å‘éªŒè¯ (é€Ÿåº¦è‡³ä¸Š)
        # æˆ‘ä»¬åŒæ—¶æ£€æŸ¥å‰ 5 ä¸ªæœç´¢ç»“æœ
        candidates = search_results[:6] 
        found_target = None
        
        with ThreadPoolExecutor(max_workers=6) as exe:
            futures = [exe.submit(check_source, res) for res in candidates]
            
            for future in as_completed(futures):
                res = future.result()
                if res:
                    found_target = res
                    # åªè¦æ‰¾åˆ°ä¸€ä¸ªèƒ½ç”¨çš„ï¼Œç«‹é©¬åœæ­¢å…¶ä»–ä»»åŠ¡ï¼ˆè™½ç„¶çº¿ç¨‹æ± æ²¡æ³•ç«‹åˆ»killï¼Œä½†æˆ‘ä»¬å¯ä»¥breakè¿”å›ï¼‰
                    # å®é™…ä¸Šä¸ºäº†æœ€å¿«å“åº”ï¼Œè°å…ˆè¿”å›å°±ç”¨è°
                    break
        
        return found_target
    def resolve_start_url(self, url):
        """
        [æ–°å¢] æ™ºèƒ½å…¥å£è§£æï¼šå¦‚æœç»™çš„æ˜¯ç›®å½•ï¼Œè‡ªåŠ¨è½¬ä¸ºç¬¬ä¸€ç« 
        """
        print(f"[SmartURL] Analyzing: {url}")
        
        # 1. ç‰¹å¾é¢„åˆ¤ï¼šå¦‚æœ URL ä»¥ .html ç»“å°¾ä¸”åŒ…å«æ•°å­—ï¼Œå¤§æ¦‚ç‡æ˜¯ç« èŠ‚ï¼Œç›´æ¥è¿”å›
        # (è¿™èƒ½èŠ‚çœä¸€æ¬¡ç½‘ç»œè¯·æ±‚)
        if re.search(r'\d+\.html$', url) and "index" not in url:
            return url
            
        # 2. çˆ¬å–é¡µé¢åˆ†æ
        # è¿™é‡Œçš„ run ä¼šè‡ªåŠ¨è¯†åˆ«ç›®å½•é“¾æ¥ (toc_url)
        # æˆ‘ä»¬åˆ©ç”¨ get_toc æ–¹æ³•ï¼Œçœ‹çœ‹å®ƒæ˜¯ä¸æ˜¯ä¸€ä¸ªç›®å½•é¡µ
        
        try:
            # å°è¯•å½“åšç›®å½•æŠ“å–
            toc_data = self.get_toc(url)
            
            # å¦‚æœæŠ“åˆ°äº†å¤§é‡ç« èŠ‚ï¼Œè¯´æ˜å®ƒç¡®å®æ˜¯ç›®å½•
            if toc_data and len(toc_data['chapters']) > 5:
                first_chap = toc_data['chapters'][0]['url']
                print(f"[SmartURL] æ£€æµ‹åˆ°ç›®å½•é¡µï¼Œè‡ªåŠ¨è·³è½¬ç¬¬ä¸€ç« : {first_chap}")
                return first_chap
                
            # å¦‚æœä¸æ˜¯ç›®å½•ï¼Œè¯´æ˜å¯èƒ½æ˜¯ä¸€ä¸ªä¸å¸¦ .html åç¼€çš„ç« èŠ‚é¡µ (å¦‚ xbqg77)
            # æˆ–è€…çˆ¬è™«æ²¡è§£æå¯¹ï¼Œä¸ºäº†å®‰å…¨ï¼ŒåŸæ ·è¿”å›
            return url
            
        except Exception as e:
            print(f"[SmartURL] Resolve Error: {e}")
            return url
    def _fetch_page_smart(self, url, retry=3):
        """åŸºç¡€è¯·æ±‚ï¼šå¢å¼ºäº†å¯¹ lxml è§£æé”™è¯¯çš„æ•è·"""
        for i in range(retry):
            try:
                headers = {
                    "Referer": url, 
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36"
                }
                resp = cffi_requests.get(url, impersonate=self.impersonate, timeout=self.timeout, headers=headers, allow_redirects=True, proxies=self.proxies)
                
                # 1. å°è¯• lxml è§£æ (é€Ÿåº¦å¿«ï¼Œä½†å¯¹ç¼–ç æ•æ„Ÿ)
                try:
                    # [ä¿®å¤] å¢åŠ  parser å‚æ•°ï¼Œå®¹é”™ç‡æ›´é«˜
                    tree = lxml_html.fromstring(resp.content, parser=lxml_html.HTMLParser(encoding='utf-8'))
                    charset = tree.xpath('//meta[contains(@content, "charset")]/@content') or tree.xpath('//meta/@charset')
                    enc = 'utf-8'
                    if charset:
                        match = re.search(r'charset=([\w-]+)', str(charset[0]), re.I)
                        enc = match.group(1) if match else charset[0]
                    return resp.content.decode(enc)
                except Exception:
                    # å¦‚æœ lxml å¤±è´¥ï¼Œå®‰é™åœ°è¿›å…¥ä¸‹é¢çš„æš´åŠ›å°è¯•ï¼Œä¸æ‰“å°é”™è¯¯
                    pass
                
                # 2. æš´åŠ›å°è¯•å¸¸è§ç¼–ç 
                for e in ['utf-8', 'gb18030', 'gbk', 'big5']:
                    try: return resp.content.decode(e)
                    except: continue
                
                # 3. æœ€åå…œåº•
                return resp.content.decode('utf-8', errors='replace')
            except: 
                time.sleep(1)
        return None

    def _get_smart_title(self, soup):
        h1_title = soup.find('h1', class_=re.compile(r'title|chapter|book|name', re.I))
        if h1_title: return h1_title.get_text(strip=True)
        h1s = soup.find_all('h1')
        for h in h1s:
            txt = h.get_text(strip=True)
            if len(txt) <= 4 or any(x in txt for x in ["ç¬”è¶£é˜", "å°è¯´ç½‘", "é˜…è¯»å™¨"]):
                if "logo" in str(h.get('class', '')).lower(): continue
                if h.find_parent(['nav', 'header']): continue
            return txt
        if soup.title: return re.split(r'[_â€”|-]', soup.title.get_text(strip=True))[0].strip()
        return "æœªçŸ¥ç« èŠ‚"

    def _clean_text_lines(self, text):
        if not text: return []
        junk = [r"ä¸€ç§’è®°ä½", r"æœ€æ–°ç« èŠ‚", r"ç¬”è¶£é˜", r"ä¸Šä¸€ç« ", r"ä¸‹ä¸€ç« ", r"åŠ å…¥ä¹¦ç­¾", r"æŠ•æ¨èç¥¨", r"æœ¬ç« æœªå®Œ", r"æœªå®Œå¾…ç»­", r"ps:"]
        lines = []
        for line in text.split('\n'):
            line = line.replace('\xa0', ' ').strip()
            if not line or len(line) < 2: continue
            if len(line) < 50 and any(re.search(p, line, re.I) for p in junk): continue
            if "{" in line and "function" in line: continue
            lines.append(line)
        return lines

    def _extract_content_smart(self, soup):
        for cid in ['txt', 'content', 'chaptercontent', 'BookText', 'showtxt', 'nr1', 'read-content']:
            div = soup.find(id=cid)
            if div:
                for a in div.find_all('a'): a.decompose()
                return self._clean_text_lines(div.get_text('\n'))
        best_div, max_score = None, 0
        for div in soup.find_all('div'):
            if div.get('id') and re.search(r'(nav|foot|header|menu)', str(div.get('id')), re.I): continue
            txt = div.get_text(strip=True)
            score = len(txt) - (len(div.find_all('a')) * 5)
            if score > max_score: max_score, best_div = score, div
        return self._clean_text_lines(best_div.get_text('\n')) if best_div else ["æ­£æ–‡è§£æå¤±è´¥"]

    def _parse_chapters_from_soup(self, soup, base_url):
        links = []
        max_valid_links = 0
        containers = soup.find_all(['div', 'ul', 'dl', 'tbody'])
        if not containers: containers = [soup.body]
        
        junk_keywords = ['æœ€æ–°ç« èŠ‚', 'å…¨æ–‡é˜…è¯»', 'æ— å¼¹çª—', 'å°è¯´', 'ç¬”è¶£é˜', 'åŠ å…¥ä¹¦æ¶', 'æŠ•æ¨èç¥¨', 'ä½œå®¶', 'ä½œè€…']

        for container in containers:
            if container.get('class') and any(x in str(container.get('class')) for x in ['nav', 'footer', 'header', 'hot', 'recommend']): continue
            temp_links = []
            for a in container.find_all('a'):
                raw_text = a.get_text(strip=True)
                href = a.get('href')
                if not href: continue
                if any(k in raw_text for k in junk_keywords) and not re.search(r'\d', raw_text): continue
                
                chap_id = parse_chapter_id(raw_text)
                is_valid = False
                if chap_id > 0: is_valid = True
                elif len(raw_text) > 2 and any(x in raw_text for x in ['ç« ', 'èŠ‚', 'å›', 'å¹•']) and not any(k in raw_text for k in junk_keywords): is_valid = True
                
                if is_valid:
                    full_url = urljoin(base_url, href)
                    match_name = re.search(r'(?:ç¬¬)?\s*[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*[ç« èŠ‚å›](.*)', raw_text)
                    pure_name = match_name.group(1).strip() if match_name else raw_text
                    if full_url:
                        # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ç”Ÿæˆå­—å…¸æ—¶ä¸å¸¦ 'title' é”®ï¼Œç»Ÿä¸€ç”± _standardize_chapters å¤„ç†
                        temp_links.append({'id': chap_id, 'raw_title': raw_text, 'name': pure_name, 'url': full_url})
            
            if len(temp_links) > max_valid_links: max_valid_links = len(temp_links); links = temp_links
        return links

    def _standardize_chapters(self, raw_chapters):
        unique = {c['url']: c for c in raw_chapters}
        processed_list = []
        for c in unique.values():
            raw_title = c.get('title') or c.get('raw_title') or ""
            if any(x in raw_title for x in ['æœ€æ–°ç« èŠ‚', 'å…¨æ–‡é˜…è¯»', 'æ— å¼¹çª—', 'txtä¸‹è½½']) and not re.search(r'\d', raw_title): continue
            chap_id = parse_chapter_id(raw_title)
            pure_name = re.sub(r'^(?:ç¬¬)?\s*[0-9é›¶ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒä¸‡]+\s*[ç« èŠ‚å›]', '', raw_title).strip()
            pure_name = re.sub(r'^\d+\s*\.?\s*', '', pure_name).strip()
            
            c['id'] = chap_id
            c['name'] = pure_name or raw_title
            c['raw_title'] = raw_title
            c['title'] = raw_title # [æ ¸å¿ƒä¿®å¤] è¡¥ä¸Šè¿™ä¸ªé”®ï¼Œé˜²æ­¢åç«¯æŠ¥é”™
            processed_list.append(c)
            
        numbered = [c for c in processed_list if c['id'] > 0]
        others = [c for c in processed_list if c['id'] <= 0]
        numbered.sort(key=lambda x: x['id'])
        
        if len(numbered) > 10:
            final_chapters = numbered
            prologues = [c for c in others if "åº" in c['raw_title'] or "å¼•" in c['raw_title']]
            final_chapters = prologues + final_chapters
        else: final_chapters = others + numbered
        return final_chapters

    def get_toc(self, toc_url):
        adapter = plugin_mgr.find_match(toc_url)
        if adapter: data = adapter.get_toc(self, toc_url)
        else: data = self._general_toc_logic(toc_url)
        
        if not data or not data.get('chapters'): return None
        final_chapters = self._standardize_chapters(data['chapters'])
        return {'title': data['title'], 'chapters': final_chapters}

    def _general_toc_logic(self, toc_url):
        html = self._fetch_page_smart(toc_url)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        raw_chapters = self._parse_chapters_from_soup(soup, toc_url)
        
        pages = set()
        for s in soup.find_all('select'):
            for o in s.find_all('option'):
                v = o.get('value')
                if v:
                    f = urljoin(toc_url, v)
                    if f.rstrip('/') != toc_url.rstrip('/'): pages.add(f)
        if pages:
            with ThreadPoolExecutor(max_workers=5) as exe:
                results = exe.map(lambda u: self._parse_chapters_from_soup(BeautifulSoup(self._fetch_page_smart(u) or "", 'html.parser'), toc_url), sorted(list(pages)))
                for sub in results: raw_chapters.extend(sub)
        return {'title': self._get_smart_title(soup), 'chapters': raw_chapters}

    def get_latest_chapter(self, toc_url):
        toc_data = self.get_toc(toc_url)
        if not toc_data or not toc_data.get('chapters'): return None
        chapters = toc_data['chapters']
        last_chapter = chapters[-1]
        # å…¼å®¹æ€§å¤„ç†
        return {
            "title": last_chapter.get('name', last_chapter.get('raw_title', 'æœªçŸ¥ç« èŠ‚')),
            "url": last_chapter['url'],
            "id": last_chapter.get('id', -1),
            "total_chapters": len(chapters)
        }

    def run(self, url):
        print(f"\n[Run] ğŸš€ å¼€å§‹å¤„ç† URL: {url}")
        
        # 1. å°è¯•åŒ¹é…æ’ä»¶
        adapter = plugin_mgr.find_match(url)
        if adapter:
            print(f"[Run] âœ¨ åŒ¹é…åˆ°é€‚é…å™¨: {adapter.__class__.__name__}")
            result = adapter.run(self, url)
            # æ‰“å°æ’ä»¶è¿”å›çš„ä¹¦å
            print(f"[Run] ğŸ“¦ æ’ä»¶è¿”å›ä¹¦å: {result.get('book_name', 'æœªè·å–')}")
            return result
        
        print(f"[Run] ğŸŒ æœªæ‰¾åˆ°æ’ä»¶ï¼Œä½¿ç”¨é€šç”¨é€»è¾‘...")
        # 2. å¦‚æœæ²¡æ’ä»¶ï¼Œæ‰§è¡Œé€šç”¨é€»è¾‘
        return self._general_run_logic(url)
    
    def _general_run_logic(self, url):
        base_url = url
        if "_" in url:
            normalized = re.sub(r'_\d+\.html', '.html', url)
            if normalized != url: base_url = normalized
        combined_content = []
        first_page_meta = None
        current_url = base_url
        visited_urls = {url, base_url}
        max_pages, page_count = 8, 0
        original_title = ""
        chap_id_match = re.search(r'/(\d+)(?:_\d+)?\.html', base_url)
        current_chap_id = chap_id_match.group(1) if chap_id_match else ""
        while page_count < max_pages:
            html = self._fetch_page_smart(current_url)
            if not html: break
            soup = BeautifulSoup(html, 'html.parser')
            current_title = self._get_smart_title(soup)
            if page_count == 0: original_title = current_title
            elif current_title != original_title and len(current_title) > 3: break
            content = self._extract_content_smart(soup)
            if content and original_title in content[0]: content = content[1:]
            combined_content.extend(content)
            next_page_url, next_chapter_url, prev_chapter_url, toc_url = None, None, None, None
            for a in soup.find_all('a'):
                txt = a.get_text(strip=True).replace(' ', '')
                href = a.get('href')
                if not href or href.startswith('javascript'): continue
                full = urljoin(current_url, href)
                if "ä¸‹ä¸€é¡µ" in txt or "ä¸‹â€”é¡µ" in txt or re.search(r'\(\d+/\d+\)', txt):
                    if current_chap_id and current_chap_id in href: next_page_url = full
                    else: next_chapter_url = full
                elif "ä¸‹ä¸€ç« " in txt or "ä¸‹ç« " in txt: next_chapter_url = full
                if page_count == 0:
                    if "ä¸Šä¸€ç« " in txt or "ä¸Šç« " in txt: prev_chapter_url = full
                    elif "ä¸Šä¸€é¡µ" in txt or "ä¸Šé¡µ" in txt:
                        if current_chap_id and current_chap_id not in href: prev_chapter_url = full
                if "ç›®å½•" in txt: toc_url = full
            for aid in ['pb_prev', 'prev_url', 'pb_next', 'next_url', 'pb_mulu']:
                tag = soup.find(id=aid)
                if not tag or not tag.get('href'): continue
                t_url = urljoin(current_url, tag['href'])
                if 'prev' in aid and page_count == 0 and not prev_chapter_url:
                    if current_chap_id and current_chap_id not in tag['href']: prev_chapter_url = t_url
                elif 'next' in aid and not next_chapter_url:
                    if current_chap_id and current_chap_id in tag['href']: next_page_url = t_url
                    else: next_chapter_url = t_url
                elif 'mulu' in aid and not toc_url: toc_url = t_url
            if page_count == 0: first_page_meta = {'title': original_title, 'prev': prev_chapter_url, 'toc_url': toc_url}
            if next_page_url and next_page_url not in visited_urls:
                current_url = next_page_url
                visited_urls.add(next_page_url)
                page_count += 1
            else:
                first_page_meta['next'] = next_chapter_url
                break
        if first_page_meta:
            first_page_meta['content'] = combined_content
            return first_page_meta
        return None

    def get_first_chapter(self, toc_url):
        res = self.get_toc(toc_url)
        return res['chapters'][0]['url'] if res and res['chapters'] else None

# ... (EpubHandler ä¿æŒä¸å˜) ...
class EpubHandler:
    def __init__(self):
        self.lib_dir = LIB_DIR
        if not os.path.exists(self.lib_dir): os.makedirs(self.lib_dir)

    def save_file(self, file_obj):
        filename = secure_filename(file_obj.filename)
        if not filename: filename = f"book_{int(time.time())}.epub"
        filepath = os.path.join(self.lib_dir, filename)
        file_obj.save(filepath)
        return filename

    def get_toc(self, filename):
        filepath = os.path.join(self.lib_dir, filename)
        if not os.path.exists(filepath): return None
        try:
            book = epub.read_epub(filepath)
            title = book.get_metadata('DC', 'title')[0][0] if book.get_metadata('DC', 'title') else filename
            chapters = [{'title': f"ç¬¬ {i+1} èŠ‚", 'url': f"epub:{filename}:{i}"} for i, _ in enumerate(book.spine)]
            return {'title': title, 'chapters': chapters}
        except: return None

    def get_chapter_content(self, filename, chapter_index):
        filepath = os.path.join(self.lib_dir, filename)
        try:
            book = epub.read_epub(filepath)
            item = book.get_item_with_id(book.spine[chapter_index][0])
            soup = BeautifulSoup(item.get_content(), 'html.parser')
            lines = [p.get_text(strip=True) for p in soup.find_all(['p', 'div', 'h1', 'h2']) if p.get_text(strip=True)]
            return {
                'title': f"ç¬¬ {chapter_index+1} èŠ‚", 'content': lines,
                'prev': f"epub:{filename}:{chapter_index-1}" if chapter_index > 0 else None,
                'next': f"epub:{filename}:{chapter_index+1}" if chapter_index < len(book.spine) - 1 else None,
                'toc_url': f"epub:{filename}:toc"
            }
        except Exception as e: return f"EPUB Error: {e}"

# å®ä¾‹åŒ–å¯¹è±¡
crawler_instance = NovelCrawler()
searcher = SearchHelper()
epub_handler = EpubHandler()