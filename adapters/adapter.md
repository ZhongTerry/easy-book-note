# ğŸ› ï¸ Smart NoteDB é€‚é…å™¨å¼€å‘æŒ‡å—

æœ¬æ–‡æ¡£æ—¨åœ¨æŒ‡å¯¼å¼€å‘è€…ä¸º **Smart NoteDB** ç¼–å†™ç‰¹å®šç«™ç‚¹çš„çˆ¬è™«é€‚é…å™¨ (Adapter)ã€‚

## 1. é€‚é…å™¨æœºåˆ¶ç®€ä»‹

Smart NoteDB é‡‡ç”¨ **â€œé€šç”¨ + æ’ä»¶â€** çš„æ··åˆçˆ¬è™«æ¨¡å¼ï¼š
1.  **é€šç”¨é€»è¾‘**ï¼š`spider_core.py` ä¸­çš„ `NovelCrawler` å¤„ç†ç»å¤§å¤šæ•°æ ‡å‡†ç»“æ„çš„ç½‘ç«™ã€‚
2.  **é€‚é…å™¨æ’ä»¶**ï¼šä½äº `adapters/` ç›®å½•ä¸‹ã€‚é’ˆå¯¹åçˆ¬ä¸¥é‡ã€ç»“æ„ç‰¹æ®Šæˆ–åˆ†é¡µé€»è¾‘å¤æ‚çš„ç½‘ç«™ï¼Œç³»ç»Ÿä¼šä¼˜å…ˆåŒ¹é…é€‚é…å™¨ã€‚

**åŠ è½½æœºåˆ¶**ï¼šç³»ç»Ÿå¯åŠ¨æ—¶ï¼Œ`AdapterManager` ä¼šè‡ªåŠ¨æ‰«æ `adapters/` ç›®å½•ä¸‹çš„æ‰€æœ‰ `.py` æ–‡ä»¶ï¼ŒåŠ è½½å…¶ä¸­ç±»ååŒ…å« `Adapter` çš„ç±»ã€‚

---

## 2. å¿«é€Ÿå¼€å§‹

åœ¨ `adapters/` ç›®å½•ä¸‹æ–°å»ºä¸€ä¸ª Python æ–‡ä»¶ï¼Œä¾‹å¦‚ `xxsite_adapter.py`ã€‚

### æ ‡å‡†æ¨¡æ¿

```python
import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin

class XxSiteAdapter:
    """
    Xxå°è¯´ç½‘é€‚é…å™¨
    ç±»åå¿…é¡»åŒ…å« 'Adapter' (åŒºåˆ†å¤§å°å†™)
    """

    def can_handle(self, url):
        """
        [å¿…éœ€] åˆ¤æ–­å½“å‰ URL æ˜¯å¦ç”±æœ¬é€‚é…å™¨å¤„ç†
        """
        return "xxsite.com" in url

    def get_toc(self, crawler, toc_url):
        """
        [å¿…éœ€] è§£æç›®å½•é¡µ
        :param crawler: ä¼ å…¥çš„ä¸»çˆ¬è™«å®ä¾‹ (ç”¨äºå‘é€è¯·æ±‚)
        :param toc_url: ç›®å½•é¡µ URL
        :return: å­—å…¸ {'title': ä¹¦å, 'chapters': [{'name': ç« èŠ‚å, 'url': é“¾æ¥}, ...]}
        """
        html = crawler._fetch_page_smart(toc_url)
        if not html: return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # 1. è·å–ä¹¦å
        book_title = soup.select_one('h1').get_text(strip=True)
        
        # 2. è·å–ç« èŠ‚åˆ—è¡¨
        chapters = []
        for link in soup.select('.chapter-list a'):
            chapters.append({
                'name': link.get_text(strip=True),
                'url': urljoin(toc_url, link['href'])
            })
            
        return {
            'title': book_title,
            'chapters': chapters
        }

    def run(self, crawler, url):
        """
        [å¿…éœ€] è§£ææ­£æ–‡é¡µ (åŒ…å«è‡ªåŠ¨ç¿»é¡µ/ç¼åˆé€»è¾‘)
        :param crawler: ä¼ å…¥çš„ä¸»çˆ¬è™«å®ä¾‹
        :param url: èµ·å§‹ç« èŠ‚ URL
        :return: å­—å…¸ (è§ä¸‹æ–‡è¯¦ç»†ç»“æ„)
        """
        # ä½¿ç”¨ crawler å‘é€è¯·æ±‚ï¼Œè‡ªåŠ¨å¤„ç† headers å’Œä»£ç†
        html = crawler._fetch_page_smart(url)
        if not html: return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # æå–æ ‡é¢˜
        title = soup.select_one('h1').get_text(strip=True)
        
        # æå–æ­£æ–‡ (æ¸…æ´—å¹¶è½¬ä¸ºåˆ—è¡¨)
        content_div = soup.select_one('#content')
        # åˆ©ç”¨ crawler çš„å†…ç½®å·¥å…·æ¸…æ´—åƒåœ¾æ–‡æœ¬
        content_lines = crawler._clean_text_lines(content_div.get_text('\n'))
        
        # è·å–ä¸Šä¸€ç« /ä¸‹ä¸€ç« /ç›®å½•é“¾æ¥
        prev_url = soup.find('a', text='ä¸Šä¸€ç« ')['href']
        next_url = soup.find('a', text='ä¸‹ä¸€ç« ')['href']
        toc_url = soup.find('a', text='ç›®å½•')['href']

        return {
            'title': title,
            'content': content_lines, # å¿…é¡»æ˜¯å­—ç¬¦ä¸²åˆ—è¡¨ List[str]
            'book_name': 'æœªçŸ¥ä¹¦å',   # å¯é€‰ï¼Œå¦‚æœæœ‰èƒ½æå–æ›´å¥½
            'prev': urljoin(url, prev_url),
            'next': urljoin(url, next_url),
            'toc_url': urljoin(url, toc_url)
        }
```

---

## 3. æ ¸å¿ƒ API è¯¦è§£

ç¼–å†™é€‚é…å™¨æ—¶ï¼Œ**ä¸è¦**è‡ªå·±ä½¿ç”¨ `requests` åº“ï¼Œè¯·åŠ¡å¿…è°ƒç”¨ä¼ å…¥çš„ `crawler` å®ä¾‹çš„æ–¹æ³•ï¼Œä»¥ç¡®ä¿æŒ‡çº¹ä¼ªè£…ï¼ˆcurl_cffiï¼‰å’Œä»£ç†è®¾ç½®ç”Ÿæ•ˆã€‚

### 3.1 `crawler._fetch_page_smart(url)`
*   **åŠŸèƒ½**ï¼šæ™ºèƒ½å‘é€ GET è¯·æ±‚ã€‚
*   **ç‰¹æ€§**ï¼šè‡ªåŠ¨å¤„ç†é‡è¯•ã€è¶…æ—¶ã€ä»¥åŠå¸¸è§ä¸­æ–‡ç¼–ç ï¼ˆGBK/UTF-8ï¼‰çš„è‡ªåŠ¨è¯†åˆ«ã€‚
*   **è¿”å›**ï¼šHTML å­—ç¬¦ä¸²ï¼ˆè§£ç åï¼‰æˆ– `None`ã€‚

### 3.2 `crawler._clean_text_lines(text)`
*   **åŠŸèƒ½**ï¼šæ¸…æ´—æ­£æ–‡æ–‡æœ¬ã€‚
*   **ç‰¹æ€§**ï¼šè‡ªåŠ¨å»é™¤å¹¿å‘Šè¯ï¼ˆå¦‚â€œä¸€ç§’è®°ä½â€ã€â€œåŠ å…¥ä¹¦ç­¾â€ï¼‰ã€å¤šä½™ç©ºè¡Œã€‚
*   **è¾“å…¥**ï¼šåŒ…å«æ¢è¡Œç¬¦çš„é•¿å­—ç¬¦ä¸²ã€‚
*   **è¿”å›**ï¼šå¹²å‡€çš„å­—ç¬¦ä¸²åˆ—è¡¨ `List[str]`ã€‚

### 3.3 `crawler._get_smart_title(soup)`
*   **åŠŸèƒ½**ï¼šå°è¯•ä» BeautifulSoup å¯¹è±¡ä¸­æ™ºèƒ½æå–ç« èŠ‚æ ‡é¢˜ã€‚

---

## 4. é«˜çº§æŠ€å·§ï¼šå¤„ç†ç« èŠ‚å†…åˆ†é¡µ

å¾ˆå¤šç½‘ç«™ä¸ºäº†éª—ç‚¹å‡»ï¼Œå°†ä¸€ç« æ‹†åˆ†ä¸º `1.html`, `1_2.html`ã€‚é€‚é…å™¨éœ€è¦è´Ÿè´£å°†å®ƒä»¬â€œç¼åˆâ€èµ·æ¥ã€‚

**æ¨èçš„ `run` æ–¹æ³•é€»è¾‘ï¼š**

```python
    def run(self, crawler, url):
        combined_content = []
        current_url = url
        first_title = ""
        meta_info = {} # å­˜ next, prev ç­‰
        
        page_count = 0
        while page_count < 10: # é˜²æ­¢æ­»å¾ªç¯ï¼Œæœ€å¤šæ‹¼10é¡µ
            html = crawler._fetch_page_smart(current_url)
            if not html: break
            soup = BeautifulSoup(html, 'html.parser')
            
            # 1. è®°å½•ç¬¬ä¸€é¡µçš„å…ƒæ•°æ®
            if page_count == 0:
                first_title = soup.select_one('h1').get_text(strip=True)
                # æå– prev, toc ...
            
            # 2. æå–æ­£æ–‡å¹¶è¿½åŠ 
            lines = crawler._clean_text_lines(soup.select_one('#content').get_text('\n'))
            combined_content.extend(lines)
            
            # 3. å¯»æ‰¾â€œä¸‹ä¸€é¡µâ€é“¾æ¥
            # æ³¨æ„ï¼šéœ€åŒºåˆ†â€œä¸‹ä¸€é¡µâ€å’Œâ€œä¸‹ä¸€ç« â€
            next_btn = soup.find('a', string=re.compile('ä¸‹ä¸€é¡µ'))
            if next_btn and 'ä¸‹ä¸€ç« ' not in next_btn.get_text():
                current_url = urljoin(current_url, next_btn['href'])
                page_count += 1
            else:
                # æ˜¯ä¸‹ä¸€ç« äº†ï¼Œè®°å½•é“¾æ¥å¹¶è·³å‡º
                if next_btn:
                    meta_info['next'] = urljoin(current_url, next_btn['href'])
                break
        
        return {
            'title': first_title,
            'content': combined_content,
            'next': meta_info.get('next'),
            # ... å…¶ä»–å­—æ®µ
        }
```

## 5. è°ƒè¯•å»ºè®®

åœ¨å¼€å‘è¿‡ç¨‹ä¸­ï¼Œå¯ä»¥åœ¨ä»£ç ä¸­æ’å…¥ `print` è¯­å¥ã€‚è¿è¡Œåç«¯æœåŠ¡æ—¶ï¼Œæ§åˆ¶å°ä¼šè¾“å‡ºè¿™äº›æ—¥å¿—ã€‚

```python
print(f"[MyAdapter] æ­£åœ¨è§£æ: {url}")
```

å¦‚æœé‡åˆ° `403 Forbidden` æˆ– Cloudflare æ‹¦æˆªï¼Œè¯·æ£€æŸ¥æ˜¯å¦åœ¨ `crawler._fetch_page_smart` è°ƒç”¨å‰éœ€è¦è®¾ç½®ç‰¹å®šçš„ Headersï¼Œæˆ–è€…è¯¥ç«™ç‚¹æ˜¯å¦å¿…é¡»ä½¿ç”¨ Seleniumï¼ˆç›®å‰æ¶æ„ä¸»è¦æ”¯æŒ curl_cffiï¼‰ã€‚