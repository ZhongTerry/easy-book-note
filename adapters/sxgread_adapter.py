import re
from urllib.parse import urljoin
from bs4 import BeautifulSoup

class SxgreadAdapter:
    """
    ä¹¦é¦™é˜ (sxgread.com) é€‚é…å™¨
    ç‰¹ç‚¹ï¼šå¯¼èˆªé“¾æ¥éšè—åœ¨ JS å˜é‡ä¸­ (prevpage, nextpage)
    """
    def get_book_name(self, soup):
        print("[SxgreadAdapter] ğŸ” å°è¯•ä»é¢åŒ…å±‘æå–ä¹¦å...")
        path = soup.find('div', class_='pagepath')
        if path:
            links = path.find_all('a')
            if len(links) >= 3:
                name = links[2].get_text(strip=True)
                print(f"[SxgreadAdapter] âœ… æå–æˆåŠŸ: {name}")
                return name
        print("[SxgreadAdapter] âŒ æå–å¤±è´¥")
        return None
    def can_handle(self, url):
        return "sxgread.com" in url

    def get_toc(self, crawler, toc_url):
        # ç›®å½•é¡µè§£æå¤ç”¨é€šç”¨é€»è¾‘ï¼Œå› ä¸ºä¹¦é¦™é˜ç›®å½•é¡µæ˜¯æ ‡å‡†çš„ HTML
        html = crawler._fetch_page_smart(toc_url)
        if not html: return None
        soup = BeautifulSoup(html, 'html.parser')
        
        # å°è¯•æå–ä¹¦å
        title = "æœªçŸ¥ä¹¦ç±"
        h1 = soup.find('h1')
        if h1: title = h1.get_text(strip=True)
        
        # åˆ©ç”¨çˆ¬è™«è‡ªå¸¦çš„é€šç”¨ç›®å½•è§£æå™¨ (å®ƒä¼šè‡ªåŠ¨å¤„ç†æ’åºå’Œå»é‡)
        chapters = crawler._parse_chapters_from_soup(soup, toc_url)
        
        return {'title': title, 'chapters': chapters}

    def run(self, crawler, url):
        html = crawler._fetch_page_smart(url)
        if not html: return None
        
        soup = BeautifulSoup(html, 'html.parser')
        meta = {}

        # 1. æ ‡é¢˜æå–
        # ä¼˜å…ˆæ‰¾ h1ï¼Œä¹¦é¦™é˜æ­£æ–‡æ ‡é¢˜åœ¨ .Noveltitle h1
        h1 = soup.find('div', class_='Noveltitle')
        if h1:
            meta['title'] = h1.get_text(strip=True)
        else:
            meta['title'] = crawler._get_smart_title(soup)
        book_name = self.get_book_name(soup) or crawler._get_smart_title(soup)
        # 2. æ­£æ–‡æå–
        # ä¹¦é¦™é˜æ­£æ–‡åœ¨ .NovelTxt
        content_div = soup.find('div', class_='NovelTxt')
        if content_div:
            # ç§»é™¤å¹¿å‘Šè„šæœ¬å’Œæ— ç”¨æ ‡ç­¾
            for junk in content_div.find_all(['script', 'style', 'div']): 
                junk.decompose()
            
            # ç§»é™¤ <br> æ ‡ç­¾ (get_text ä¼šè‡ªåŠ¨å¤„ç†æ¢è¡Œï¼Œä½†ä¸ºäº†ä¿é™©)
            text = content_div.get_text('\n')
            meta['content'] = crawler._clean_text_lines(text)
        else:
            meta['content'] = ["æ­£æ–‡æå–å¤±è´¥ï¼Œè¯·å°è¯•åˆ·æ–°æˆ–æ›´æ¢æºã€‚"]
        meta["book_name"] = book_name
        # 3. [æ ¸å¿ƒ] å¯¼èˆªæå– (Regex è§£æ JS)
        # æºç ç¤ºä¾‹: var prevpage="/book/1/738/4083161.html";
        
        # æå–ä¸Šä¸€é¡µ
        prev_match = re.search(r'var\s+prevpage\s*=\s*["\']([^"\']+)["\']', html)
        if prev_match:
            link = prev_match.group(1)
            # è¿™é‡Œçš„ index.html é€šå¸¸æŒ‡ç›®å½•ï¼Œå¦‚æœä¸Šä¸€é¡µæ˜¯ç›®å½•ï¼Œè¯´æ˜è¿™æ˜¯ç¬¬ä¸€ç« 
            if "index.html" not in link:
                meta['prev'] = urljoin(url, link)

        # æå–ä¸‹ä¸€é¡µ
        next_match = re.search(r'var\s+nextpage\s*=\s*["\']([^"\']+)["\']', html)
        if next_match:
            link = next_match.group(1)
            # å¦‚æœä¸‹ä¸€é¡µæ˜¯ index.htmlï¼Œè¯´æ˜æ˜¯æœ€åä¸€ç« ï¼Œæˆ–è€…æ˜¯æ²¡æœ‰ä¸‹ä¸€ç« äº†
            # æ³¨æ„ï¼šä¹¦é¦™é˜æœ‰æ—¶å€™æœ€åä¸€ç« çš„ nextpage æ˜¯ç©ºçš„ ""
            if link and "index.html" not in link:
                meta['next'] = urljoin(url, link)

        # æå–ç›®å½•
        toc_match = re.search(r'var\s+bookpage\s*=\s*["\']([^"\']+)["\']', html)
        if toc_match:
            meta['toc_url'] = urljoin(url, toc_match.group(1))

        return meta